% This is part of Mes notes de mathématique
% Copyright (c) 2008-2017
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Normes et distances}\label{Sect_definition}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

La définition d'une norme a déjà été donné à la définition \ref{DefNorme}. Il est possible de définir de nombreuses normes sur $\eR^n$. Citons en quelque unes. 

\begin{definition}
    Les normes $\| . \|_{L^p}$ ($p\in\eN$) sont définies de la façon suivante :
    \begin{equation}		\label{EqDeformeLp}
        \| x \|_{L^p}=\Big( \sum_{i=1}^n| x_i |^p\Big)^{1/p},
    \end{equation}
    pour tout $x=(x_1,\ldots,x_n)\in\eR^n$. 
\end{definition}

Le fait que \( x\mapsto\| x \|_{L^2}\) soit une norme provient de la propriété suivante :
\begin{equation}
    \sqrt{ (a+b)^2 }\leq \sqrt{ a^2 }+\sqrt{ b^2 },
\end{equation}
laquelle se démontre en passant au carré :
\begin{equation}        \label{EQooRYNYooTzZpPz}
    (a+b^2)=a^2+b^2+2ab\leq a^2+b^2+2| ab |=\big( \sqrt{ a^2 }+\sqrt{ b^2 } \big)^2.
\end{equation}

Parmi ces normes, celles qui seront le plus souvent utilisées dans ces notes sont
\begin{equation}
	\begin{aligned}[]
		\| x \|_{L^1}&=\sum_{i=1}^n| x_i |,\\
		\| x \|_{L^2}&=\Big( \sum_{i=1}^n| x_i |^2 \Big)^{1/2}.
	\end{aligned}
\end{equation}

\begin{definition}
    La norme $L^2$ est la \defe{norme euclidienne}{norme!euclidienne}. 
\end{definition}

\begin{definition}
    
Nous définissons également la \defe{norme supremum}{norme!supremum} par
\begin{equation}
	\| x \|_{\infty}=\sup_{1\leq i\leq n}| x_i |.
\end{equation}
\end{definition}
Nous admettons sans démonstration que les fonctions $\| . \|_{L^p}\colon \eR^n\to \eR^+$ sont bien des normes.

\newcommand{\CaptionFigDistanceEuclide}{La \emph{norme} euclidienne induit la \emph{distance} euclidienne. D'où son nom. Le point $C$ est construit aux coordonnées $(A_x,B_y)$.}
\input{auto/pictures_tex/Fig_DistanceEuclide.pstricks}

Soient $A=(A_x,A_y)$ et $B=(B_x,B_y)$ deux éléments de $\eR^2$. La distance\footnote{Ne pas confondre «distance» et «norme».} euclidienne entre $A$ et $B$ est donnée par $\| A-B \|_2$. En effet, sur la figure \ref{LabelFigDistanceEuclide}, la distance entre les points $A$ et $B$ est donnée par
\begin{equation}
	| AB |^2=| AC |^2+| CB |^2=| A_x-B_x |^2+| A_y-B_y |^2,
\end{equation}
par conséquent,
\begin{equation}
	| AB |=\sqrt{| A_x-B_x |^2+| A_y-B_y |^2}=\| A-B \|_2.
\end{equation}

\begin{remark}
	Si $A$, $B$ et $C$ sont trois points dans le plan $\eR^2$, alors l'inégalité triangulaire $| AB |\leq| AC |+| CB |$ est précisément la propriété \ref{ItemDefNormeiii} de la norme (définition \ref{DefNorme}). En effet l'inégalité triangulaire s'exprime de la façon suivante en terme de la norme $\| . \|_2$ :
	\begin{equation}	\label{EqNDeuxAmBNNdd}
		\| A-B \|_2\leq \| A-C \|_2+\| C-B \|_2.
	\end{equation}
	En notant $u=A-C$ et $v=C-B$, l'équation \eqref{EqNDeuxAmBNNdd} devient exactement la propriété de définition de la norme :
	\begin{equation}
		\| u+v \|_2\leq \| u \|_2+\| v \|_2.
	\end{equation}
	Ceci explique pourquoi cette propriété des norme est appelée «inégalité triangulaire».
\end{remark}

Les distances que nous avons vues jusqu'à présent sont des distances définies à partir d'une norme. La définition générale d'une distance est la définition \ref{DefMVNVFsX}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Produit scalaire}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}      \label{DEFooEEQGooNiPjHz}
    Une \defe{forme bilinéaire}{forme!bilinéaire} sur un espace vectoriel \( E\) est une application \( b\colon E\times E\to \eK\) telle que
    \begin{enumerate}
        \item
            \( b(u,v)=b(v,u)\),
        \item
            \( b(u+v,w)=b(u,w)+b(v,w)\),
        \item
            \( b(\lambda u,v)=\lambda b(u,v)\)
    \end{enumerate}
    pour tout \( u,v,w\in E\) et \( \lambda\in \eK\) où \( \eK\) est un corps commutatif.
\end{definition}

\begin{definition}      \label{DEFooJIAQooZkBtTy}
    Si $g$ est une application bilinéaire sur un espace vectoriel \( E\) nous disons qu'elle est
    \begin{enumerate}
        \item
            \defe{définie positive}{application!définie positive} si $g(u,u)\geq 0$ pour tout $u\in E$ et $g(u,u)=0$ si et seulement si $u=0$.
        \item
            \defe{semi-définie positive}{application!semi-définie positive} si $g(u,u)\geq 0$ pour tout $u\in E$. Nous dirons aussi parfois qu'elle est simplement «positive».
        \end{enumerate}
\end{definition}
Cela est évidemment à lier à la définition \ref{DefAWAooCMPuVM} et la proposition \ref{PROPooUAAFooEGVDRC} : une application bilinéaires est définie positive si et seulement si sa matrice symétrique associée l'est.

\begin{definition}\label{DefVJIeTFj}
    Un \defe{produit scalaire}{produit!scalaire!en général} sur un espace vectoriel \( E\) est une forme bilinéaire\footnote{Définition \ref{DEFooEEQGooNiPjHz}.} symétrique strictement définie positive\footnote{Définition \ref{DEFooJIAQooZkBtTy}.}.
\end{definition}

Étant donné que l'inégalité de Cauchy-Schwarz sera surtout utilisée dans le cas où un produit scalaire est bel et bien donné, nous l'énonçons et le démontrons avec des notations adaptée à l'usage. Le produit scalaire sera noté \( X\cdot Y\) pour \( b(X,Y)\) si \( b\) est la forme bilinéaire.
\begin{theorem}[Inégalité de Cauchy-Schwarz]      \label{ThoAYfEHG}
	Si $X$ et $Y$ sont des vecteurs, alors
    \begin{equation}        \label{EQooZDSHooWPcryG}
		| X\cdot Y |\leq\| X \|\| Y \|.
	\end{equation}
    Nous avons une égalité si et seulement si \( X\) et \( Y\) sont multiples l'un de l'autre.
\end{theorem}
\index{Cauchy-Schwarz}
\index{inégalité!Cauchy-Schwarz}

%TODO : mettre au point les notations.
\begin{proof}
	Étant donné que les deux membres de l'inéquation sont positifs, nous allons travailler en passant au carré afin d'éviter les racines carrés dans le second membre.

	Nous considérons le polynôme
	\begin{equation}
		P(t)=\| X+tY \|^2=(X+tY)\cdot(X+tY)=X\cdot X+tX\cdot Y+tY\cdot X+t^2Y\cdot Y.
	\end{equation}
	En ordonnant les termes selon les puissance de $t$,
	\begin{equation}
		P(t)=\| Y \|^2t^2+2(X\cdot Y)t+\| X \|^2.
	\end{equation}
	Cela est un polynôme du second degré en $t$. Par conséquent le discriminant\footnote{Le fameux $b^2-4ac$.} doit être négatif. Nous avons donc
	\begin{equation}
		\Delta=4(X\cdot Y)^2-4\| X \|^2\| Y \|^2\leq 0,
	\end{equation}
	ce qui donne immédiatement
	\begin{equation}
		(X\cdot Y)^2\leq\| X \|^2\| Y^2 \|.
	\end{equation}

    En ce qui concerne le cas d'égalité, si nous avons \( X\cdot Y=\| X \|\| Y \|\), alors le discriminant \( \Delta\) ci-dessus est nul et le polynôme \( P\) admet une racine double \( t_0\). Pour cette valeur nous avons
    \begin{equation}
        P(t_0)=| X+t_0Y |=0,
    \end{equation}
    ce qui implique \( X+t_0Y=0\) et donc que \( X\) et \( Y\) sont liés.
\end{proof}

Vu que nous allons voir un pâté d'espaces avec des produits scalaires, nous leur donnons un nom.
\begin{definition}\label{DefLZMcvfj}
    Un espace vectoriel \defe{euclidien}{euclidien!espace} est un espace vectoriel de dimension finie muni d'un produit scalaire (définition \ref{DefVJIeTFj}).
\end{definition}

\begin{definition}[Isométrie, thème \ref{THMooVUCLooCrdbxm}]      \label{DEFooGGTYooXsHIZj}
    Une \defe{isométrie}{isométrie!forme bilinéaire} d'une forme bilinéaire \( b\) sur l'espace vectoriel \( E\) est une application bijective \( f\colon E\to E\) telle que \( b\big( f(u),f(v) \big)=b(u,v)\) pour tout \( u,v\in E\).
\end{definition}
En particulier une isométrie d'un espace euclidien est une application bijective qui préserve le produit scalaire.

\begin{proposition} \label{PropEQRooQXazLz}
    Si \( x,y\mapsto x\cdot y\) est un produit scalaire sur un espace vectoriel \( E\), alors \( N(x)=\sqrt{x\cdot x}\) est une norme vérifiant l'identité du parallélogramme :
    \begin{equation}        \label{EqYCLtWfJ}
        \| x-y \|^2+\| x+y \|^2=2\| x \|^2+2\| y \|^2.
    \end{equation}
\end{proposition}

\begin{proof}

    Prouvons l'inégalité triangulaire\index{inégalité!triangulaire!produit scalaire}. Si \( x,y\in E\) nous avons
    \begin{equation}
        \| x+y \|=\sqrt{\| x \|^2+\| y \|^2+2x\cdot y}.
    \end{equation}
    Par l'inégalité de Cauchy-Schwarz, théorème \ref{ThoAYfEHG} nous avons aussi
    \begin{equation}
        \| x \|^2+\| y \|^2+2x\cdot y\leq \| x \|^2+\| y \|^2+2\| x \|\| y \|=\big( \| x \|+\| y \| \big)^2,
    \end{equation}
    donc
    \begin{equation}
        \| x+y \|\leq \sqrt{\big( \| x \|+\| y \| \big)^2}=\| x \|+\| y \|.
    \end{equation}

    La seconde assertion est seulement un calcul :
			\begin{equation}
				\begin{aligned}[]
					\| x-y \|^2+\| x+y \|^2&=(x-y)\cdot (x-y)+(x+y)\cdot(x+y)\\
					&=x\cdot x-x\cdot y-y\cdot x+y\cdot y\\
					&\quad +x\cdot x+x\cdot y+y\cdot x+y\cdot y\\
					&=2x\cdot x+2y\cdot y\\
					&=2\| x \|^2+2\| y \|^2.
				\end{aligned}
			\end{equation}
\end{proof}

Le produit scalaire permet de donner une norme via la formule suivante :
\begin{equation}
    \| x \|^2=x\cdot x.
\end{equation}

\begin{lemma}[\cite{KXjFWKA}]   \label{LemLPOHUme}
    Soit \( V\) un espace vectoriel muni d'un produit scalaire et de la norme associée. Si \( x,y\in V\) satisfont à \( \| x+y \|=\| x \|+\| y \|\), alors il existe \( \lambda\geq 0\) tel que \( x=\lambda y\).
\end{lemma}

\begin{proof}
    Quitte à raisonner avec \( x/\| x \|\) et \( y/\| y \|\), nous supposons que \( \| x \|=\| y \|=1\). Dans ce cas l'hypothèse signifie que \( \| x+y \|^2=4\). D'autre part en écrivant la norme en terme de produit scalaire,
    \begin{equation}
        \| x+y \|^2=\| x \|^2+\| y \|^2+2\langle x, y\rangle ,
    \end{equation}
    ce qui nous mène à affirmer que \( \langle x, y\rangle =1=\| x \|\| y \|\). Nous sommes donc dans le cas d'égalité de l'inégalité de Cauchy-Schwarz\footnote{Théorème \ref{ThoAYfEHG}.}, ce qui nous donne un \( \lambda\) tel que \( x=\lambda y\). Étant donné que \( \| x \|=\| y \|=1\) nous avons obligatoirement \( \lambda=\pm 1\), mais si \( \lambda=-1\) alors \( \langle x, y\rangle =-1\), ce qui est le contraire de ce qu'on a prétendu plus haut. Par soucis de cohérence, nous allons donc croire que \( \lambda=1\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Projection et angles}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Propriétés du produit scalaire]
	Si $X$ et $Y$ sont des vecteurs de $\eR^3$, alors
	\begin{description}
		\item[Symétrie] $X\cdot Y=Y\cdot X$;
		\item[Linéarité] $(\lambda X+\mu X')\cdot Y=\lambda(X\cdot Y)+\mu(X'\cdot Y)$ pour tout $\lambda$ et $\mu$ dans $\eR$;
		\item[Défini positif] $X\cdot X\geq 0$ et $X\cdot X=0$ si et seulement si $X=0$.
	\end{description}
\end{proposition}
Note : lorsque nous écrivons $X=0$, nous voulons voulons dire $X=\begin{pmatrix}
	0	\\ 
	0	\\ 
	0	
\end{pmatrix}$.


\begin{definition}
	La \defe{norme}{norme!vecteur} du vecteur $X$, notée $\| X \|$, est définie par 
	\begin{equation}
		\| X \|=\sqrt{X\cdot X}=\sqrt{x^2+y^2+z^2}
	\end{equation}
	si $X=(x,y,z)$. Cette norme sera parfois nommée «norme euclidienne».
\end{definition}
Cette définition est motivée par le théorème de Pythagore. Le nombre $X\cdot X$ est bien la longueur de la «flèche» $X$. Plus intrigante est la définition suivante :
\begin{definition}
	Deux vecteurs $X$ et $Y$ sont \defe{orthogonaux}{orthogonal!vecteur} si $X\cdot Y=0$. 
\end{definition}
Cette définition de l'orthogonalité est motivée par la proposition suivante.

\begin{proposition}		\label{PropProjScal}
	Si nous écrivons $\pr_Y$  l'opération de projection sur la droite qui sous-tend $Y$, alors nous avons
	\begin{equation}
		\| \pr_YX \|=\frac{ X\cdot Y }{ \| Y \| }.
	\end{equation}
\end{proposition}

\begin{proof}
	Les vecteurs $X$ et $Y$ sont des flèches dans l'espace. Nous pouvons choisir un système d'axe orthogonal tel que les coordonnées de $X$ et $Y$ soient
	\begin{equation}
		\begin{aligned}[]
			X&=\begin{pmatrix}
				x	\\ 
				y	\\ 
				0	
			\end{pmatrix},
			&Y&=\begin{pmatrix}
				l	\\ 
				0	\\ 
				0	
			\end{pmatrix}
		\end{aligned}
	\end{equation}
	où $l$ est la longueur du vecteur $Y$. Pour ce faire, il suffit de mettre le premier axe le long de $Y$, le second dans le plan qui contient $X$ et $Y$, et enfin le troisième axe dans le plan perpendiculaire aux deux premiers.

	Un simple calcul montre que $X\cdot Y=xl+y\cdot 0+0\cdot 0=xl$. Par ailleurs, nous avons $\| \pr_YX \|=x$. Par conséquent,
	\begin{equation}
		\| \pr_YX \|=\frac{ X\cdot Y }{ l }=\frac{ X\cdot Y }{ \| Y \| }.
	\end{equation}
\end{proof}

\begin{corollary}
	Si la norme de $Y$ est $1$, alors le nombre $X\cdot Y$ est la longueur de la projection de $X$ sur $Y$.
\end{corollary}

\begin{proof}
	Poser $\| Y \|=1$ dans la proposition \ref{PropProjScal}.
\end{proof}

\begin{remark}
    Outre l'orthogonalité, le produit scalaire permet de savoir l'angle entre deux vecteurs à travers la définition \ref{DEFooSVDZooPWHwFQ}. D'autres interprétations géométriques du déterminant sont listées dans le thème \ref{THMooUXJMooOroxbI}.
\end{remark}

Nous sommes maintenant en mesure de déterminer, pour deux vecteurs quelconques $u$ et $v$, la projection orthogonale de $u$ sur $v$. Ce sera le vecteur $\bar u$ parallèle à $v$ tel que $u-\bar u$ est orthogonal à $v$. Nous avons donc
\begin{equation}
    \bar u=\lambda v
\end{equation}
et 
\begin{equation}
    (u-\lambda v)\cdot v=0.
\end{equation}
La seconde équation donne $u\cdot v-\lambda v\cdot v=0$, ce qui fournit $\lambda$ en fonction de $u$ et $v$ :
\begin{equation}
    \lambda=\frac{ u\cdot v }{ \| v \|^2 }.
\end{equation}
Nous avons par conséquent
\begin{equation}
    \bar u=\frac{ u\cdot v }{ \| v \|^2 }v.
\end{equation}
Armés de cette interprétation graphique du produit scalaire, nous comprenons pourquoi nous disons que deux vecteurs sont orthogonaux lorsque leur produit scalaire est nul.

Nous pouvons maintenant savoir quel est le coefficient directeur d'une droite orthogonale à une droite donnée. En effet, supposons que la première droite soit parallèle au vecteur $X$ et la seconde au vecteur $Y$. Les droites seront perpendiculaires si $X\cdot Y=0$, c'est à dire si
\begin{equation}
	\begin{pmatrix}
		x_1	\\ 
		y_1	
	\end{pmatrix}\cdot\begin{pmatrix}
		y_1	\\ 
		y_2	
	\end{pmatrix}=0.
\end{equation}
Cette équation se développe en 
\begin{equation}		\label{Eqxuyukljsca}
	x_1y_1=-x_2y_2.
\end{equation}
Le coefficient directeur de la première droite est $\frac{ x_2 }{ x_1 }$. Isolons cette quantité dans l'équation \eqref{Eqxuyukljsca} :
\begin{equation}
	\frac{ x_2 }{ x_1 }=-\frac{ y_1 }{ y_2 }.
\end{equation}
Donc le coefficient directeur de la première est l'inverse et l'opposé du coefficient directeur de la seconde.

\begin{example}
	Soit la droite $d\equiv y=2x+3$. Le coefficient directeur de cette droite est $2$. Donc le coefficient directeur d'une droite perpendiculaires doit être $-\frac{ 1 }{ 2 }$.
\end{example}

\begin{proof}[Preuve alternative]
	La preuve peut également être donnée en ne faisant pas référence au produit scalaire. Il suffit d'écrire toutes les quantités en termes des coordonnées de $X$ et $Y$. Si nous posons
	\begin{equation}
		\begin{aligned}[]
			X&=\begin{pmatrix}
				x_1	\\ 
				x_2	\\ 
				x_2	
			\end{pmatrix},
			&Y&=\begin{pmatrix}
				y_1	\\ 
				y_2	\\ 
				y_3	
			\end{pmatrix},
		\end{aligned}
	\end{equation}
	l'inégalité à prouver devient
	\begin{equation}
		(x_1y_1+x_2y_2+x_3y_3)^2\leq (x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2).
	\end{equation}
	Nous considérons la fonction
	\begin{equation}
		\varphi(t)=(x_1+ty_1)^2+(x_2+ty_2)^2+(x_3+ty_3)^2
	\end{equation}
	En tant que norme, cette fonction est évidement positive pour tout $t$. En regroupant les termes de chaque puissance de $t$, nous avons
	\begin{equation}
		\varphi(t)=(y_1^2+y_2^2+y_3^2)t^2+2(x_1y_1+x_2y_2+x_3y_3)t+(x_1^2+x_2^2+x_3^2).
	\end{equation}
	Cela est un polynôme du second degré en $t$. Par conséquent le discriminant doit être négatif. Nous avons donc
	\begin{equation}
		4(x_1y_1+x_2y_2+x_3y_3)^2-(x_1^2+x_2^2+x_3^2)(y_1^2+y_2^2+y_3^2)\leq 0.
	\end{equation}
	La thèse en découle aussitôt.
\end{proof}

\begin{proposition}
	La norme euclidienne a les propriétés suivantes :
	\begin{enumerate}
		\item
			Pour tout vecteur $X$ et réel $\lambda$,  $\| \lambda X \|=| \lambda |\| X \|$. Attention à ne pas oublier la valeur absolue !
		\item
			Pour tout vecteurs $X$ et $Y$, $\| X+Y \|\leq \| X \|+\| Y \|$.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Nous ne prouvons pas le premier point.
    % TODO : faire la preuve
    Pour le second, nous avons les inégalités suivantes :
	\begin{subequations}
		\begin{align}
			\| X+Y \|^2&=\| X \|^2+\| Y \|^2+2X\cdot Y\\
			&\leq\| X \|^2+\| Y \|^2+2|X\cdot Y|\\
			&\leq\| X \|^2+\| Y \|^2+2\| X \|\| Y \|\\
			&=\big( \| X \|+\| Y \| \big)^2
		\end{align}
	\end{subequations}
    Nous avons utilisé d'abord la majoration $| x |\geq x$ qui est évident pour tout nombre $x$; et ensuite l'inégalité de Cauchy-Schwarz \ref{ThoAYfEHG}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Procédé de Gram-Schmidt}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Procédé de Gram-Schmidt]    \label{PropUMtEqkb}
    Un espace euclidien possède une base orthonormée.
\end{proposition}
\index{espace!euclidien}
\index{Gram-Schmidt}

\begin{proof}
    Soit \( E\) un espace euclidien et \( \{ v_1,\ldots, v_n \}\), une base quelconque de \( E\). Nous posons d'abord
    \begin{equation}
        \begin{aligned}[]
            f_1&=v_1,&e_1&=\frac{ f_1 }{ \| f_1 \| }.
        \end{aligned}
    \end{equation}
    Ensuite
    \begin{equation}
        \begin{aligned}[]
            f_2&=v_2-\langle v_2, e_1\rangle e_1,&e_2&=\frac{ f_2 }{ \| f_2 \| }.
        \end{aligned}
    \end{equation}
    Notons que \( \{ e_1,e_2 \}\) est une base de \( \Span\{ v_1,v_2 \}\). De plus elle est orthogonale :
    \begin{equation}
        \langle e_1, f_2\rangle =\langle e_1, v_2\rangle -\langle v_2, e_1\rangle \underbrace{\langle e_1, e_1\rangle}_{=1} =0.
    \end{equation}
    Le fait que \( \| e_1 \|=\| e_2 \|=1\) est par construction. Nous avons donc donné une base orthonormée de \( \Span\{ v_1,v_2 \}\).

    Nous continuons par récurrence en posant
    \begin{equation}
        \begin{aligned}[]
            f_k&=v_k-\sum_{i=1}^{k-1}\langle v_k, e_i\rangle e_i,&e_k&=\frac{ f_k }{ \| f_k \| }.
        \end{aligned}
    \end{equation}
    Pour tout \( j<k\) nous avons
    \begin{equation}
        \langle e_j, f_k\rangle =\langle e_j, v_k\rangle -\sum_{i=1}^{k-1}\langle v_k, e_i\rangle \underbrace{\langle e_i, e_j\rangle}_{=\delta_{ij}} =0
    \end{equation}
\end{proof}
Cet algorithme de Gram-Schmidt nous donne non seulement l'existence de bases orthonormée pour tout espace euclidien, mais aussi le moyen d'en construire à partir de n'importe quelle base.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Hermitien, orthogonal, adjoint}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}  \label{DefMZQxmQ}
Si \( E\) est un espace vectoriel sur \( \eC\), nous disons qu'une application \( \langle ., .\rangle \colon E\times E\to \eC\) est un \defe{produit scalaire hermitien}{produit!scalaire!hermitien}\index{hermitien!produit scalaire} si pour tout \( u,v\in E\) nous avons
\begin{enumerate}
    \item
        \( \langle u, v\rangle =\overline{ \langle v, u\rangle  }\)
    \item
        \( \lambda\langle u, v\rangle =\langle \lambda u, v\rangle =\langle u, \bar \lambda v\rangle \)
    \item
        \( \langle u, u\rangle \in \eR^+\) et \( \langle u, u\rangle =0\) si et seulement si \( u=0\).
\end{enumerate}
Un espace vectoriel sur \( \eC\) muni d'un produit scalaire hermitien est un espace vectoriel hermitien.
\end{definition}

\begin{example}
    L'ensemble \( E=\eC^n\) vu comme espace vectoriel de dimension \( n\) sur \( \eC\)  est muni d'une forme sesquilinéaire
    \begin{equation}    \label{EqFormSesqQrjyPH}
        \langle x, y\rangle =\sum_{k=1}^nx_k\bar y_k
    \end{equation}
    pour tout \( x,y\in\eC^n\). Cela est un espace vectoriel hermitien.
\end{example}

\begin{definition}      \label{DEFooROVNooFlTbSK}
    Soit \( E\) un espace vectoriel hermitien et \( A\in\End(E)\). L'opérateur \defe{adjoint}{opérateur!adjoint} de \( A\) est l'opérateur noté \( A^*\) définit par
    \begin{equation}    \label{EQooHWYKooFzAGgB}
        \langle Av, w\rangle =\langle v, A^*w\rangle 
    \end{equation}
    pour tout \( v,w\in \eC^n\).
\end{definition}

La prescription \eqref{EQooHWYKooFzAGgB} définit bien l'opérateur \( A^*\) de façon unique. En effet si \( \{ e_i \}\) est une base orthonormée alors
\begin{equation}
    \langle e_i, A^*e_j\rangle =(A^*e_j)_i=\langle Ae_i, e_j\rangle =(Ae_i)_j.
\end{equation}
Autrement dit, \( (A^*e_j)_i=(Ae_i)_j\).

\begin{proposition}     \label{PROPooSHZMooGwdfBd}
    En ce qui concerne le déterminant,
    \begin{equation}
        \det(A^*)=\det(A)^*
    \end{equation}
    où l'étoile à droite dénote la conjugaison complexe dans \( \eC\).
\end{proposition}

\begin{proof}
    Écrivons la définition \eqref{EQooOJEXooXUpwfZ} avec l'expression explicite \eqref{EQooOJEXooXUpwfZ} du déterminant. Le tout avec la base canonique :
    \begin{equation}
            \det(A)=\det_{(e_1,\ldots, e_n)}(Ae_1,\ldots, Ae_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^ne_{\sigma(i)}^*(Ae_i).
    \end{equation}
    Mais nous pouvons développer :
    \begin{equation}
        e^*_{\sigma(i)}(Ae_i)=\langle e_{\sigma(i)}, Ae_i\rangle =\langle A^*e_{\sigma(i)}, e_i\rangle =\langle e_i, A^*e_{\sigma(i)}\rangle^*=e_i^*(A^*e_{\sigma(i)})^*.
    \end{equation}
    Notez que dans la dernière expression, les trois \( {}^*\) ont trois significations différentes. Par conséquent,
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^{n}e_i^*(A^*e_{\sigma(i)})^*.
    \end{equation}
    Mais \( e_i^*(A^*e_{\sigma(i)})=e_{\sigma(j)}^*(A^*e_{j})\) pour \( j=\sigma(i)\), donc le produit ne change pas si on déplace le \( \sigma\) :
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}\epsilon(\sigma)\prod_{i=1}^{n}e_{\sigma(i)}^*(A^*e_{i})^*=\det(A^*)^*.
    \end{equation}
\end{proof}

\begin{definition}      \label{DEFooKEBHooWwCKRK}
    Un opérateur \( A\) est \defe{hermitien}{opérateur!hermitien} si \( A^*=A\). On dit aussi \defe{autoadjoint}{opérateur!autoadjoint}.
\end{definition}

Le mot «hermitien» est réservé aux opérateurs sur des espaces hermitiens, c'est à dire des espaces vectoriels sur \( \eC\). Le mot «autoadjoint» par contre est plutôt utilisé dans le cadre d'opérateurs sur les espaces réels\footnote{Voir la définition \ref{DEFooYNEQooGQgbCf}.}. En conséquence de quoi, ces deux mots sont synonymes, mais il est préférable d'utiliser «hermitien» lorsque l'espace vectoriel est sur \( \eC\) et «autoadjoint» lorsqu'il est sur \( \eR\).

\begin{remark}
    Le fait d'être hermitien n'implique en rien le fait d'être inversible.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Opérateur orthogonal, matrice orthogonale}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooYKCSooURQDoS}
    Un opérateur est \defe{orthogonal}{orthogonal!opérateur} lorsque \( A^*=A^{-1}\) où \( A^*\) est l'adjoint de \( A\) définit en \ref{DEFooROVNooFlTbSK}.
\end{definition}

\begin{definition}      \label{DEFooUHANooLVBVID}
    Une matrice \( U\) est \defe{orthogonale}{matrice!orthogonale}\index{orthogonal!matrice} si \( U^t=U^{-1}\). Le \defe{groupe orthogonal}{groupe!orthogonal} noté \( \gO(n)\) est l'ensemble des matrices orthogonales \( n\times n\).
\end{definition}

\begin{lemma}       \label{LEMooSSALooSBFzJb}
    Soit un opérateur \( A\colon \eR^n\to \eR^n\) muni du produit scalaire usuel. Il est orthogonal si et seulement si sa matrice dans la base canonique est orthogonale\footnote{Définition \ref{DEFooUHANooLVBVID}.}.
\end{lemma}

\begin{proof}
    Soit la base canonique \( \{ e_i \}_{i=1,\ldots, n}\) de \( \eR^n\). Nous avons
    \begin{equation}
        \langle AA^*e_i, e_j\rangle =\langle e_i, e_j\rangle =\delta_{ij},
    \end{equation}
    donc \( \big( (AA^*)e_j \big)_j=\delta_{ij}\), ou encore \( (AA^*)_{ij}=\delta_{ij}\), ce qui signifie que la matrice $AA^*$ est l'identité.
\end{proof}


Pour rappel, une isométrique d'un espace euclidien est une bijection qui préserve le produit scalaire, définition \ref{DEFooGGTYooXsHIZj}.

\begin{proposition}[Thème \ref{THMooVUCLooCrdbxm}]     \label{PropKBCXooOuEZcS}
    À propos de matrices orthogonales.
    \begin{enumerate}
        \item
            L'ensemble des matrice réelles orthogonales forme un groupe noté \( \gO(n,\eR)\)\nomenclature[B]{\( \gO(n,\eR)\)}{le groupe des matrices orthogonales}.
        \item
            Si \( A\) est une matrice orthogonale, alors \( \det(A)=\pm 1\).
        \item       \label{ITEMooOWMBooHUatNb}
            Le groupe \( \gO(n)\) est le groupe des isométries de \( \eR^n\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Si \( A\) et \( B\) sont orthogonales, alors
    \begin{equation}
        (AB)(AB)^t=ABB^tA^t=A\mtu A^t=\mtu.
    \end{equation}
    Vu que \( \mtu\) est orthogonale, nous avons bien un groupe.

    En ce qui concerne le déterminant, \( AA^t=\mtu\) donne \( \det(A)\det(A^t)=1\) ou encore \( \det(A)^2=1\). D'où le fait que \( \det(A)=\pm 1\).

    En ce qui concerne le déterminant, nous savons par la proposition \ref{PROPooSHZMooGwdfBd} que \( \det(A)=\det(A^*)\) (nous sommes dans le cas réel). Étant donné que le déterminant est un morphisme et que \( AA^*=\id\) avec \( \det(\id)=1\) nous avons aussi \( \det(A)^2=1\), c'est à dire \( \det(A)=\pm1\).

    D'autre part si \( A\) est une isométrie de \( \eR^n\) alors pour tout \( x,y\in \eR^n\) nous avons \( \langle Ax, Ay\rangle =\langle x, y\rangle \). En particulier,
    \begin{equation}
        \langle A^tAx, y\rangle =\langle x, y\rangle 
    \end{equation}
    pour tout \( x,y\in \eR^n\). En prenant \( y=e_i\) nous trouvons
    \begin{equation}
        (A^tAx)_i=x_i,
    \end{equation}
    ce qui signifie que pour tout \( x\), \( A^tAx=x\), ou encore que \( A^tA\) est l'identité.

    Réciproquement si \( A^tA\) est l'identité nous avons
    \begin{equation}
        \langle a, y\rangle =\langle A^tAx, y\rangle =\langle Ax, Ay\rangle ,
    \end{equation}
    ce qui prouve que \( A\) est une isométrie.
\end{proof}

\begin{definition}      \label{DEFooJLNQooBKTYUy}
    Le sous-groupe des matrices orthogonales de déterminant \( 1\) est le groupe \defe{spécial orthogonal}{groupe!spécial orthogonal} noté \( \SO(n)\).
\end{definition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Directions conservées}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

Nous savons qu'une application \emph{linéaire} $A\colon \eR^3\to \eR^3$ est complètement définie par la donnée de son action sur les trois vecteurs de base, c'est à dire par la donnée de
\begin{equation}
	\begin{aligned}[]
		Ae_1,&&Ae_2&&\text{et}&&Ae_3.
	\end{aligned}
\end{equation}
Nous allons former la matrice de $A$ en mettant simplement les vecteurs $Ae_1$, $Ae_2$ et $Ae_3$ en colonne. Donc la matrice
\begin{equation}		\label{EqExempleALin}
	A=\begin{pmatrix}
		3	&	0	&	0	\\
		0	&	1	&	0	\\
		0	&	1	&	0
	\end{pmatrix}
\end{equation}
signifie que l'application linéaire $A$ envoie le vecteur $e_1$ sur $\begin{pmatrix}
	3	\\ 
	0	\\ 
	0	
\end{pmatrix}$, le vecteur $e_2$ sur $\begin{pmatrix}
	0	\\ 
	0	\\ 
	1	
\end{pmatrix}$ et le vecteur $e_3$ sur $\begin{pmatrix}
	0	\\ 
	1	\\ 
	0	
\end{pmatrix}$.
Pour savoir comment $A$ agit sur n'importe quel vecteur, on applique la règle de produit vecteur$\times$matrice :
\begin{equation}
	\begin{pmatrix}
		1	&	2	&	3	\\
		4	&	5	&	6	\\
		7	&	8	&	9
	\end{pmatrix}\begin{pmatrix}
		x	\\ 
		y	\\ 
		z	
	\end{pmatrix}=
	\begin{pmatrix}
		x+2y+3z	\\ 
		4x+5y+6z	\\ 
		7x+8y+9z	
	\end{pmatrix}.
\end{equation}

Une chose intéressante est de savoir quelles sont les directions invariantes de la transformation linéaire. Par exemple, on peut lire sur la matrice \eqref{EqExempleALin} que la direction $\begin{pmatrix}
	1	\\ 
	0	\\ 
	0	
\end{pmatrix}$ est invariante : elle est simplement multipliée par $3$. Dans cette direction, la transformation est juste une dilatation. Afin de savoir si $v$ est un vecteur d'une direction conservée, il faut voir s'il existe un nombre $\lambda$ tel que $Av=\lambda v$, c'est à dire voir si $v$ est simplement dilaté.

L'équation $Av=\lambda v$ se récrit $(A-\lambda\mtu)v=0$, c'est à dire qu'il faut résoudre l'équation
\begin{equation}
	(A-\lambda\mtu)\begin{pmatrix}
		x	\\ 
		y	\\ 
		z	
	\end{pmatrix}=
	\begin{pmatrix}
		0	\\ 
		0	\\ 
		0	
	\end{pmatrix}.
\end{equation}
Nous savons qu'une telle équation ne peut avoir de solutions que si $\det(A-\lambda\mtu)=0$. La première étape est donc de trouver les $\lambda$ qui vérifient cette condition.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Comment trouver la matrice d'une symétrie donnée ?}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecMtrSym}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à un plan}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Comment trouver par exemple la matrice $A$ qui donne la symétrie autour du plan $z=0$ ? La définition d'une telle symétrie est que les vecteurs du plan $z=0$ ne bougent pas, tandis que les vecteurs perpendiculaires changent de signe. Ces informations vont permettre de trouver comment $A$ agit sur une base de $\eR^3$. En effet :
\begin{enumerate}

	\item
		Le vecteur $\begin{pmatrix}
			1	\\ 
			0	\\ 
			0	
		\end{pmatrix}$ est dans le plan $z=0$, donc il ne bouge pas,

	\item
		le vecteur $\begin{pmatrix}
			0	\\ 
			1	\\ 
			0	
		\end{pmatrix}$ est également dans le plan, donc il ne bouge pas non plus,

	\item
		et le vecteur $\begin{pmatrix}
			0	\\ 
			0	\\ 
			1	
		\end{pmatrix}$ est perpendiculaire au plan $z=0$, donc il va changer de signe.

\end{enumerate}
Cela nous donne directement les valeurs de $A$ sur la base canonique et nous permet d'écrire 
\begin{equation}
	A=\begin{pmatrix}
		1	&	0	&	0	\\
		0	&	1	&	0	\\
		0	&	0	&	-1
	\end{pmatrix}.
\end{equation}
Pour écrire cela, nous avons juste mit en colonne les images des vecteurs de base. Les deux premiers n'ont pas changé et le troisième a changé.

Et si maintenant on donne un plan moins facile que $z=0$ ? Le principe reste le même : il faudra trouver deux vecteurs qui sont dans le plan (et dire qu'ils ne bougent pas), et puis un vecteur qui est perpendiculaire au plan\footnote{Pour le trouver, penser au produit vectoriel.}, et dire qu'il change de signe.

Voyons ce qu'il en est pour le plan $x=-z$. Il faut trouver deux vecteurs linéairement indépendants dans ce plan. Prenons par exemple
\begin{equation}		\label{EqffudE}
	\begin{aligned}[]
		f_1&=\begin{pmatrix}
			0	\\ 
			1	\\ 
			0	
		\end{pmatrix},&f_2&=\begin{pmatrix}
			1	\\ 
			0	\\ 
			-1	
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Nous avons 
\begin{equation}
	\begin{aligned}[]
		Af_1&=f_1\\
		Af_2&=f_2.
	\end{aligned}
\end{equation}
Afin de trouver un vecteur perpendiculaire au plan, calculons le produit vectoriel :
\begin{equation}
	f_3=f_1\times f_2=\begin{vmatrix}
		e_1	&	e_2	&	e_3	\\
		0	&	1	&	0	\\
		1	&	0	&	-1
	\end{vmatrix}=-e_1-e_3=\begin{pmatrix}
		-1	\\ 
		0	\\ 
		-1	
	\end{pmatrix}.
\end{equation}
Nous avons 
\begin{equation}
	Af_3=-f_3.
\end{equation}
Afin de trouver la matrice $A$, il faut trouver $Ae_1$, $Ae_2$ et $Ae_3$. Pour ce faire, il faut d'abord écrire $\{ e_1,e_2,e_3 \}$ en fonction de $\{ f_1,f_2,f_3 \}$. La première des équation \eqref{EqffudE} dit que 
\begin{equation}
	f_1=e_2.
\end{equation}
Ensuite, nous avons
\begin{equation}
	\begin{aligned}[]
		f_2&=e_1-e_3\\
		f_3&=-e_1-e_3.
	\end{aligned}
\end{equation}
La somme de ces deux équations donne $-2e_3=f_2+f_3$, c'est à dire
\begin{equation}
	e_3=-\frac{ f_2+f_3 }{ 2 }
\end{equation}
Et enfin, nous avons
\begin{equation}
	e_1=\frac{ f_2-f_3 }{ 2 }.
\end{equation}

Maintenant nous pouvons calculer les images de $e_1$, $e_2$ et $e_3$ en faisant
\begin{equation}
	\begin{aligned}[]
		Ae_1&=\frac{ Af_2-Af_3 }{ 2 }=\frac{1 }{2}\begin{pmatrix}
			0	\\ 
			0	\\ 
			-2	
		\end{pmatrix}=\begin{pmatrix}
			0	\\ 
			0	\\ 
			-1	
		\end{pmatrix},\\
		Ae_2&=Af_1=f_1=\begin{pmatrix}
			0	\\ 
			1	\\ 
			0	
		\end{pmatrix},\\
		Ae_3&=-\frac{ f_2-f_3 }{ 2 }=-\frac{ 1 }{2}\begin{pmatrix}
			2	\\ 
			0	\\ 
			0	
		\end{pmatrix}=\begin{pmatrix}
			-1	\\ 
			0	\\ 
			0	
		\end{pmatrix}.
	\end{aligned}
\end{equation}
La matrice $A$ s'écrit maintenant en mettant les trois images trouvées en colonnes :
\begin{equation}
	A=\begin{pmatrix}
		0	&	0	&	-1	\\
		0	&	1	&	0	\\
		-1	&	0	&	0
	\end{pmatrix}.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Symétrie par rapport à une droite}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le principe est exactement le même : il faut trouver trois vecteurs $f_1$, $f_2$ et $f_3$ sur lesquels on connaît l'action de la symétrie. Ensuite il faudra exprimer $e_1$, $e_2$ et $e_3$ en termes de $f_1$, $f_2$ et $f_3$.

Le seul problème est de trouver les trois vecteurs $f_i$. Le premier est tout trouvé : c'est n'importe quel vecteur sur la droite. Pour les deux autres, il faut un peu ruser parce qu'il faut impérativement qu'ils soient perpendiculaire à la droite. Pour trouver $f_2$, on peut écrire
\begin{equation}
	f_2=\begin{pmatrix}
		1	\\ 
		0	\\ 
		x	
	\end{pmatrix},
\end{equation}
et puis fixer le $x$ pour que le produit scalaire de $f_2$ avec $f_1$ soit nul. S'il n'y a pas moyen (genre si $f_1$ a sa troisième composante nulle), essayer avec $\begin{pmatrix}
	x	\\ 
	1	\\ 
	0	
\end{pmatrix}$. Une fois que $f_2$ est trouvé (il y a des milliards de choix possibles), trouver $f_3$ est super facile : prendre le produit vectoriel entre $f_1$ et $f_2$.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{En résumé}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
La marche à suivre est

\begin{enumerate}

	\item
		Trouver trois vecteurs $f_1$, $f_2$ et $f_3$ sur lesquels on connaît l'action de la symétrie. Typiquement : des vecteurs qui sont sur l'axe ou le plan de symétrie, et puis des perpendiculaires. Pour la perpendiculaire, penser au produit scalaire et au produit vectoriel.

	\item
		Exprimer la base canonique $e_1$, $e_2$ et $e_3$ en termes de $f_1$, $f_2$, $f_3$.

	\item
		Trouver $Ae_1$, $Ae_2$ et $Ae_3$ en utilisant leur expression en termes des $f_i$, et le fait que l'on connaisse l'action de $A$ sur les $f_i$.

	\item
		La matrice s'obtient en mettant les images des $e_i$ en colonnes.
\end{enumerate}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espace vectoriel normé}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SeckwyQjK}

\begin{definition}[\cite{BrunelleMatricielle}]  \label{DefOYPooZIoWnI}
    Soit \( E\) un espace vectoriel (pas spécialement de dimension finie). Une  \defe{norme}{norme} sur $E$ est une application $\| . \|\colon E\to \eR$ telle que
    \begin{enumerate}
        \item
            $\| v \|=0$ seulement si $A=0$,
        \item
            $\| \lambda v \|=| \lambda |\cdot\| v \|$,
        \item
            $\| v+w \|\leq\| v \|+\| w \|$

    \end{enumerate}
    pour tout $v,w\in E$ et pour tout $\lambda\in\eR$.
\end{definition}

\begin{definition}  \label{DefDQRooVGbzSm}
    Si \( V\) et \( W\) sont des espaces vectoriels nous munissons \( \aL(V,W)\) d'une structure d'espace vectoriel en définissant la somme et le produit par un scalaire de la façon suivante. Si $T$ et $U$ sont des élément de $\aL(V,W)$ et si $\lambda$ est un réel, nous définissons les éléments $T+U$ et $\lambda T$ par
    \begin{enumerate}
        \item
            $(T+U)(x)=T(x)+U(x)$;
        \item
            $(\lambda T)(x)=\lambda T(x)$
    \end{enumerate}
    pour tout \( x\in V\).
\end{definition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Norme opérateur}
%---------------------------------------------------------------------------------------------------------------------------

La proposition suivante donne une norme (au sens de la définition \ref{DefNorme}) sur $\aL(V,W)$ afin d'obtenir un espace vectoriel normé.
\begin{proposition}		\label{PropNormeAppLineaire}
    Soit le nombre
	\begin{equation}
        \|T\|_{\aL}=\sup_{x\in V}\frac{\|T(x)\|_{W}}{\|x\|_{V}}.
	\end{equation}
    \begin{enumerate}
        \item
            Si \( V\) est de dimension finie, alors \( \| T \|_{\aL}<\infty\).
        \item
            L'application \( T\mapsto\| T \|_{\aL}\) est une norme sur l'espace vectoriel des applications linéaires \( V\to W\).
        \item
            Nous avons la formule
            \begin{equation}    \label{EqFZPooIoecGH}
                \| T \|_{\aL}=\sup_{x\in V}\frac{\|T(x)\|_{W}}{\|x\|_{V}} =\sup_{\|x\|_{V}=1}\|T(x)\|_{W}
            \end{equation}
    \end{enumerate}
\end{proposition}
\index{norme!d'une application linéaire}

\begin{proof}
    Si \( V\) est de dimension finie alors l'ensemble $\{ \| x \|= 1 \}$ est compact par le théorème de Borel-Lebesgue \ref{ThoXTEooxFmdI}. Alors la fonction 
    \begin{equation}
        x\mapsto \frac{ \| T(x) \| }{ \| x \| }
    \end{equation}
    est une fonction continue sur un compact. Le corollaire \ref{CorFnContinueCompactBorne} nous dit alors qu'elle est bornée. Le supremum est donc un nombre réel fini.

    Nous vérifions que l'application $\| . \|$ de $\aL(V,W)$ dans $\eR$ ainsi définie est effectivement une norme.
    \begin{enumerate}
        \item
            $\|T\|_{\aL}=0$ signifie que $\|T(x)\|=0$ pour tout $x$ dans $V$. Comme  $\|\cdot\|_W$ est une norme nous concluons que $T(x)=0_{n}$ pour tout $x$ dans $V$, donc $T$ est l'application nulle. 
    \item
        Pour tout $a$ dans $\eR$ et tout  $T$ dans $\aL(V,W)$ nous avons
        \begin{equation}
            \|aT\|_{\mathcal{L}}=\sup_{\|x\|_{V}\leq 1}\|aT(x)\|_{W}=|a|\sup_{\|x\|_{V}\leq 1}\|T(x)\|_{W}=|a|\|T\|_{\mathcal{L}}.
        \end{equation}
    \item 
        Pour tous $T_1$ et $T_2$ dans $\aL(V,W)$ nous avons
      \begin{equation}\nonumber
        \begin{aligned}
           \|T_1+ T_2\|_{\mathcal{L}}&=\sup_{\|x\|\leq 1}\|T_1(x)+T_2(x)\|\leq\\
     &\leq\sup_{\|x\|\leq 1}\|T_1(x)\| +\sup_{\|x\|\leq 1}\|T_2(x)\|\\
     &=\|T_1\|\|T_2\|.
        \end{aligned}
      \end{equation}
    \end{enumerate}


    Enfin nous prouvons la formule alternative \eqref{EqFZPooIoecGH}. Nous allons montrer que les ensembles sur lesquels ont prend le supremum sont en réalité les mêmes :
    \begin{equation}
        \underbrace{\left\{ \frac{ \| Ax \| }{ \| x \| }\right\}_{x\neq 0}}_{A}=\underbrace{\left\{ \| Ax \|\tq \| x \|=1 \right\}}_{B}.
    \end{equation}
    Attention : ce sont des sous-ensembles de réels; pas de sous-ensembles de \( \eM(\eR)\) ou des sous-ensembles de \( \eR^n\).

    Pour la première inclusion, prenons un élément de \( A\), et prouvons qu'il est dans \( B\). C'est à dire que nous prenons \( x\in V\) et nous considérons le nombre \( \| Ax \|/\| x \|\). Le vecteur \( y=x/\| x \|\) est un vecteur de norme $1$, donc la norme de \( Ay\) est un élément de \( B\), mais
    \begin{equation}
        \| Ay \|=\frac{ \| Ax \| }{ \| x \| }.
    \end{equation}
    Nous avons donc \( A\subset B\).

    L'inclusion \( B\subset A\) est immédiate.
\end{proof}

La proposition \ref{PropNormeAppLineaire} nous permet de définir une famille de normes sur \( \aL(E,F)\) dès que \( E\) et \( F\) sont des espaces vectoriels normés.
\begin{definition}[Norme opérateur\cite{ooTZRDooWmjBJi}, thème \ref{THEMEooHSLLooBQpFAr}]          \label{DefNFYUooBZCPTr}
    Soit un opérateur \( A\) entre les espaces vectoriels normés \( E\) et \( F\). Le nombre
    \begin{equation}
        \| A \|_{\aL}=\sup_{x\in E}\frac{\|A(x)\|_{F}}{\|x\|_{E}} =\sup_{\|x\|_{E}=1}\|T(x)\|_{F}
    \end{equation}
    est la \defe{norme opérateur}{norme!d'application linéaire} de $A$. Nous disons que cette norme est \defe{subordonnée}{subordonnée!norme} aux normes choisies sur \( E\) et \( F\). 
\end{definition}
En d'autres termes, il y a autant de normes opérateur sur \( \aL(E,F)\) qu'il y a de paires de choix de normes sur \( E\) et \( F\). En particulier, cela donne lieu à toutes les normes \( \| A \|_p\) qui correspondent aux normes \( \| . \|_p\) sur \( \eR^n\). 

La norme opérateur est liée à la continuité par la proposition \ref{PropmEJjLE}.

Lorsque nous considérons un espace vectoriel d'applications linéaires, nous considérons toujours\footnote{Sauf lorsque les événements nous forceront à trahir.} dessus la topologie liée à cette norme. 

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Norme d'algèbre}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Norme d'algèbre\cite{ooTZRDooWmjBJi}]  \label{DefJWRWQue}
    Si \( A\) est une algèbre\footnote{Définition \ref{DefAEbnJqI}.}, une \defe{norme d'algèbre}{norme!d'algèbre} sur \( A\) est une norme telle que pour toute \( u,v\in A\),
    \begin{equation}
        \| uv \|\leq \| u \|\| v \|.
    \end{equation}
\end{definition}
L'intérêt d'une norme d'algèbre est entre autres de mieux se comporter pour les séries, voir par exemple \ref{subsecEVnZXgf}.

\begin{definition}[\cite{ooYLHAooCzQvoa}]      \label{DEFooEAUKooSsjqaL}
    Le \defe{rayon spectral}{rayon!spectral} d'une matrice carrée $A$, noté $\rho(A)$, est défini de la manière suivante :
    \begin{equation}
        \rho(A)=\max_i|\lambda_i|
    \end{equation}
    où les $\lambda_i$ sont les valeurs propres de $A$.
\end{definition}

\begin{definition}
    La \defe{topologie forte}{topologie!forte} sur l'espace des opérateurs est la topologie de la norme opérateur.  
\end{definition}

Il existe aussi la \defe{topologie faible}{topologie!faible} donnée par la notion de convergence\quext{Est-ce qu'on peut décrire cette topologie à partir de ses ouverts ? Facilement ?} \( A_i\to A\) si et seulement si \( A_ix\to Ax\) pour tout \( x\in E\).
    %TODO : il faut mettre au clair quelle est vraiment la topologie faible à partir des ouverts.

\begin{probleme}
    Je crois, mais demande confirmation, que la topologie faible est celle des semi normes \( \{ p_v \}_{v\in E}\) données par \( p_v(A)=\| A \|\). En effet la notion de convergence associée par la proposition \ref{PropQPzGKVk} est \( A_i\to A\) si et seulement si \( p_v(A_i-A)\to 0\). Cette condition signifie \( \| A_i(v)-A(v) \|\to 0\), c'est à dire \( A_i(v)\to A(v)\).

    Si le lecteur veut parler de cela au jury d'un concours, il est évident qu'il devra être capable d'ajouter des petits symboles au-dessus de toutes les flèches «\( \to\)» du paragraphe précédent pour indiquer pour quelles topologies sont les convergences dont on parle.
\end{probleme}

\begin{remark}
    Il faut noter que la topologie faible n'est pas une topologie métrique. Cela même si la condition \( A_ix\to Ax\), elle, est métrique vu qu'elle est écrite dans \( E\).
    et que dans le cas où \( E\) est de dimension infinie, elle est réellement différente de la topologie forte. Nous verrons à la sous-section \ref{subsecaeSywF} que dans le cas des projections sur un espaces de Hilbert, l'égalité
    \begin{equation}
        \sum_{i=1}^{\infty}\pr_{u_i}=\id
    \end{equation}
    est vraie pour la topologie faible, mais pas pour la topologie forte.
\end{remark}

\begin{definition}[\cite{ooAISYooXtUafT}]      \label{DEFooTLQUooJvknvi}

    Soient \( E\) et \( F\) deux espaces vectoriels normés.

    \begin{itemize}
        \item
            L'ensemble des applications linéaires \( E\to F\) est noté \( \aL(E,F)\).
        \item Un \defe{morphisme}{morphisme!espace vectoriel normé} est une application linéaire \( E\to F\) continue\footnote{Nous allons immédiatement voir, dans la proposition \ref{PropmEJjLE}\ref{ITEMooPEQJooFYONJX} qu'être continue est équivalent à être borné.} pour la topologie de la norme opérateur. L'ensemble des morphismes est noté \( \cL(E,F)\)\nomenclature[B]{\( \cL(E,F)\)}{applications linéaires bornées (continues)}.
        \item
            Un \defe{isomorphisme}{isomorphisme!espace vectoriel normé} est un morphisme continu inversible dont l'inverse est continu. Nous notons \( \GL(E,F)\) l'ensemble des isomorphismes entre \( E\) et \( F\).
    \end{itemize}
\end{definition}

Le point important de la définition \ref{DEFooTLQUooJvknvi} est la continuité. En dimension infine, la continuité n'est par exemple pas équivalente à l'inversibilité (penser à \( e_k\mapsto ke_k\)).

\begin{proposition}[\cite{GKPYTMb}] \label{PropmEJjLE} 
Inégalités sur la norme opérateur. Soient \( E\) et \( F\), des espaces vectoriels normés. Nous notons \( \| . \|\) les normes sur \( E\), \( F\), et nous notons également \( \| . \|\) la norme opérateur qui en découle sur \( \aL(E,F)\). 
    \begin{enumerate}       
        \item   \label{ITEMooLXGEooHoQHoE}
    Pour tout \( A\in \aL(E,F)\), et pour tout \( u\in E\) nous avons la majoration
    \begin{equation}
        \| Au \|\leq \| A \|\| u \|
    \end{equation}
    où la norme sur \( A\) est la norme opérateur subordonnée à la norme sur \( u\).


        \item\label{ITEMooHTVXooPNPQqN}
            La norme opérateur est une norme d'algèbre\footnote{Définition \ref{DefJWRWQue}.} : pourvu que \( A\) et \( B\) soient bornés nous avons :
    \begin{equation}
        \| AB \|\leq \| A \|\| B \|
    \end{equation}
    pour tout \( A,B\in\GL(E)\). 
\item       \label{ITEMooPEQJooFYONJX}
    Une application linéaire \( E\to F\) est continue si et seulement si elle est bornée.
\item       \label{ITEMooVQQFooWrHWeO}
    Le rayon spectral d'une matrice est plus petit que sa norme : \( \rho(A)\leq \| A \|\).

    \end{enumerate}
\end{proposition}

\begin{proof}
    Chaque point.
        \begin{subproof}

\item[Pour \ref{ITEMooLXGEooHoQHoE}]

    Si \( u\in E\) alors, étant donné que le supremum d'un ensemble est plus grand ou égal à tous les éléments qui le compose,
    \begin{equation}
        \| A \|=\sup_{x\in E}\frac{ \| Ax \| }{ \| x \| }\geq \frac{ \| Au \| }{ \| u \| },
    \end{equation}
    donc le résultat annoncé : \( \| Au \|\leq \| A \|\| u \|\).

        \item[Pour \ref{ITEMooHTVXooPNPQqN}]
            
    Nous avons
    \begin{subequations}
        \begin{align}
            \| AB \|&=\sup_{x\in E}\frac{ \| ABx \| }{ \| x \| }\\
            &=\sup_{\stackrel{x\in E}{Bx\neq 0}}\frac{ \| ABx \| }{ \| x \| }\\
            &=\sup_{\stackrel{x\in E}{Bx\neq 0}}\frac{ \| ABx \| }{ \| x \| }\frac{ \| Bx \| }{ \| Bx \| }\\
            &=\sup_{\stackrel{x\in E}{Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| }\frac{ \| Bx \| }{ \| x \| }\\
            &\leq\sup_{\stackrel{x\in E}{Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| } \sup_{\stackrel{y\in E}{By\neq 0}}\frac{ \| By \| }{ \| y \| }\\
            &\leq\| A \|\| B \|.
        \end{align}
    \end{subequations}
    La dernière inégalité provient que dans \( \sup_{\stackrel{x\in E}{Bx\neq 0}}\| ABx \|/\| Bx \|\) le supremum est fait sur un ensemble plus petit que celui sur lequel porte la définition de la norme de \( A\) : seulement l'image de \( B\) au lieu de tout l'espace de départ de \( A\).
    Nous avons donc bien \( \| AB \|\leq \| A \|\| B \|\).

\item[Pour \ref{ITEMooPEQJooFYONJX}]

    Nous commençons par supposer que \( A\) est bornée. Par la proposition \ref{PropmEJjLE}\ref{ITEMooHTVXooPNPQqN}, pour tout \( x,y\in E\), nous avons
    \begin{equation}
        \| A(x)-A(y) \|=\| A(x-y) \|\leq \| A \|\| x-y \|.
    \end{equation}
    En particulier si \( x_n\stackrel{E}{\longrightarrow}x\) alors
    \begin{equation}
        0\leq \| A(x_n)-A(x) \|\leq \| A \|\| x-x_n \|\to 0
    \end{equation}
    et \( A\) est continue en vertu de la caractérisation séquentielle de la continuité, proposition \ref{PropFnContParSuite}.

Supposons maintenant que \( \| A \|\) ne soit pas borné, c'est à dire que l'ensemble \( \{ \| A(x) \|\tq \| x \|=1 \}\) ne soit pas borné. Alors pour tout \( k\geq 1\) il existe \( x_k\in B(0,1)\) tel que \( \| A(x_k) \|>k\). La suite \( x_k/k\) tend vers zéro parce que \( \| x_k \|=1\), mais \( \| A(x_k) \|\geq 1\) pour tout \( k\). Cela montre que \( A\) n'est pas continue.

\item[Pour \ref{ITEMooVQQFooWrHWeO}]

    Soit \( \lambda\) une valeur propre réalisant le maximum : \( | \lambda |=\rho(A)\), et un vecteur propre \( u\) pour la valeur propre \( \lambda\) que nous supposons être de norme \( 1\) : \( \| u \|=1\). En prenant la norme dans \( Au=\lambda u\) et en tenant compte de \ref{ITEMooLXGEooHoQHoE} :
        \begin{equation}
            | \lambda |\| u \|=\| Au \|\leq \| A \|\| u \|.
        \end{equation}
        En tenant compte de \( \| u \|=1\) et \( | \lambda |=\rho(A)\),
        \begin{equation}
            \rho(A)=| \lambda |\leq \| A \|.
        \end{equation}
        \end{subproof}
    
\end{proof}
Notons qu'en réalité nous n'avons utilisé seulement le fait que \( B\) était surjective

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Application linéaire continue et bornée}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons vu dans la proposition \ref{PropmEJjLE}\ref{ITEMooPEQJooFYONJX} que pour une application linéaire, être bornée est équivalent à être continue. Nous allons maintenant voir un certain nombre d'exemples illustrant ce fait.

\begin{example}[Une application linéaire non continue]  \label{ExHKsIelG}
    Soit \( V\) l'espace vectoriel normé des suites \emph{finies} de réels muni de la norme usuelle $\| c \|=\sqrt{\sum_{i=0}^{\infty}| c_i |^2}$ où la somme est finie. Nous nommons \( \{ e_k \}_{k\in \eN}\) la base usuelle de cet espace, et nous considérons l'opérateur \( f\colon V\to V\) donnée par \( f(e_k)=ke_k\). C'est évidemment linéaire, mais ce n'est pas continu en zéro. En effet la suite \( u_k=e_k/k\) converge vers \( 0\) alors que \( f(u_k)=e_k\) ne converge pas.
\end{example}

Cet exemple aurait pu également être donnée dans un espace de Hilbert, mais il aurait fallu parler de domaine.
%TODO : le faire, et regarder si Hilbet n'est pas la complétion de cet espace. Référencer à l'endroit qui définit l'espace vectoriel librement engendré. Ici ce serait par N.

%TODO : dire qu'une application bilinéaire sur RxR n'est pas une application linéaire sur R^2

\begin{example}[Une autre application linéaire non continue\cite{GTkeGni}]      \label{EXooDMVJooAJywMU}
    En dimension infinie, une application linéaire n'est pas toujours continue. Soit \( E\) l'espace des polynômes à coefficients réels sur \( \mathopen[ 0 , 1 \mathclose]\) muni de la norme uniforme. L'application de dérivation \( \varphi\colon E\to E\), \( \varphi(P)=P'\) n'est pas continue.

    Pour la voir nous considérons la suite \( P_n=\frac{1}{ n }X^n\). D'une part nous avons \( P_n\to 0\) dans \( E\) parce que \( P_n(x)=\frac{ x^n }{ n }\) avec \( x\in \mathopen[ 0 , 1 \mathclose]\). Mais en même temps nous avons \( \varphi(P_n)=X^{n-1}\) et donc \( \| \varphi(P_n) \|=1\).

    Nous n'avons donc pas \( \lim_{n\to \infty} \varphi(P_n)=\varphi(\lim_{n\to \infty} P_n)\) et l'application \( \varphi\) n'est pas continue en \( 0\). Elle n'est donc continue nulle part par linéarité.

    Nous avons utilisé le critère séquentiel de la continuité, voir la définition \ref{DefENioICV} et la proposition \ref{PropFnContParSuite}.
\end{example}


\begin{remark}  \label{RemOAXNooSMTDuN}
Cette proposition permet de retrouver l'exemple \ref{ExHKsIelG} plus simplement. Si \( \{ e_k \}_{k\in \eN}\) est une base d'un espace vectoriel normé formée de vecteurs de norme \( 1\), alors l'opérateur linéaire donné par \( u(e_k)=ke_k\) n'est pas borné et donc pas continu.
\end{remark}

C'est également ce résultat qui montre que le produit scalaire est continu sur un espace de Hilbert par exemple.

\begin{lemma}   \label{LemWWXVSae}
Soit \( F\) un espace de Banach et deux suites \( A_k\to A\) et \( B_k\to B\) dans \( \aL(F,F)\). Alors \( A_k\circ B_k\to A\circ B\) dans \( \aL(F,F)\), c'est à dire
\begin{equation}
    \lim_{n\to \infty} (A_kB_k)=\left( \lim_{n\to \infty} A_k \right)\left( \lim_{n\to \infty} B_k \right).
\end{equation}
\end{lemma}

\begin{proof}
    Il suffit d'écrire
    \begin{equation}
        \| A_kB_k-AB \|\leq \| A_kB_k-A_kB \|+\| A_kB-AB \|.
    \end{equation}
    Le premier terme tend vers zéro pour \( k\to\infty\) parce que 
    \begin{subequations}
        \begin{align}
            \| A_kB_k-A_kB \|&=\| A_k(B_k-B) \|\\
            &\leq \| A_k \|\| B_k-B \|\to \| A \|\cdot 0\\
            &=0
        \end{align}
    \end{subequations}
    où nous avons utilisé la propriété fondamentale de la norme opérateur : la proposition \ref{PropmEJjLE}. Le second terme tend également vers zéro pour la même raison.
\end{proof}

\begin{proposition}[Distributivité de la somme infinie] \label{PropQXqEPuG}
    Soient \( E\) un espace normé, une suite \( (u_k)\) dans \( \GL(E)\) ainsi que \( a\in\GL(E)\). Pourvu que la série \( \sum_{n=0}^{\infty}u_k\) converge nous avons
    \begin{equation}
        \left( \sum_{k=0}^{\infty}u_k \right)a=\sum_{k=0}^{\infty}(u_ka).
    \end{equation}
\end{proposition}

\begin{proof}
    Par définition de la somme infinie,
    \begin{equation}
        \spadesuit=\left( \sum_{k=0}^{\infty}u_k \right)a=\left( \lim_{n\to \infty} \sum_{k=0}^nu_k \right)a.
    \end{equation}
    Le lemme \ref{LemWWXVSae} appliqué à \( n\mapsto\sum_{k=0}^nu_k\) et à la suite constante \( a\) nous donne
    \begin{equation}    \label{EqOAoopjz}
        \spadesuit=\lim_{n\to \infty} \left( \sum_{k=0}u_ka \right),
    \end{equation}
    ce que nous voulions par distributivité de la somme finie : dans \eqref{EqOAoopjz}, le \( a\) est dans ou hors de la somme, au choix. L'important est qu'il soit dans la limite.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Normes de matrices et d'applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecNomrApplLin}

\begin{theorem}[Norme matricielle et rayon spectral\cite{ooBCKVooVunKyT}]       \label{THOooNDQSooOUWQrK}
    La norme $2$ d'une matrice est liée au rayon spectral de la façon suivante :
    \begin{equation}
        \|A\|_2=\sqrt{\rho(A{^t}A)}
    \end{equation}
    ou plus généralement par \( \| A \|_2=\sqrt{\rho(A^*A)}\).
\end{theorem}

\begin{proposition} \label{PropMAQoKAg}
    La fonction
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eR)\times \eM(n,\eR)&\to \eR \\
            (X,Y)&\mapsto \tr(X^tY) 
        \end{aligned}
    \end{equation}
    est un produit scalaire sur \( \eM(n,\eR)\).
\end{proposition}
\index{trace!produit scalaire sur \( \eM(n,\eR)\)}
\index{produit!scalaire!sur \( \eM(n,\eR)\)}

\begin{proof}
    Il faut vérifier la définition \ref{DefVJIeTFj}.
    \begin{itemize}
        \item La bilinéairité est la linéarité de la trace.
        \item La symétrie de \( f\) est le fait que \( \tr(A^t)=\tr(A)\).
        \item L'application \( f\) est définie positive parce que si \( X\in \eM\), alors \( X^tX\) est symétrique définie positive, donc diagonalisable avec des nombres positifs sur la diagonale. La trace étant un invariant de similitude, nous avons \( f(X,X)=\tr(X^tX)\geq 0\). De plus si \( \tr(X^tX)=0\), alors \( X^tX=0\) (pour la même raison de diagonalisation). Mais alors \( \| Xu \|=0\) pour tout \( u\in E\), ce qui signifie que \( X=0\).
    \end{itemize}
\end{proof}

\begin{example}
	Soit $m=n$, un point $\lambda$ dans $\eR$ et $T_{\lambda}$ l'application linéaire définie par $T_{\lambda}(x)=\lambda x$. La norme de $T_{\lambda}$ est alors
\[
\|T_{\lambda}\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|\lambda x\|_{\eR^n}= |\lambda|.
\]
Notez que $T_{\lambda}$ n'est rien d'autre que l'homothétie de rapport $\lambda$ dans $\eR^m$.
\end{example}

\begin{example}
	Considérons la rotation $T_{\alpha}$ d'angle $\alpha$ dans $\eR^2$. Elle est donnée par l'équation matricielle
	\begin{equation}
		T_{\alpha}\begin{pmatrix}
			x	\\ 
			y	
		\end{pmatrix}=\begin{pmatrix}
			\cos\alpha	&	\sin\alpha	\\ 
			-\sin\alpha	&	\cos\alpha	
		\end{pmatrix}\begin{pmatrix}
			x	\\ 
			y	
		\end{pmatrix}=\begin{pmatrix}
			\cos(\alpha)x+\sin(\alpha)y	\\ 
			-\sin(\alpha)x+\cos(\alpha)y	
		\end{pmatrix}
	\end{equation}
	Étant donné que cela est une rotation, c'est une isométrie : $\| T_{\alpha}x \|=\| x \|$. En ce qui concerne la norme de $T_{\alpha}$ nous avons
	\begin{equation}
		\| T_{\alpha} \|=\sup_{x\in\eR^2}\frac{ \| T_{\alpha}(x) \| }{ \| x \| }=\sup_{x\in\eR^2}\frac{ \| x \| }{ \| x \| }=1.
	\end{equation}
	Toutes les rotations dans le plan ont donc une norme $1$. La même preuve tient pour toutes les rotations en dimension quelconque. 
\end{example}

%TODO : le théorème de fuite des compacts qui dit qu'une solution de y'=f(y,t) cesse d'exister seulement si elle tend vers +- infini.

\begin{example}
  Soit $m=n$, un point $b$ dans $\eR^m$ et $T_b$ l'application linéaire définie par $T_b(x)=b\cdot x$ (petit exercice : vérifiez qu'il s'agit vraiment d'une application linéaire).  La norme de $T_b$ satisfait les inégalités suivantes 
 \[
\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\leq \sup_{\|x\|_{\eR^m}\leq 1}\|b \|_{\eR^n}\|x\cdot x\|_{\eR^n}\leq\|b \|_{\eR^n},
\]
\[
\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\geq \left\|b\cdot \frac{b}{\|b \|_{\eR^n}}\right\|_{\eR^n}=\|b \|_{\eR^n},
\]
donc $\|T_b\|_{\mathcal{L}}=\|b \|_{\eR^n}$.
\end{example}

\begin{proposition}
    Une application linéaire de \( \eR^m\) dans \( \eR^n\) est continue.
\end{proposition}

\begin{proof}
      Soit $x$ un point dans $\eR^m$. Nous devons vérifier l'égalité
      \begin{equation}
       \lim_{h\to 0_m}T(x+h)=T(x).
      \end{equation}
      Cela revient à prouver que $\lim_{h\to 0_m}T(h)=0$, parce que $T(x+h)=T(x)+T(h)$. Nous pouvons toujours majorer $\|T(h)\|_n$ par $\|T\|_{\mathcal{L}(\eR^m,\eR^n)}\| h \|_{\eR^m}$ (proposition \ref{PropmEJjLE}\ref{ITEMooLXGEooHoQHoE}). Quand $h$ s'approche de $ 0_m $ sa norme $\|h\|_m$ tend vers $0$, ce que nous permet de conclure parce que nous savons que de toutes façons, $\| T \|_{\aL}$ est fini.
\end{proof}

Note : dans un espace de dimension infinie, la linéarité ne suffit pas pour avoir la continuité : il faut de plus être borné (ce que sont toutes les applications linéaires \( \eR^m\to\eR^n\)). Voir la proposition \ref{PropmEJjLE}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Produit vectoriel}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooIKGNooSrlXTL}

\begin{definition}      \label{DEFooTNTNooRjhuJZ}
	Soient $u$ et $v$, deux vecteurs de $\eR^3$. Le \defe{produit vectoriel}{produit!vectoriel} de $u$ et $v$ est le vecteur $u\times v$ défini par 
	\begin{equation}
		\begin{aligned}[]
		u\times v&=\begin{vmatrix}
			e_1	&	e_2	&	e_3	\\
			u_1	&	u_2	&	u_3	\\
			v_1	&	v_2	&	v_3
		\end{vmatrix}\\
		&=
		(u_2v_3-u_3v_2)e_1+(u_3v_1-u_1v_3)e_2+(u_1v_2-u_2v_1)e_3
		\end{aligned}
	\end{equation}
	où les vecteurs $e_1$, $e_2$ et $e_3$ sont les vecteurs de la base canonique de $\eR^3$.
\end{definition}
La notion de produit vectoriel est propre à $\eR^3$; il n'y a pas de généralisation simple aux espaces $\eR^m$.

Nous n'allons pas nous attarder sur les nombreuses propriétés du produit vectoriel. Les principales sont résumées dans la proposition suivante.
\begin{proposition}
	Si $u$ et $v$ sont des vecteurs de $\eR^3$, alors le vecteur $u\times v$ est l'unique vecteur qui est perpendiculaire à $u$ et $v$ en même temps, de norme égal à la surface du parallélogramme construit sur $u$ et $v$ et tel que les vecteurs $u$, $v$, $u\times v$ forment une base dextrogyre.
\end{proposition}
La chose importante à retenir est que le produit vectoriel permet de construire un vecteur simultanément perpendiculaire à deux vecteurs donnés. Le vecteur $u\times v$ est donc linéairement indépendant de $u$ et $v$. En pratique, si $u$ et $v$ sont déjà linéairement indépendants, alors le produit vectoriel permet de compléter une base de $\eR^3$.

À l'aide du produit vectoriel et du produit scalaire, nous construisons le \defe{produit mixte}{produit!mixte} de trois vecteurs de $\eR^3$ par la formule
\begin{equation}
	(u\times v)\cdot w=\begin{vmatrix}
			u_1	&	u_2	&	u_3	\\
			v_1	&	v_2	&	v_3	\\
			w_1	&	w_2	&	w_3	
	\end{vmatrix}.
\end{equation}

Pourquoi nous ne considérons pas la combinaison $(u\cdot v)\times w$ ?

\begin{proposition}		 \label{PropScalMixtLin}
	Les applications produit scalaire, vectoriel et mixte sont multilinéaires. Spécifiquement, nous avons les propriétés suivantes.
	\begin{enumerate}
		\item
			Les applications produit scalaire et vectoriel sont bilinéaires. Le produit mixte est trilinéaire.
		\item
			Le produit vectoriel est antisymétrique, c'est à dire $u\times v=-v\times u$.
		\item
			Nous avons $u\times v=0$ si et seulement si $u$ et $v$ sont colinéaires, c'est à dire si et seulement si l'équation $\alpha u+\beta v=0$ a une solution différente de la solution triviale $(\alpha,\beta)=(0,0)$.
		\item		\label{ItemPropScalMixtLiniv}
			Pour tout $u$ et $v$ dans $\eR^3$, nous avons
			\begin{equation}
				\langle u, v\rangle^2 +\| u\times v \|^2=\| u \|^2\| v \|^2
			\end{equation}
		\item
			Par rapport à la dérivation, le produit scalaire et vectoriel vérifient une règle de Leibnitz. Soit $I$ un intervalle de $\eR$, et si $u$ et $u$ sont dans $C^1(I,\eR^3)$, alors
			\begin{equation}		\label{EqFormLeibProdscalVect}
				\begin{aligned}[]
					\frac{ d }{ dt }\big( u(t)\cdot v(t) \big)&=\big( u'(t)\cdot v(t) \big)+\big( u(t)\cdot v'(t) \big)\\
					\frac{ d }{ dt }\big( u(t)\times v(t) \big)&=\big( u'(t)\times v(t) \big)+\big( u(t)\times v'(t) \big).
				\end{aligned}
			\end{equation}
		\end{enumerate}
\end{proposition}

Les deux formules suivantes, qui mêlent le produit scalaire et le produit vectoriel, sont souvent utiles en analyse vectorielle :
\begin{equation}
	\begin{aligned}[]
		(u\times v)\cdot w&=u\cdot(v\times w)\\
		(u\times v)\times w&=-(v\cdot w)u+(u\cdot w)v		\label{EqFormExpluxxx}
	\end{aligned}
\end{equation}
pour tout vecteurs $u$, $v$ et $w$ dans $\eR^3$. Nous les admettons sans démonstration. La seconde formule est parfois appelée \defe{formule d'expulsion}{formule!d'expulsion (produit vectoriel)}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit vectoriel}
%---------------------------------------------------------------------------------------------------------------------------

Une application importante du déterminant $3\times 3$ est qu'il détermine le \defe{produit vectoriel}{produit!vectoriel} entre deux vecteurs. Pour cela nous introduisons les vecteurs de base
\begin{equation}
    \begin{aligned}[]
        e_x&=\begin{pmatrix}
            1    \\ 
            0    \\ 
            0    
        \end{pmatrix}
        ,&e_y=\begin{pmatrix}
            0    \\ 
            1    \\ 
            0    
        \end{pmatrix},&e_z=\begin{pmatrix}
            0    \\ 
            0    \\ 
            1    
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Ensuite, si $v$ et $w$ sont des vecteurs dans $\eR^3$, nous définissons
\begin{equation}
    \begin{aligned}[]
        \begin{pmatrix}
            v_x    \\ 
            v_y    \\ 
            v_z    
        \end{pmatrix}\times\begin{pmatrix}
            w_x    \\ 
            w_y    \\ 
            w_z    
        \end{pmatrix}=
        \begin{vmatrix}
              e_x  &   e_y    &   e_z    \\
              v_x  &   v_y    &   v_z    \\
              w_x  &   w_y    &   w_z    \\
        \end{vmatrix}&=
        (v_yw_z-w_yvz)e_x\\
        &-(v_xw_z-w_xvz)e_y\\
        &+(v_xw_y-w_xvy)e_z\in\eR^3
    \end{aligned}
\end{equation}

Ce produit vectoriel peut aussi être écrit sous la forme
\begin{equation}        \label{EqProdVectEspilonijk}
    v\times w=\sum_{i,j,k}\epsilon_{ijk}v_iw_j1_k.
\end{equation}
où $\epsilon_{ijk}$ est défini par $\epsilon_{xyz}=1$ et ensuite $\epsilon_{ijk}$ est $1$ ou $-1$ suivant que la permutation des $x$, $y$ et $z$ est paire ou impaire. C'est à dire que \( \epsilon_{ijk}\) est la signature de la permutation qui amène \( (1,2,3)\) sur \( (i,j,k)\).


\begin{proposition}
    Le produit vectoriel est une opération antisymétrique, c'est à dire
    \begin{equation}
        v\times w=-w\times v.
    \end{equation}
    En particulier $v\times v=0$ pour tout vecteur $v\in\eR^3$.
\end{proposition}

\begin{proposition}
    Le produit vectoriel est linéaire. Pour tout vecteurs $a$, $b$, $c$ et pour tout nombre $\alpha$ et $\beta$ nous avons
    \begin{equation}
        \begin{aligned}[]
            a\times (\alpha b +\beta c)&=\alpha(a\times b)+\beta(a\times c)\\
            (\alpha a+\beta b)\times c&=\alpha(a\times c)+\beta(b\times c).
        \end{aligned}
    \end{equation}
\end{proposition}

Les trois vecteurs de base $e_x$, $e_y$ et $e_y$ ont des produits vectoriels faciles à retenir :
\begin{equation}
    \begin{aligned}[]
        e_x\times e_y&=e_z\\
        e_y\times e_z&=e_x\\
        e_z\times e_x&=e_y
    \end{aligned}
\end{equation}

\begin{example}
    Calculons le produit vectoriel $v\times w$ avec
    \begin{equation}
        \begin{aligned}[]
            v&=\begin{pmatrix}
                3    \\ 
                -1    \\ 
                1    
            \end{pmatrix}&w=\begin{pmatrix}
                1    \\ 
                2    \\ 
                -1    
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Les vecteurs s'écrivent sous la forme $v=3e_x-e_y+e_z$ et $w=e_x+2e_y-e_z$. Le produit vectoriel s'écrit
    \begin{equation}
        \begin{aligned}[]
            (3e_x-e_y+e_z)\times (e_x+2e_y-e_z)&=6e_x\times e_y-3e_x\times e_z\\
                                &\quad -e_y\times e_x + e_y\times e_z\\
                                &\quad + e_z\times e_x + 2e_z\times e_y\\
                                &=6e_z+3e_y+e_z+e_x+e_y-2e_x\\
                                &=-e_x+4e_y+7e_z.
        \end{aligned}
    \end{equation}
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit mixte}
%---------------------------------------------------------------------------------------------------------------------------

Si $a$, $b$ et $c$ sont trois vecteurs, leur \defe{produit mixte}{produit!mixte} est le nombre $a\cdot(b\times c)$. En écrivant le produit vectoriel sous forme de somme de trois déterminants $2\times 2$, nous avons
\begin{equation}
    \begin{aligned}[]
        a\cdot& (b\times c)\\&=(a_1e_x+a_2e_y+a_3e_z)\cdot\left(
        \begin{vmatrix}
            b_2    &   b_3    \\ 
            c_2    &   c_3    
        \end{vmatrix}e_x-\begin{vmatrix}
            b_1    &   b_3    \\ 
            c_1    &   c_3    
        \end{vmatrix}e_y+\begin{vmatrix}
            b_1    &   b_2    \\ 
            c_1    &   c_2    
        \end{vmatrix}\right)\\
        &=a_1\begin{vmatrix}
            b_2    &   b_3    \\ 
            c_2    &   c_3    
        \end{vmatrix}-a_2\begin{vmatrix}
            b_1    &   b_3    \\ 
            c_1    &   c_3    
        \end{vmatrix}+a_3\begin{vmatrix}
            b_1    &   b_2    \\ 
            c_1    &   c_2    
        \end{vmatrix}\\
        &=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3    \\
            c_1    &   c_2    &   c_3
        \end{vmatrix}.
    \end{aligned}
\end{equation}
Le produit mixte s'écrit donc sous forme d'un déterminant. Nous retenons cette formule:
\begin{equation}        \label{EqProduitMixteDet}
    a\cdot (b\times c)=\begin{vmatrix}
        a_1    &   a_2    &   a_3    \\
        b_1    &   b_2    &   b_3    \\
        c_1    &   c_2    &   c_3
    \end{vmatrix}.
\end{equation}

Un grand intérêt du produit vectoriel est qu'il fournit un vecteur qui est simultanément perpendiculaire aux deux vecteurs donnés.
\begin{proposition}     \label{PROPooTUVKooOQXKKl}
    Le produit vectoriel $a\times b$ est un vecteur orthogonal à $a$ et $b$.
\end{proposition}

\begin{proof}
    Vérifions que $a\perp (a\times b)$. Pour cela, nous calculons $a\cdot (a\times b)$, c'est à dire le produit mixte
    \begin{equation}
        a\cdot(a\times b)=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3
        \end{vmatrix}=0.
    \end{equation}
    L'annulation de ce déterminant est due au fait que deux de ses lignes sont égales.
\end{proof}

Ces résultats admettent une intéressante généralisation.
\begin{lemma}       \label{LEMooFRWKooVloCSM}
    Soit \( X\in \eR^n\) ainsi que \( v_1,\ldots, v_{n-1}\in \eR^n\). Alors
    \begin{enumerate}
        \item
            Nous avons
            \begin{equation}        \label{EQooMQNPooRHHBjz}
                \det(X,v_1,\ldots, v_{n-1})=X\cdot 
                \det\begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\ 
                        &   v_{n-1}    &   
                 \end{pmatrix}
            \end{equation}
        \item
            Le vecteur
            \begin{equation}
                \det\begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\ 
                        &   v_{n-1}    &   
                 \end{pmatrix}
            \end{equation}
            est orthogonal à tous les \( v_i\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Vu que les deux côtés de \eqref{EQooMQNPooRHHBjz} vus comme fonctions de \( X\), sont des applications linéaires de \( \eR^n\) dans \( \eR\), il suffit de vérifier l'égalité sur une base.

    Nous posons \( \tau_i\colon \eR^n\to \eR^{n-1}\),
    \begin{equation}
        \tau_i(v)_k=\begin{cases}
            v_k    &   \text{si } k<i\\
            v_{k+1}    &    \text{si } k\geq i\text{.}
        \end{cases}
    \end{equation}
    et nous avons d'une part
    \begin{equation}
        e_k\cdot
                \det
                \begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\ 
                        &   v_{n-1}    &   
                 \end{pmatrix}
                 =\det\begin{pmatrix}
                     \tau_kv_1   \\ 
                     \vdots   \\ 
                     \tau_kv_{n-1}   
                 \end{pmatrix}
            \end{equation}
     et d'autre part,
     \begin{equation}
         \det(e_k,v_1,\ldots, v_{n-1})=\det
         \begin{pmatrix}
             0&&&\\  
             \vdots&&&\\  
             1&v_1&\cdots&v_{n-1}\\  
             \vdots&&&\\  
             0&&&  
         \end{pmatrix}=\det(\tau_k v_1,\ldots, \tau_k v_{n-1}).
     \end{equation}
     La première assertion est démontrée.

     En ce qui concerne la seconde, il suffit d'appliquer la première et se souvenir qu'un déterminant est nul lorsque deux lignes sont égales. En effet :
     \begin{equation}
         v_k\cdot \det
                \begin{pmatrix}
                     e_1   &   \ldots    &   e_n    \\
                        &   v_1    &       \\
                        &   \vdots    &       \\ 
                        &   v_{n-1}    &   
                 \end{pmatrix}
                 =
                 \det(v_k,v_1,\ldots, v_n)=0.
     \end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Topologie}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Boules et sphères}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
	Soit $(V,\| . \|)$, un espace vectoriel normé, $a\in V$ et $r>0$. Nous allons abondamment nous servir des ensembles suivants :
	\begin{enumerate}

		\item
			la \defe{boule ouverte}{boule!ouverte} $B(a,r)=\{ x\in V\tq \| x-a \|<r \}$;
		\item
			la \defe{boule fermée}{boule!fermée} $\bar B(a,r)=\{ x\in V\tq \| x-a \|\leq r \}$;
		\item
			la \defe{sphère}{sphère} $S(a,r)=\{ x\in V\tq \| x-a \|=r \}$.

	\end{enumerate}
\end{definition}
Les différences entre ces trois ensembles sont très importantes. D'abord, les \emph{boules} sont pleines tandis que la \emph{sphère} est creuse. En comparant à une pomme, la boule ouverte serait la pomme «sans la peau», la boule fermée serait «avec la peau» tandis que la sphère serait seulement la peau. Nous avons
\begin{equation}
	\bar B(a,r)=B(a,r)\cup S(a,r).
\end{equation}

\begin{definition}
	Une partie $A$ de $V$ est dite \defe{bornée}{borné!partie de $V$} s'il existe un réel $R$ tel que $A\subset B(0_V,R)$.
\end{definition}
Une partie est donc bornée si elle est contenue dans une boule de rayon fini.

\begin{example}
	Dans $\eR$, les boules sont  les intervalles ouverts et fermés tandis que la sphère est donnée par les points extrêmes des intervalles :
	\begin{equation}
		\begin{aligned}[]
			B(a,r)&=\mathopen] a-r , a+r \mathclose[,\\
			\bar B(a,r)&=\mathopen[ a-r , a+b \mathclose],\\
			S(a,r)&=\{ a-r,a+r \}.
		\end{aligned}
	\end{equation}
\end{example}

\begin{example}
	Si nous considérons $\eR^2$, la situation est plus riche parce que nous avons plus de normes. Essayons de voir les sphères de centre $(0,0)\in\eR^2$ et de rayon $r$ pour les normes $\| . \|_1$, $\| . \|_2$ et $\| . \|_{\infty}$.

	Pour la norme $\| . \|_1$, la sphère de rayon $r$ est donnée par l'équation
	\begin{equation}
		| x |+| y |=r.
	\end{equation}
	Pour la norme $\| . \|_2$, l'équation de la sphère de rayon $r$ est
	\begin{equation}
		\sqrt{x^2+y^2}=r,
	\end{equation}
	et pour la norme supremum, la sphère de rayon $r$ a pour équation
	\begin{equation}
		\max\{ | x |,| y | \}=r.
	\end{equation}
	Elles sont dessinées sur la figure \ref{LabelFigLesSpheres}
\newcommand{\CaptionFigLesSpheres}{Les sphères de rayon $1$ pour les trois normes classiques.}
\input{auto/pictures_tex/Fig_LesSpheres.pstricks}
\end{example}

\newcommand{\CaptionFigBoulePtLoin}{Le point $P$ est un peu plus loin que $x$, en suivant la même droite.}
\input{auto/pictures_tex/Fig_BoulePtLoin.pstricks}

\begin{proposition}		\label{PropBoitPtLoin}
	Soient $V$ un espace vectoriel normé, $a$ dans $V$ et $x$ tel que $d(a,x)=r$, c'est à dire $x\in S(a,r)$. Dans ce cas, toute boule centrée en $x$ contient un point $P$ tel que $d(P,a)>r$ et un point $Q$ tel que $d(Q,a)<r$.
\end{proposition}

\begin{proof}
	Soit une boule de rayon $\delta$ autour de $x$. Le but est de trouver un point $P$ tel que $d(P,a)>r$ et $d(P,x)<\delta$. Pour cela, nous prenons $P$ sur la même droite que $x$ (en partant de $a$), mais juste «un peu plus loin» (voir figure \ref{LabelFigBoulePtLoin}). Plus précisément, nous considérons le point
	\begin{equation}
		P=x+\frac{ v }{ N }
	\end{equation}
	où $v=x-a$ et $N$ est suffisamment grand pour que $d(x,P)$ soit plus petit que $\delta$. Cela est toujours possible parce que
	\begin{equation}
		d(P,x)=\| P-x \|=\frac{ \| v \| }{ N }
	\end{equation}
	peut être rendu aussi petit que l'on veut par un choix approprié de $N$. Montrons maintenant que $d(a,P)>d(a,x)$ :
	\begin{equation}
		\begin{aligned}[]
			d(a,P)&=\| a-x-\frac{ v }{ N }\| \\
			&=\| a-x+\frac{ a }{ N }-\frac{ x }{ N } \|\\
			&=\| \big( 1+\frac{1}{ N }(a-x) \big) \|\\
			&>\| a-x \|=d(a,x).
		\end{aligned}
	\end{equation}
	Nous laissons en exercice le soin de trouver un point $Q$ tel que $d(Q,a)<r$ et $d(Q,x)<\delta$.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Ouverts, fermés, intérieur et adhérence}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
	Soit $(V,\| . \|)$ un espace vectoriel normé et $A$, une partie de $V$. Un point $a$ est dit \defe{intérieur}{intérieur!point} à $A$ s'il existe une boule ouverte centrée en $a$ et contenue dans $A$.

	On appelle \defe{l'intérieur}{intérieur!d'un ensemble} de $A$ l'ensemble des points qui sont intérieurs à $A$. Nous notons $\Int(A)$ l'intérieur de $A$.
\end{definition}
Notons que $\Int(A)\subset A$ parce que si $a\in\Int(A)$, nous avons $B(a,r)\subset A$ pour un certain $r$ et en particulier $a\in A$.

\begin{example}
	Trouver l'intérieur d'un intervalle dans $\eR$ consiste à «ouvrir là où c'est fermé». 
	\begin{enumerate}

		\item
			$\Int\big(\mathopen[ 0 , 1 [\big)=\mathopen] 0 , 1 \mathclose[$. 
			
			Prouvons d'abord que $\mathopen] 0,1  \mathclose[\subset\Int(\mathopen[ 0 , 1 [)$. Si $a\in\mathopen] 0 , 1 \mathclose[$, alors $a$ est strictement supérieur à $0$ et strictement inférieur à $1$. Dans ce cas, la boule de centre $a$ et de rayon $\frac{ \min\{ a,1-a \} }{ 2 }$ est contenue dans $\mathopen] 0 , 1 \mathclose[$ (voir figure \ref{LabelFigIntervalleUn}). Cela prouve que $a$ est dans l'intérieur de $\mathopen[ 0 , 1 [$.

\newcommand{\CaptionFigIntervalleUn}{Trouver le rayon d'une boule autour de $a$. Une boule qui serait centrée en $a$ avec un rayon strictement plus petit à la fois de $a$ et de $1-a$ est entièrement contenue dans le segment $\mathopen] 0 , 1 \mathclose[$.}
\input{auto/pictures_tex/Fig_IntervalleUn.pstricks}

			Prouvons maintenant que $\Int\big( \mathopen[ 0 , 1 [ \big)\subset\mathopen] 0 , 1 \mathclose[$. Vu que l'intérieur d'un ensemble est inclus à l'ensemble, nous savons déjà que $\Int\big( \mathopen[ 0 , 1 [ \big)\subset\mathopen[ 0 , 1 [$. Nous devons donc seulement montrer que $0$ n'est pas dans l'intérieur de $\mathopen[ 0 , 1 [$. C'est le cas parce que toute boule du type $B(0,r)$ contient le point $-r/2$ qui n'est pas dans $\mathopen[ 0 , 1 [$.

		\item
			$\Int\Big( \mathopen[ 0 , \infty [ \Big)=\mathopen] 0 , \infty \mathclose[$.
		\item
			$\Int\big( \mathopen] 2 , 3 \mathclose[ \big)=\mathopen] 2 , 3 \mathclose[$.

	\end{enumerate}
	
\end{example}

\begin{example}			\label{ExempleIntBoules}
	Les intérieurs des boules et sphères sont importantes à savoir.
	\begin{enumerate}
		\item 
			$\Int\big( B(a,r) \big)=B(a,r)$. Si $x\in B(a,r)$, nous avons $d(a,x)<r$. Alors la boule $B\big(x,r-d(x,a)\big)$ est incluse à $B(a,r)$, et donc $x$ est dans l'intérieur de $B(a,r)$. Conseil : faire un dessin.
		\item
			$\Int\big( \bar B(a,r) \big)=B(a,r)$. Par le point précédent, la boule $B(a,r)$ est certainement dans l'intérieur de la boule fermée. Il reste à montrer que les points de $\bar B(a,r)$ qui ne sont pas dans $B(a,r)$ ne sont pas dans l'intérieur. Ces points sont ceux dont la distance à $a$ est \emph{égale} à $r$. Le résultat découle alors de la proposition \ref{PropBoitPtLoin}.
			
		\item
			$\Int\big( S(a,r) \big)=\emptyset$. Si $x\in S(a,r)$, toute boule centrée en $a$ contient des points qui ne sont pas à distance $r$ de $a$.
			
			Notez que la sphère est un exemple d'ensemble non vide mais d'intérieur vide.
	\end{enumerate}
\end{example}


\begin{definition}
	Une partie $A$ de l'espace vectoriel normé $(V,\| . \|)$ est dite \defe{ouverte}{ouvert} si chacun de ses points est intérieur. La partie $A$ est donc ouverte si $A\subset\Int(A)$. Par convention, nous disons que l'ensemble vide $\emptyset$ est ouvert.

	Une partie est dite \defe{fermée}{fermé} si son complémentaire est ouvert. La partie $A$ est donc fermée si $V\setminus A$ est ouverte.
\end{definition}

Remarque : un ensemble $A$ est ouvert si et seulement si $\Int(A)=A$.

\begin{definition}
	Une partie $A$ de l'espace vectoriel normé $V$ est dite \defe{compacte}{compact} si elle est fermée et bornée.
\end{definition}

Nous verrons tout au long de ce cours que les ensembles compacts, et les fonctions définies sur ces ensembles ont de nombreuses propriétés agraables.

\begin{example}		\label{ExempleFermeIntevrR}
	En ce qui concerne les intervalles de $\eR$,
	\begin{itemize}
		\item $\mathopen] 1 , 2 \mathclose[$ est ouvert;
		\item $\mathopen[ 3,  4 \mathclose]$ est fermé;
		\item $\mathopen[ 5 , 6 [$ n'est ni ouvert ni fermé;
	\end{itemize}
	Les intervalles fermés de $\eR$ sont toujours compacts.
\end{example}

\begin{proposition}		\label{PropTopologieAx}
	Soit $V$ un espace vectoriel normé.
	\begin{enumerate}
		\item
			L'ensemble $V$ lui-même et le vide sont à la fois fermées et ouvertes.
		\item
			Toute union d'ouverts est ouverte.
		\item
			Toute intersection \emph{finie} d'ouverts est ouverte.
		\item		\label{ItemPropTopologieAxiv}
			Le vide et $V$ sont les seules parties de $V$ à être à la fois fermées et ouvertes.
	\end{enumerate}
\end{proposition}

\begin{proof}
	L'ingrédient principal de cette démonstration est que si $a$ est un point d'un ouvert $\mO$, alors il existe une boule autour de $a$ contenue dans $\mO$ parce que $a$ doit être dans l'intérieur de $\mO$.
	\begin{enumerate}

		\item
			Nous avons déjà dit que, par définition, l'ensemble vide est ouvert. Cela implique que $V$ lui-même est fermé (parce que son complémentaire est le vide). De plus, $V$ est ouvert parce que toutes les boules sont inclues à $V$. Le vide est alors fermé (parce que son complémentaire est $V$).
		\item
			Soit une famille $(\mO_i)_{i\in I}$ d'ouverts\footnote{L'ensemble $I$ avec lequel nous «numérotons» les ouverts $\mO_i$ est \emph{quelconque}, c'est à dire qu'il peut être $\eN$, $\eR$, $\eR^n$ ou n'importe quel autre ensemble, fini ou infini.}, et l'union
			\begin{equation}
				\mO=\bigcup_{i\in I}\mO_i.
			\end{equation}
			Soit maintenant $a\in\mO$. Nous devons prouver qu'il existe une boule centrée en $a$ entièrement contenue dans $\mO$. Étant donné que $a\in\mO$, il existe $i\in I$ tel que $a\in\mO_i$ (c'est à dire que $a$ est au moins dans un des $\mO_i$). Par hypothèse l'ensemble $\mO_i$ est ouvert et donc tous ses points (en particulier $a$) sont intérieurs; il existe donc une boule $B(a,r)$ centrée en $a$ telle que $B(a,r)\subset\mO_i\subset\mO$.
		
		\item
			Soit une famille finie d'ouverts $(\mO_k)_{k\in\{ 1,\ldots,n \}}$, et $a\in\mO$ où
			\begin{equation}
				\mO=\bigcap_{k=1}^n\mO_k.
			\end{equation}
			Vu que $a$ appartient à chaque ouvert $\mO_k$, nous pouvons trouver, pour chacun de ces ouverts, une boule $B(a,r_k)$ contenue dans $\mO_k$. Chacun des $r_k$ est strictement positif, et nous n'en avons qu'un nombre fini, donc le nombre $r=\min\{ r_1,\ldots,r_n \}$ est strictement positif. La boule $B(a,r)$ est inclue dans toutes les autres (parce que $B(a,r)\subset B(a,r')$ lorsque $r\leq r'$), par conséquent
			\begin{equation}
				B(a,r)\subset\bigcap_{k=1}^nB(a,r_k)\subset\bigcap_{k=1}^n\mO_k=\mO,
			\end{equation}
			c'est à dire que la boule de rayon $r$ est une boule centrée en $a$ contenue dans $\mO$, ce qui fait que $a$ est intérieur à $\mO$.
		\item
			Nous acceptons ce point sans démonstration. 
	\end{enumerate}
   % TODO : trouver et mettre une preuve du dernier point.
	
\end{proof}

La proposition dit que toute intersection \emph{finie} d'ouvert est ouverte. Il est faux de croire que cela se généralise aux intersections infinies, comme le montre l'exemple suivant :
\begin{equation}
	\bigcap_{i=1}^{\infty}\mathopen] -\frac{1}{ n } , \frac{1}{ n } \mathclose[=\{ 0 \}.
\end{equation}
Chacun des ensembles $\mathopen] -\frac{1}{ n } , \frac{1}{ n } \mathclose[$ est ouvert, mais le singleton $\{ 0 \}$ est fermé (pourquoi ?).

Nous reportons à la proposition \ref{PropBorneSupInf} la preuve du fait que tout ensemble borné de $\eR$ possède un infimum et un supremum.



\begin{definition}
	L'ensemble des ouverts de $V$ est la \defe{topologie}{topologie} de $V$. La topologie dont nous parlons ici est dite \defe{induite}{induite!topologie} par la norme $\| . \|$ de $V$ (parce que cette norme définit la notion de boule et qu'à son tour la notion de boule définit la notion d'ouverts). Un \defe{voisinage}{voisinage} de $a$ dans $V$ est un ensemble contenant un ouvert contenant $a$.
\end{definition}

Il existe de nombreuses topologies sur un espace vectoriel donné, mais certaines sont plus fameuses que d'autres. Dans le cas de $V=\eR^n$, la topologie \defe{usuelle}{topologie!usuelle sur $\eR^n$} est celle induite par la norme euclidienne. Lorsque nous parlons de boules, de fermés, de voisinages ou d'autres notions topologiques (y compris de convergence, voir plus bas) dans $\eR^n$, nous sous-entendons toujours la topologie de la norme euclidienne.

\begin{example}
	Les ensemble suivants sont des voisinages de $3$ dans $\eR$ :
	\begin{itemize}
		\item
			$\mathopen] 1 , 5 \mathclose[$;
		\item
			$\mathopen[ 0 , 10 \mathclose]$;
		\item
			$\eR$.
	\end{itemize}
	Les ensembles suivants ne sont pas des voisinages de $3$ dans $\eR$ :
	\begin{itemize}
		\item 
			$\mathopen] 1 , 3 \mathclose[$;
		\item
			$\mathopen] 1 , 3 \mathclose]$;
		\item
			$\mathopen[ 0 , 5 [\setminus\{ 3 \}$.
	\end{itemize}
\end{example}

\begin{proposition}
	Dans un espace vectoriel normé,
	\begin{enumerate}
		\item
			toute intersection de fermés est fermée;
		\item
			toute union \emph{finie} de fermés est fermée.
	\end{enumerate}
\end{proposition}
Encore une fois, l'hypothèse de finitude de l'intersection est indispensable comme le montre l'exemple suivant :
\begin{equation}
	\bigcup_{n=1}^{\infty}\mathopen[ -1+\frac{1}{ n } , 1-\frac{1}{ n } \mathclose]=\mathopen] -1 , 1 \mathclose[.
\end{equation}
Chacun des intervalles dont on prend l'union est fermé tandis que l'union est ouverte.

\begin{definition}
	Soit $A$, une partie de l'espace vectoriel normé $V$. Un point $a\in V$ est dit \defe{adhérent}{adhérence} à $A$ dans $V$ si pour tout $\varepsilon>0$,
	\begin{equation}
		B(a,\varepsilon)\cap A\neq\emptyset.
	\end{equation}
	Nous notons $\bar A$ l'ensemble des points adhérents à $a$ et nous disons que $\bar A$ est l'adhérence de $A$. L'ensemble $\bar A$ sera aussi souvent nommé \defe{fermeture}{fermeture} de l'ensemble $A$.
\end{definition}
Un point peut être adhérent à $A$ sans faire partie de $A$, et nous avons toujours $A\subset\bar A$.

\begin{example}
	La terminologie «fermeture» de $A$ pour désigner $\bar A$ provient de deux origines.
	\begin{enumerate}
		\item
			L'ensemble $\bar A$ est le plus petit fermé contenant $A$. Cela signifie que si $B$ est un fermé qui contient $A$, alors $\bar A\subset A$. Nous acceptons cela sans preuve.
            % position 25804
            %Nous allons prouver cette affirmation dans l'exercice \ref{exoGeomAnal-0008}.
		\item
			Pour les intervalles dans $\eR$, trouver $\bar A$ revient à fermer les extrémités qui sont ouvertes, comme on en a parlé dans l'exemple \ref{ExempleFermeIntevrR}.
	\end{enumerate}
\end{example}

\begin{example}
	Dans $\eR$, l'infimum et le supremum d'un ensemble sont des points adhérents. En effet si $M$ est le supremum de $A\subset\eR$, pour tout $\varepsilon$, il existe un $a\in A$ tel que $a>M-\varepsilon$, tandis que $M>a$. Cela fait que $a\in B(M,\varepsilon)$, et en particulier que pour tout rayon $\varepsilon$, nous avons $B(M,\varepsilon)\cap A\neq\emptyset$.

	Le même raisonnement montre que l'infimum est également dans l'adhérence de $A$.
\end{example}

\begin{example}		\label{ParlerEncoredeF}
	Il ne faut pas conclure de l'exemple précédent qu'un point limite ou adhérent est automatiquement un minimum ou un maximum. En effet, si nous regardons l'ensemble formé par les points de la suite $x_n=(-1)^n/n$, le nombre zéro est un point adhérent et une limite, mais pas un infimum ni un maximum.
\end{example}

\begin{lemma}
	Si $B$ est une partie fermée de $V$, alors $B=\bar B$.
\end{lemma}

\begin{proof}
	Supposons qu'il existe $a\in\bar B$ tel que $a\notin B$. Alors il n'y a pas d'ouverts autour de $a$ qui soit contenu dans $\complement B$. Cela prouve que $\complement B$ n'est pas ouvert, et par conséquent que $B$ n'est pas fermé. Cela est une contradiction qui montre que tout point de $\bar B$ doit appartenir à $B$ lorsque $B$ est fermé.
\end{proof}

\begin{example}
	Au niveau des intervalles dans $\eR$, prendre l'adhérence consiste à «fermer là où c'est ouvert». Attention cependant à ne pas fermer l'intervalle en l'infini.
	\begin{enumerate}
		\item
			$\overline{ \mathopen[ 0 , 2 [ }=\mathopen[ 0 , 2 \mathclose]$.
		\item
			$\overline{ \mathopen] 3 , \infty \mathopen] }=\mathopen[ 3 , \infty [$.
	\end{enumerate}
\end{example}

\begin{proposition}
	Soit $V$ un espace vectoriel normé et $a\in V$. Les trois conditions suivantes sont équivalentes :
	\begin{enumerate}
		\item
			$a\in\bar A$;
		\item
			il existe une suite d'éléments $x_n$ dans $A$ qui converge vers $a$;
		\item
			$d(a,A)=0$.
	\end{enumerate}
\end{proposition}
Notez que dans cette proposition, nous ne supposons pas que $a$ soit dans $A$.

\begin{proposition}		\label{PropComleIntBar}
	Pour toute partie $A$ d'un espace vectoriel normé nous avons
	\begin{enumerate}
		\item
			$V\setminus\bar A=\Int(V\setminus A)$,
		\item
			$V\setminus\Int(A)=\overline{ V\setminus A }$.
	\end{enumerate}
\end{proposition}

En utilisant les notations du complémentaire (\ref{AppComplement}), les deux points de la proposition se récrivent
\begin{enumerate}
	\item
		$\complement \bar A=\Int(\complement A)$,
	\item\label{ItemLemPropComplementiv}
		$\complement\Int(A)=\overline{ \complement A }$.
\end{enumerate}

\begin{proof}
	Nous avons $a\in V\setminus\bar A$ si et seulement si $a\notin\bar A$. Or ne pas être dans $\bar A$ signifie qu'il existe un rayon $\varepsilon$ tel que la boule $B(a,\varepsilon)$ n'intersecte pas $A$. Le fait que la boule $B(a,\varepsilon)$ n'intersecte pas $A$ est équivalent à dire que $B(a,\varepsilon)\subset V\setminus A$. Or cela est exactement la définition du fait que $a$ est à l'intérieur de $V\setminus A$. Nous avons donc montré que $a\in V\setminus \bar A$ si et seulement si $a\in\Int(V\setminus A)$. Cela prouve la première affirmation.

	Pour prouver la seconde affirmation, nous appliquons la première au complémentaire de $A$ : $\complement(\overline{ \complement A })=\Int(\complement\complement A)$. En prenant le complémentaire des deux membres nous trouvons successivement
	\begin{equation}
		\begin{aligned}[]
			\complement\complement(\overline{ \complement A })&=\complement\Int(\complement\complement A),\\
			\overline{ \complement A }&=\complement\Int(A),
		\end{aligned}
	\end{equation}
	ce qu'il fallait démontrer.
\end{proof}

Attention à ne pas confondre $\complement \bar A$ et $\overline{ \complement A }$. Ces deux ensembles ne sont pas égaux. En effet, en tant que complément d'un fermé, l'ensemble $\complement \bar A$ est certainement ouvert, tandis que, en tant que fermeture, l'ensemble $\overline{ \complement A }$ est fermé. Pouvez-vous trouver des exemples d'ensembles $A$ tels que $\complement \bar A=\overline{ \complement A }$ ?

\begin{proposition}
	Soient $A$ et $B$ deux parties de l'espace vectoriel normé $V$.
	\begin{enumerate}
		\item
			Pour les inclusions, si $A\subset B$, alors $\Int(A)\subset\Int(B)$ et $\bar A\subset\bar B$.
		\item
			Pour les unions, $\overline{ A\cup B }=\overline{ A }\cup\overline{ B }$ et $\overline{ A\cap B }\subset\bar A\cap\bar B$.
		\item
			Pour les intersections, $\Int(A)\cap\Int(B)=\Int(A\cap B)$ et $\Int(A)\cup\Int(B)\subset\Int(A\cup B)$.
	\end{enumerate}
\end{proposition}

\begin{proof}
	\begin{enumerate}
		\item
			Si $a$ est dans l'intérieur de $A$, il existe une boule autour de $a$ contenue dans $A$. Cette boule est alors contenue dans $B$ et donc est une boule autour de $a$ contenue dans $B$, ce qui fait que $a$ est dans l'intérieur de $B$. Si maintenant $a$ est dans l'adhérence de $A$, toute boule centrée en $a$ contient un élément de $A$ et donc un élément de $B$, ce qui prouve que $a$ est dans l'adhérence de $B$.
		\item
			Nous avons $A\subset A\cup B$ et donc, en utilisant le premier point, $\bar A\subset\overline{ A\cup B }$. De la même manière, $\bar B\subset\overline{ A\cup B }$. En prenant l'union, $\bar A\cup\bar B\subset\overline{ A\cup B }$.

			Réciproquement, soit $a\in\overline{ A\cup B }$ et montrons que $a\in\bar A\cup\bar B$. Supposons par l'absurde que $a$ ne soit ni dans $\bar A$ ni dans $\bar B$. Il existe donc des rayon $\varepsilon_1$ et $\varepsilon_2$ tels que
			\begin{equation}
				\begin{aligned}[]
					B(a,\varepsilon_1)\cap A&=\emptyset,\\
					B(a,\varepsilon_2)\cap B&=\emptyset.
				\end{aligned}
			\end{equation}
			En prenant $r=\min\{ \varepsilon_1,\varepsilon_2 \}$, la boule $B(a,r)$ est inclue aux deux boules citées et donc n'intersecte ni $A$ ni $B$. Donc $a\notin\overline{ A\cup B }$, d'où la contradiction.

		\item
			Si nous appliquons le second point à $\complement A$ et $\complement B$, nous trouvons
			\begin{equation}
				\overline{ \complement A\cup\complement B }=\overline{ \complement A}\cup\overline{ \complement B}.
			\end{equation}
			En utilisant les propriétés du lemme \ref{LemPropsComplement}, le membre de gauche devient
			\begin{equation}	\label{Eq2707CACBCAB}
				\overline{ \complement A\cup\complement B }=\overline{ \complement(A\cap B) }=\complement\Int(A\cap B),
			\end{equation}
			tandis que le membre de droite devient
			\begin{equation}		\label{Eq2707cAcBACAACB}
				\overline{ \complement A }\cup\overline{ \complement B }=\complement\Int(A)\cup\complement\Int(A)=\complement\Big( \Int(A)\cap\Int(B) \Big).
			\end{equation}
			En égalisant le membre de droite de \eqref{Eq2707CACBCAB} avec celui de \eqref{Eq2707cAcBACAACB} et en passant au complémentaire nous trouvons
			\begin{equation}
				\Int(A\cap B)=\Int(A)\cap\Int(B),
			\end{equation}
			comme annoncé.

			La dernière affirmation provient du fait que $\Int(A)\subset\Int(A\cup B)$ et de la propriété équivalente pour $B$.
	\end{enumerate}
\end{proof}

\begin{remark}
	Nous avons prouvé que $\overline{ A\cap B }\subset\bar A\cap\bar B$. Il arrive que l'inclusion soit stricte, comme dans l'exemple suivant. Si nous prenons $A=\mathopen[ 0 , 1 \mathclose]$ et $B=\mathopen] 1 , 2 \mathclose]$, nous avons $A\cap B=\emptyset$ et donc $\overline{ A\cap B }=\emptyset$. Par contre nous avons $\bar A\cap\bar B=\{ 1 \}$.
\end{remark}

\begin{definition}
	La \defe{frontière}{frontière} d'un sous-ensemble $A$ de l'espace vectoriel normé $V$ est l'ensemble des points $a\in V$ tels que
	\begin{equation}
		\begin{aligned}[]
			B(a,r)\cap A&\neq \emptyset,\\
			B(a,r)\cap \complement A&\neq \emptyset,
		\end{aligned}
	\end{equation}
	pour tout rayon $r$. En d'autres termes, toute boule autour de $a$ contient des points de $A$ et des points de $\complement A$. La frontière de $A$ se note $\partial A$\nomenclature[T]{$\partial A$}{La frontière de l'ensemble $A$}.
\end{definition}

\begin{proposition}		\label{PropDescFrpbsmI}
	La frontière d'une partie $A$ d'un espace vectoriel normé $V$ s'exprime sous la forme
	\begin{equation}
		\partial A=\bar A\setminus\Int(A).
	\end{equation}
\end{proposition}

\begin{proof}
	Le fait pour un point $a$ de $V$ d'appartenir à $\bar A$ signifie que toute boule centrée en $a$ intersecte $A$. De la même façon, le fait de ne pas appartenir à $\Int(A)$ signifie que toute boule centrée en $a$ intersecte $\complement A$.
\end{proof}

La description de la frontière donnée par la proposition \ref{PropDescFrpbsmI} est celle qu'en pratique nous utilisons le plus souvent. Dans certains textes, elle est prise comme définition de la frontière.

\begin{lemma}
	La frontière de $A$ peut également s'exprimer des façons suivantes :
	\begin{equation}
		\partial A= \bar A\cap\complement\Int(A)=\bar A\cap\overline{ \complement A },
	\end{equation}
\end{lemma}

\begin{proof}
	En partant de $\partial A=\bar A\setminus \Int(A)$, la première égalité est une application de la propriété \ref{ItemLemPropComplementiii} du lemme \ref{LemPropsComplement}. La seconde égalité est alors la proposition \ref{PropComleIntBar}.
\end{proof}

\begin{example}
	Dans $\eR$, la frontière d'un intervalle est la paire constituée des points extrêmes. En effet
	\begin{equation}
		\partial\mathopen[ a , b [=\overline{ \mathopen[ a , b [ }\setminus\Int\big( \mathopen[ a , b [ \big)=\mathopen[ a , b \mathclose]\setminus\mathopen] a , b \mathclose[=\{ a,b \}.
	\end{equation}

	Toujours dans $\eR$ nous avons
	\begin{equation}
		\partial\eR=\bar\eR\setminus\Int(\eR)=\eR\setminus\eR=\emptyset,
	\end{equation}
	et
	\begin{equation}
		\partial\eQ=\bar\eQ\setminus\Int(\eQ)=\eR\setminus\emptyset=\eR.
	\end{equation}
\end{example}

%TODO : prouver que la boule fermée est la fermeture de la boule ouverte.

\begin{example}
	Dans $\eR^n$, nous avons
	\begin{equation}
		\partial B(a,r)=\partial\bar B(a,r)=S(a,r).
	\end{equation}

    Cela est un boulot pour la proposition \ref{PropBoitPtLoin}. Si \( x\in S(a,r)\) alors tout boule autour de \( x\) contient des points à distance strictement plus grande et plus petite que \( d(a,x)\), c'est à dire des points dans \( B(a,r)\) et hors de \( B(a,r)\). Cela prouve que les points de \( S(a,r)\) font partie de \( \partial B(a,r)\), c'est à dire que \( S(a,r)\subset \partial B(a,r)\); et idem pour \( \bar B(a,r)\). 

Pour prouver l'inclusion inverse, soit \( x\in \partial B(a,r)\). Vu que toute boule autour de \( x\) contient des points intérieurs à \( B(a,r)\), pour tout \( \epsilon>0\), \( d(a,x)-\epsilon< r \), c'est à dire que \( d(a,x)\leq r\). De la même manière toute boule autour de \( x\) contient des points hors de \( B(a,r)\) signifie que pour tout \( \epsilon\), \( d(a,x)+\epsilon>r\) ou encore que \( d(a,x)\geq r\). Les deux ensemble implique que \( d(a,x)=r\).
\end{example}

\begin{remark}
    Il serait toutefois faux de croire que \( \partial A=\partial \bar A\) pour toute partie \( A\) de \( \eR^n\). En effet si \( A=\eR\setminus\{ 0 \}\) nous avons \( \partial A=\{ 0 \}\) et \( \bar A=\eR\), donc \( \partial \bar A=\emptyset\).
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Point isolé, point d'accumulation}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
	Soit $D$, une partie de $V$.
	\begin{enumerate}
		\item
			Un point $a\in D$ est dit \defe{isolé}{isolé!point dans un espace vectoriel normé} dans $D$ relativement à $V$ s'il existe un $\varepsilon>0$ tel que
			\begin{equation}
				B(a,\varepsilon)\cap D=\{ a \}.
			\end{equation}
		\item
			Un point $a\in V$ est un \defe{point d'accumulation}{accumulation!dans espace vectoriel normé} de $D$ si pour tout $\varepsilon>0$,
			\begin{equation}
				\Big( B(a,\varepsilon)\setminus\{ a \}\Big)\cap D\neq \emptyset.
			\end{equation}
	\end{enumerate}
\end{definition}

\newcommand{\CaptionFigAccumulationIsole}{L'ensemble décrit par l'équation \eqref{Eq2807BouleIso}. Le point $P$ est un point isolé de $D$, tandis que  les points $S$ et $Q$ sont des points d'accumulation.}
\input{auto/pictures_tex/Fig_AccumulationIsole.pstricks}

\begin{example}
	Considérons la partie suivante de $\eR^2$ :
	\begin{equation}	\label{Eq2807BouleIso}
		D=\{ (x,y)\tq x^2+y^2<1\}\cup\{ (1,1) \}.
	\end{equation}
	Comme on peut le voir sur la figure \ref{LabelFigAccumulationIsole}, le point $P=(1,1)$ est un point isolé de $D$ parce qu'on peut tracer une boule autour de $P$ sans inclure d'autres points de $D$ que $P$ lui-même. Le point $Q=(-1,0)$ est un point d'accumulation de $D$ parce que toute boule autour de $Q$ contient des points de $D$.

    Le point $S$, étant un point intérieur, est un point d'accumulation : toute boule autour de $S$ intersecte $D$.
    
    Notez cependant que le point $Q$ lui-même n'est pas dans $D$ parce que l'inégalité qui définit $D$ est stricte.
\end{example}

\begin{remark}
    À propos de la position des points d'accumulation et des points isolés.
    \begin{enumerate}
        \item
            Les points intérieurs sont tous des points d'accumulation.
        \item
            Les points isolés ne sont jamais intérieurs.
        \item
            Certains points d'accumulation ne font pas partie de l'ensemble. Par exemple le point $1$ est un point d'accumulation de $E=\mathopen] 0 , 1 \mathclose[$.
        \item
            Les points de la frontière sont soit d'accumulation soit isolés.
    \end{enumerate}
\end{remark}


\begin{example}
	Tous les points de $\eR$ sont des points d'accumulation de $\eQ$ parce que dans toute boule autour d'un réel, on peut trouver un nombre rationnel.
\end{example}

\begin{remark}
	L'ensemble des points d'accumulation d'un ensemble n'est pas exactement son adhérence. En effet, un point isolé dans $A$ est dans l'adhérence de $A$, mais n'est pas un point d'accumulation de $A$.
\end{remark}

\begin{lemma}[Continuité des projections]       \label{LEMooHAODooYSPmvH}
    Soient deux espaces vectoriels \( V\) et \( W\) ainsi qu'une fonction continue \( f\colon V\times W\to \eR\). Alors pour tout \( v_0\in V\) la fonction 
    \begin{equation}
        \begin{aligned}
            f_{v_0}\colon W&\to \eR \\
            w&\mapsto f(v_0,w) 
        \end{aligned}
    \end{equation}
    est continue.    
\end{lemma}
