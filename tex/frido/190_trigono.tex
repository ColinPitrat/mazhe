% This is part of (everything) I know in mathematics
% Copyright (c) 2011-2017
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Isométries dans \( \eR^n\)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    Un \defe{hyperplan}{hyperplan} de \( \eR^n\) est un sous-espace affine de dimension \( n-1\). 
\end{definition}
    
\begin{lemmaDef}
    Si un hyperplan \( H\) de \( \eR^n\) est donné, et si \( x\in \eR^n\), il existe un unique point \( y\in \eR^n\) tel que
    \begin{enumerate}
        \item
            \( x-y\perp H\),
        \item
            Le segment \( [x,y]\) coupe \( H\) en son milieu.
    \end{enumerate}
    La \defe{réflexion}{réflexion!par rapport à un hyperplan} \( \sigma_H\) est l'application $\sigma_H\colon \eR^n\to \eR^n $ qui à \( x\) fait correspondre ce \( y\).
\end{lemmaDef}

\begin{proof}
    Il faut vérifier que les conditions données définissent effectivement un unique point de \( \eR^n\). Soit \( H_0\) le sous-espace vectoriel parallèle à \( H\) et une base orthonormée \( \{ e_1,\ldots, e_{n-1} \}\) de \( H_0\). Nous complétons cela en une base orthonormée de \( \eR^n\) avec un vecteur \( e_n\). Si \( H=H_0+v\), quitte à décomposer \( v\) en une partie parallèle et une partie perpendiculaire à \( H\), nous avons
    \begin{equation}
        H=H_0+\lambda e_n
    \end{equation}
    pour un certain \( \lambda\).

    Une droite passant par \( x\) et perpendiculaire à \( H\) est de la forme \( t\mapsto x+te_n\). Si \( x=\sum_{i=1}^{n}x_ie_i\) alors l'unique point de cette droit à être dans \( H\) est le point tel que \(   x_ne_n+te_n=\lambda e_n   \), c'est à dire \( t=-x_n\). L'unique point \( y\) sur cette droite à être tel que \( [x,y ]\) coupe \( H\) en son milieu est celui qui correspond à \( t=-2x_n\). 
\end{proof}

Notons au passage que cette preuve donne une formule pour \( \sigma_H\) :
\begin{equation}        \label{EQooRTWLooLPsUpY}
    \sigma_H(x)=\sum_{i=1}^{n-1}x_ie_i-x_ne_n.
\end{equation}
Il s'agit donc de changer le signe de la composante perpendiculaire à \( H\).

\begin{lemma}       \label{LEMooWYVRooQmWqvM}
    Dans cette même base si \( H_0\) est l'hyperplan parallèle à \( H\) et passant par l'origine, nous écrivons \( H=H_0+\lambda e_n\) pour un certain \( \lambda\). Alors
    \begin{equation}
        \sigma_H=\sigma_{H_0}+2\lambda e_n.
    \end{equation}
\end{lemma}

\begin{proof}
    Un élément \( x\in \eR^n\) peut être décomposé dans la base adéquate en \( x=x_H+x_ne_n\). Nous savons de la formule \eqref{EQooRTWLooLPsUpY} que 
    \begin{equation}
        \sigma_H(x)=x_H-x_ne_n.
    \end{equation}
    Mais vu que \( \sigma_{H_0}(x_H)=x_H-2\lambda e_n\) nous avons
    \begin{equation}
            \sigma_{H_0}(x)+2\lambda e_n=\sigma_{H_0}(x_H+x_ne_n)+2\lambda e_N=x_H-2\lambda e_n-x_ne_n+2\lambda e_n=x_H-x_ne_n.
    \end{equation}
\end{proof}

Le lemme suivant est une généralisation du fait que tous les points de la médiatrice d'un segment sont à égale distance des deux extrémités du segment (très utile lorsqu'on étudie les triangles isocèles).
\begin{lemma}[\cite{ooZYLAooXwWjLa}]        \label{LEMooDPLYooJKZxiM}
    Soient deux points distincts \( x_0,y_0\in \eR^n\) l'ensemble \( H\subset \eR^n\) donné par
    \begin{equation}
        H=\{ x\in \eR^n\tq d(x,x_0)=d(x,y_0) \}.
    \end{equation}
    Alors \( H\) est l'hyperplan orthogonal au vecteur \( v=y_0-x_0\) et \( H\) passe par le milieu du segment \( [x_0,y_0] \).
\end{lemma}

\begin{proof}
    Nous savons que
    \begin{equation}
        d(x,x_0)^2=\langle x-x_0, x-x_0\rangle =\| x \|^2+\| x_0 \|^2-2\langle x, x_0\rangle,
    \end{equation}
    ou encore
    \begin{equation}
        \| x_0 \|^2-\| y_0 \|^2=2\langle x, x_0-y_0\rangle .
    \end{equation}
    En posant \( v=y_0-x_0\) et en considérant la forme linéaire
    \begin{equation}
        \begin{aligned}
            \beta\colon \eR^n&\to \eR \\
            x&\mapsto \langle x, v\rangle , 
        \end{aligned}
    \end{equation}
    Nous avons \( x\in H\) si et seulement si \( \beta(x)=\frac{ 1 }{2}\big( \| y_0 \|^2-\| x_0 \|^2 \big)=\lambda\). En d'autres termes, \( H=\beta^{-1}(\lambda)\). Par la proposition \ref{PROPooAKJBooMkmsiV} la partie \( H\) est un sous-espace affine. C'est même un translaté de \( \ker(\beta)\), et comme \( \ker(\beta)\) est l'espace vectoriel des vecteurs perpendiculaires à \( v\), nous avons \( \dim(H)=\dim\big( \ker(\beta) \big)=n-1\).

    Le fait que \( H\) contienne le milieu du segment \( [x_0,y_0]\) est par définition.
\end{proof}

Pour le lemme suivant, et pour que la récurrence se passe bien nous disons que l'ensemble vide est un espace vectoriel de dimension \( -1\).
\begin{lemma}       \label{LEMooJCDRooGAmlwp}
    Si \( f\in\Isom(\eR^n)\) satisfait 
    \begin{equation}
        \dim\big( \Fix(f) \big)=n-k
    \end{equation}
    alors \( f\) peut être écrit comme composition d'au plus \( k\) réflexions hyperplanes.
\end{lemma}

\begin{proof}
    Nous faisons une récurrence sur \( k\geq 0\). 
    
    Pour l'initialisation, si \( k=0\) alors \( \dim\big( \Fix(f) \big)=n\), c'est à dire que \( f\) fixe tout \( \eR^n\), autant dire que \( f\) est l'identité, une composition de zéro réflexions.

    Pour la récurrence, nous supposons que le lemme est démontré jusqu'à \( k\geq 0\). Soit donc \( f\in\Isom(\eR^n)\) tel que 
    \begin{equation}
        \dim\big( \Fix(f) \big)=n-(k+1).
    \end{equation}
    Vu que \( k\geq 0\), la dimension de \( \Fix(f)\) est strictement plus petite que \( n\), donc il existe un \( x_0\in \eR^n\) tel que \( f(x_0)\neq x_0\). Nous posons
    \begin{equation}
        H=\{ x\in \eR^n\tq d(x,x_0)=d\big( x,f(x_0) \big)  \}.
    \end{equation}
    Par le lemme \ref{LEMooDPLYooJKZxiM}, ce \( H\) est l'hyperplan orthogonal à \( v=f(x_0)-x_0\) et passant par le milieu du segment \( [x_0,f(x_0)]\).

    Nous posons \( g=\sigma_H\circ f\). Vu que \( g(x_0)=\sigma_H(f(x_0))=x_0\), ce \( x_0\) est un point fixe de \( g\). Le fait que \( \sigma_H\big( f(x_0) \big)=x_0\) est vraiment la définition de l'hyperplan \( H\).

    Nous avons donc
    \begin{equation}
        x_0\in\Fix(g)\setminus\Fix(f).
    \end{equation}
    Mais nous prouvons de plus que \( \Fix(f)\subset\Fix(g)\). En effet si \( y\in Fix(f)\) alors \( y\in H\) parce que 
    \begin{equation}
        d(y,x_0)=d\big( f(y),f(x_0) \big)=d\big( y, f(x_0) \big).
    \end{equation}
    Vu que \( y\in H\) nous avons \( y\in \Fix(g)\) parce que 
    \begin{equation}
        g(y)=\sigma_H\big( f(y) \big)=\sigma_H(y)=y.
    \end{equation}
    Tout cela pour dire que l'ensemble \( \Fix(g)\) est \emph{strictement} plus grand que \( \Fix(f)\). Et comme ce sont des espaces affines nous pouvons parler de dimension :
    \begin{equation}
        \dim\big( \Fix(g) \big)>\dim\big( \Fix(f) \big).
    \end{equation}
    Par hypothèse de récurrence, l'application \(  g\) peut être écrite comme composition de \( k\) réflexions. Donc l'application
    \begin{equation}
        f=\sigma_H\circ g
    \end{equation}
    est une composition de \( k+1\) réflexions.
\end{proof}

\begin{lemma}       \label{LEMooMCVKooKzmlAg}
    Soit un hyperplan \( H\) et un vecteur \( v\) de \( \eR^n\). Nous avons
    \begin{equation}
        \tau_v\circ \sigma_H\circ\tau_v^{-1}=\sigma_{\tau_v(H)}.
    \end{equation}
\end{lemma}

\begin{proof}
    Pour ce faire nous considérons une base adaptée. Les vecteurs \( \{ e_1,\ldots, e_{n-1} \}\) forment une base orthonormée de \( H_0\) et \( e_n\) complète en une base orthonormée de \( \eR^n\). Soit \( H_0\) l'hyperplan parallèle à \( H\) et passant par l'origine; nous avons, pour un certain \( \lambda\in \eR\),
    \begin{equation}
        H=H_0+\lambda e_n
    \end{equation}
    D'un autre côté, le vecteur \( v\) peut être décomposé en \( v=v_1+v_2\) où \( v_1\perp H\) et \( v_2\parallel H\). Alors 
    \begin{equation}
        \tau_v(H)=H+v=H+v_2=H_0+\lambda e_n+v_2.
    \end{equation}
    Nous pouvons maintenant utiliser le lemme \ref{LEMooWYVRooQmWqvM} pour exprimer la transformation \( \sigma_{\tau_v(H)}\) :
    \begin{equation}        \label{EQooNYKFooXprXav}
        \sigma_{\tau_v(H)}(x)=\sigma_{H_0}(x)+ 2\lambda e_n+2v_2
    \end{equation}
    
    Mais d'autre part, 
    \begin{equation}
        (\tau_v\circ \sigma_H\circ\tau_{v}^{-1})(x)=v+\sigma_H(x-v)=v+\sigma_{H_0}(x-v)+2\lambda e_n.
    \end{equation}
    Vue la décomposition de \( v=v_1+v_2\) nous avons \( \sigma_{H_0}(v)=-v_1+v_2\) et donc
    \begin{equation}        \label{EQooGOHEooALPRFB}
        (\tau_v\circ \sigma_H\circ\tau_{v}^{-1})(x)= v+  \sigma_{H_0}(x)+v_1-v_2+2\lambda e_n=\sigma_{H_0}+2v_1+2\lambda e_n.
    \end{equation}
    Les expressions \eqref{EQooNYKFooXprXav} et \eqref{EQooGOHEooALPRFB} coïncident, d'où l'égalité recherchée.
\end{proof}

\begin{theorem}[\cite{ooZYLAooXwWjLa}]
    Toute isométrie de \( \eR^n\) peut être écrite comme composition d'au plus \( n+1\) réflexions.
    
    Une isométrie de \( \eR^n\) préserve l'orientation si et seulement si est elle composition d'un nombre pair de réflexions.
\end{theorem}

\begin{proof}
    La première partie de ce théorème n'est rien d'autre que le lemme \ref{LEMooJCDRooGAmlwp} parce que le pire cas est celui où le fixateur de \( f\) est réduit à l'ensemble vide, et dans ce cas l'application \( f\) est une composition de \( n+1\) réflexions.

    Pour la seconde partie nous définissons
    \begin{equation}
        \begin{aligned}
            \epsilon\colon \Isom(\eR^n)&\to \{ \pm 1 \} \\
            \tau_v\circ \alpha&\mapsto \det(\alpha)
        \end{aligned}
    \end{equation}
    où nous nous référons à la décomposition unique d'un élément de \( \Isom(\eR^n)\) sous la forme \( \tau_v\circ \alpha\) avec \( \alpha\in O(n)\) donnée par le théorème \ref{THOooQJSRooMrqQct}\ref{ITEMooEWSIooNKzRxB}.

    Le noyau de \( \epsilon\) est alors la partie 
    \begin{equation}
        \ker(\epsilon)=\eR^n\times_{\AD}\SO(n).
    \end{equation}
    Une isométrie \( f\) préserve l'orientation si et seulement si \( \epsilon(f)=1\). Vu que toutes les isométries sont des composition de réflexions (première partie), il nous suffit de montrer que \( \epsilon(\epsilon_H)=-1\) pour qu'une isométrie préserve l'orientation si et seulement si elle est composition d'un nombre pair de réflexions.

    Nous commençons par prouver que pour tout vecteur \( v\), \( \epsilon\big( \sigma_H \big)=\epsilon\big( \sigma_{\tau_v(H)} \big)\). Pour cela nous utilisons le lemme \ref{LEMooMCVKooKzmlAg} et le fait que \( \epsilon\) est un homomorphisme :
    \begin{equation}
        \epsilon(\sigma_{\tau_v(H)})=\epsilon(\tau_v)\epsilon(\sigma_H)\epsilon(\tau_v^{-1})=\epsilon(\sigma_H)
    \end{equation}
    parce que la partie linéaire d'une translation est l'identité (et donc \( \epsilon(\tau_v)=1\) pour tout \( v\)).

    Nous avons donc \( \epsilon(\sigma_H)=\epsilon(\sigma_{H_0})\). En ce qui concerne \( \sigma_{H_0}\), dans la base adaptée la matrice est
    \begin{equation}
        \sigma_{H_0}=\begin{pmatrix}
             1   &       &       &       \\
                &   \ddots    &       &       \\
                &       &   1    &       \\ 
                &       &       &   -1     
         \end{pmatrix},
    \end{equation}
    dont le déterminant est \( -1\).
\end{proof}

Pour en savoir plus sur le groupe des isométries, il faut lire le théorème de Cartan-Dieudonné dans \cite{JGAdTA}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Groupes finis d'isomorphismes}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Si \( X\) est une partie finie de \( \eR^n\), le \defe{barycentre}{barycentre!cas vectoriel} de \( X\) est le point
    \begin{equation}
        B_X=\frac{1}{ | X | }\sum_{x\in X}x
    \end{equation}
    où \( | X |\) est le cardinal de \( X\).
\end{definition}
Cela est à mettre en relation avec la définition dans le cadre affine \ref{LemtEwnSH}.

\begin{lemma}[\cite{ooZYLAooXwWjLa}]
    Soit une partie finie \( X\) de \( \eR^n\) et une application affine \( f\in\Aff(\eR^n)\). Alors
    \begin{equation}
        f(B_X)=B_{f(X)}.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous savons que toute application affine est une composée de translation et d'une application linéaire : \( f=\tau_v\circ g\) avec \( v\in \eR^n\) et \( g\in \GL(n,\eR)\). Nous vérifions le résultat séparément pour \( \tau_v\) et pour \( g\).

    D'une part,
    \begin{equation}
        B_{\tau_v(X)}=\frac{1}{ | \tau_v(X) | }\sum_{y\in \tau_v(X)}y=\frac{1}{ | X | }\sum_{x\in X}(x+v)=B_x+\frac{1}{ | X | }\sum_{x\in X}v=B_x+v=\tau_v(B_X).
    \end{equation}
    Nous avons utilisé le fait que \( X\) et \( \tau_v(X)\) possèdent le même nombre d'éléments, ainsi que le fait d'avoir une somme de \( | X |\) termes tous égaux à \( v\).

    D'autre part,
    \begin{equation}
        B_{g(X)}=\frac{1}{ | X | }\sum_{x\in X}g(x)=g\big( \frac{1}{ |X | }\sum_{x\in X}x \big)=g(B_X)
    \end{equation}
    où nous avons utilisé la linéarité de \( g\) dans tous ses retranchements.
\end{proof}

\begin{proposition}     \label{PROPooLAEBooWdcBoe}
    Points fixes d'un sous-groupe.
    \begin{enumerate}
        \item
            Soit \( H\) un sous-groupe finie de \( \Isom(\eR^n)\). Alors il existe \( v\in \eR^n\) tel que \( f(v)=v\) pour tout \( f\in H\).
        \item
            Si \( H\) est un sous-groupe de \( \Isom(\eR^n)\) n'acceptant pas de points fixes, alors il est infini.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le groupe \( H\) agit sur \( \eR^n\), et si \( x\in \eR^n\) nous pouvons considérer son orbite \( Hx\), qui est une partie finie de \( \eR^n\). Considérons son barycentre
    \begin{equation}
        v=B_{Hx}
    \end{equation}
    Soit \( f\in H\). Alors \( f(v)=f(B_{Hx})=B_{f(Hx)}=B_{Hx}=v\), donc \( v\) est fixé par \( H\).

    La seconde affirmation n'est rien d'autre que la contraposée de la première.
\end{proof}

\begin{proposition}     \label{PROPooEUFIooDUIYzi}
    À propos de groupes finis d'isométries.
    \begin{enumerate}
        \item
            Tout sous groupe finie de \( \Isom(\eR^n)\) est isomorphe à un sous-groupe fini de \( \gO(n)\).
        \item
            Tout sous-groupe fini de \( \Isom^+(\eR^n)\) est isomorphe à un sous-groupe fini de \( \SO(n)\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Soit \( H\) un sous-groupe fini de \( \Isom(\eR^n)\) et \( v\in \eR^n\) un élément fixé par \( H\) (comme garantit par la proposition \ref{PROPooLAEBooWdcBoe}). Nous posons
    \begin{equation}
        \begin{aligned}
            \phi\colon H&\to \Isom(\eR^n) \\
            f&\mapsto \tau_v^{-1}\circ f\circ \tau_v. 
        \end{aligned}
    \end{equation}

    \begin{subproof}
        \item[\( \phi\) est un homomorphisme]
            Les opération du type \( \phi=\AD(\tau_v)\) sont toujours des homomorphismes.
        \item[\( \phi\) consiste à extraire la partie linéaire]
            Si \( f=\tau_w\circ g\) alors
            \begin{subequations}
                \begin{align}
                    \phi(f)(x)&=(\tau_{-v}\circ\tau_w\circ g\circ\tau_v)(x)\\
                    &=\tau_{w-v}(   g(x)+g(v)  )\\
                    &=g(x)+g(v)-v+w
                \end{align}
            \end{subequations}
            Mais \( g(v)+w=f(v)\) et nous savons que \( f(v)=v\). Donc il ne reste que \( \phi(f)(x)=g(x)\).
        \item[\( \phi\) est injective]
            Si \( f=\tau_w\circ g\) vérifie \( \phi(f)=\id\), il faut en particulier que \( g=\id\). Mais \( H\) est fini et ne peut donc pas contenir de translations non triviales. Donc \( w=0\) et \( f=\id\).
    \end{subproof}
    Donc \( \phi\) est une injection à valeur dans les transformation linéaires de \( \Isom(\eR^n)\). Autrement dit, \( \phi\) est un isomorphisme entre \( H\) et son image, laquelle image est dans \( \gO(n)\).

    En ce qui concerne la seconde partie, si \( f\in\Isom^+(\eR^n)\), alors \( \phi(f)\) y est aussi, tout en étant linéaire. Donc \( \phi(f)\in \SO(n)\).
\end{proof}

L'extraction de la partie linéaire est injective ? Certe c'est prouvé, mais on peut se demander ce qu'il se passe si \( H\) contient deux éléments qui ont la même partie linéaire. Cela n'est pas possible parce si \( f_1=\tau_{w_1}\circ g\) et \( f_2=\tau_{w_2}\circ g\) sont dans \( H\) alors \( f_1f_2^{-1}=\tau_{w_1+w_2}\) est également dans \( H\), ce qui n'est pas possible si \( H\) est fini.

\begin{definition}[Groupe de symétrie d'une partie de \( \eR^n\)\cite{ooZYLAooXwWjLa}]
    Si \( Y\) est une partie de \( \eR^n\), nous définissons le \defe{groupe des symétries}{groupe!des symétries} de \( Y\) par
    \begin{equation}
        \Sym(Y)=\{ f\in\Isom(\eR^n)\tq f(Y)=Y \}.
    \end{equation}
    Nous définissons aussi le groupe des symétries propres de \( Y\) par
    \begin{equation}
        \Sym^+(Y)=\{ f\in\Isom^+(\eR^n)\tq f(Y)=Y \}.
    \end{equation}
\end{definition}

\begin{theorem}[\cite{ooZYLAooXwWjLa}]
    Soit \( Y\subset \eR^2\) tel que le groupe \( \Sym^+(Y)\) soit fini d'ordre \( n\). Alors c'est un groupe cyclique d'ordre \( n\).

    Si \( \Sym^+(Y)\) est fini, alors \( \Sym(Y)\) est soit cyclique d'ordre \( n\) soit isomorphe au groupe diédral d'ordre \( 2n\).
\end{theorem}

\begin{proof}
    Nous savons déjà par la proposition \ref{PROPooEUFIooDUIYzi} que \( \Sym^+(Y)\) est isomorphe à un sous-groupe \( H^+\) d'ordre \( n\) de \( \SO(2)\). Vérifions que ce groupe est cyclique. Si \( n=1\), c'est évident. Si \( n\geq 2\) alors nous savons que \( H^+\) est constitué de rotations d'angles dans \( \mathopen[ 0 , 2\pi \mathclose[\) et vu que c'est un ensemble fini, il possède une rotation d'angle minimal (à part zéro). Notons \( \alpha_0\) cet angle.

        Nous montrons que \( H^+\) est engendré par la rotation d'angle \( \alpha_0\). Soit une rotation d'angle \( \alpha\). Étant donné que \( \alpha_0<\alpha\) nous pouvons effectuer la division euclidienne\footnote{Théorème \ref{ThoDivisEuclide}.} de \( \alpha\) par \( \alpha_0\) et obtenir
        \begin{equation}
            \alpha=k\alpha_0+\beta
        \end{equation}
        avec \( \beta<\alpha_0\). Mézalors \( R(\beta)=R(\alpha)R(\alpha_0)^{-k}\) est également un élément du groupe. Cela contredit la minimalité dès que \( \beta\neq 0\). Avoir \( \beta=0\) revient à dire que \( \alpha\) est un multiple de \( \alpha_0\), ce qui signifie que le groupe \( H^+\) est cyclique engendré par \( \alpha_0\). 

        Notons au passage que nous avons automatiquement \( \alpha_0=\frac{ 2\pi }{ n }\) parce qu'il faut \( R(\alpha_0)^n=\id\). Nous avons prouvé que \( \Sym^+(Y) \) est cyclique d'ordre \( n\).

        Nous étudions maintenant le groupe \( \Sym(Y)\). Par la proposition \ref{PROPooEUFIooDUIYzi} nous avons un homomorphisme injectif
        \begin{equation}
            \phi\colon \Sym(Y)\to \gO(2),
        \end{equation}
        et en posant \( H=\phi\big( \Sym(Y) \big)\) nous avons un isomorphisme de groupes \( \phi\colon \Sym(Y)\to H\). Nous savons aussi que ce \( \phi\) se restreint en
        \begin{equation}
            \phi\colon   \Sym^+(Y) \to H^+\subset\SO(2)
        \end{equation}
        où \( H^+=\phi\big( \Sym^+(Y) \big)=H\cap\SO(2)\). Le groupe \( H^+\) est cyclique et est engendré par la rotation \( R(2\pi/n)\).

        Supposons un instant que \( H\subset \SO(2)\). Alors nous avons \( H=H^+\) et \( \phi\) est un isomorphisme entre \( \Sym(Y)\) et le groupe cyclique engendré par \( R(2\pi/n)\).

        Nous supposons à présent que \( H\) n'est pas un sous-ensemble de \( \SO(2)\). Quelles sont les isométries de \( \eR^2\) qui ne sont pas de déterminant \( 1\) ? Il faut regarder dans le théorème \ref{THOooVRNOooAgaVRN} quelles sont les isométries contenant un nombre impair de réflexions. Ce sont les réflexions et les réflexions glissées. Or il ne peut pas y avoir de réflexions glissées dans un groupe fini parce que si \( f\) est une réflexion glissée, tous les \( f^k\) sont différents.

        Nous en déduisons que si \( H\) n'est pas inclus à \( \SO(2)\), il contient une réflexion que nous nommons \( \sigma\). Nous allons en déduire que \( H\simeq H^+\times_{\AD}C_2\) où \( C_2=\{ \id,\sigma \}\). Si \( h\in H\) nous pouvons écrire 
        \begin{equation}
            h=(h\sigma^{\epsilon})\sigma^{\epsilon}
        \end{equation}
        pour n'importe quelle valeur de \( \epsilon\), et en particulier pour \( \epsilon=\pm 1\). 

        Si \( h\in \SO(2)\) alors nous écrivons \( h=h\epsilon^{0}\) et si \( h\notin\SO(2)\) nous écrivons \( h=(h\sigma)\sigma\). Vu que \( h\sigma\in\SO(2)\), cette dernière écriture est encore de la forme \( \SO(2)\times C_2\). Quoi qu'il en soit tout élément de \( H\) s'écrit comme un produit 
        \begin{equation}
            H=H^+C_2.
        \end{equation}
        Cette décomposition est unique parce que si \( h_1c_1=h_2c_2\) alors \( h_2^{-1}h_1=c_2c_1^{-1}\), et comme \( h_2^{-1}h_1\in H^+\) nous avons \( c_2c_1^{-1}\in H^+\) et donc \( c_1=c_2\). Partant nous avons aussi \( h_1=h_2\). Pour avoir le produit semi-direct il faut encore montrer que \( \AD(C_2)H^+\subset H^+\). Le seul cas à vérifier est \( \AD(\sigma)H^+\subset H^+\). Vu que les éléments de \( H^+\) sont caractérisés par le fait d'avoir un déterminant positif, nous avons 
        \begin{equation}
            \AD(\sigma)R(\alpha)=\sigma R(\alpha)\sigma^{-1}\in H^+.
        \end{equation}
\end{proof}

\begin{remark}
    Tout ceci est cohérent avec le théorème de Burnside \ref{ThooJLTit} parce que le sous-groupe fini de \( \SO(n)\) engendré par la rotation \( R(2\pi/n)\) est un groupe d'exposant fini, à savoir que si \( h\) est dans ce groupe, \( h^n=\id\).
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Sylvester}
%---------------------------------------------------------------------------------------------------------------------------

% TODO : Il y a une démonstration sur wikipédia, à voir.

\begin{theorem}[de Sylvester]   \label{ThoQFVsBCk}
    Soit $Q$ une forme quadratique réelle de signature \( (p,q)\). Alors pour toute base orthonormée on a
    \begin{subequations}
        \begin{align}
            p&=\Card\{ i\tq Q(e_i)>0 \}\\
            q&=\Card\{ i\tq Q(e_i)<0 \}.
        \end{align}
    \end{subequations}
    Le rang de \( Q\) est \( p+q\).

    Si \( A\) est la matrice de \( Q\) dans une base, alors il existe une matrice inversible \( P\) telle que
    \begin{equation}
        P^tAP=\begin{pmatrix}
            -\mtu_q    &       &       \\
                &   \mtu_p    &       \\
                &       &   0
        \end{pmatrix}.
    \end{equation}
\end{theorem}
\index{théorème!Sylvester}
\index{rang}
\index{matrice!semblables}
\index{forme!quadratique}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Groupe diédral}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecHibJId}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Définition et générateurs : vue géométrique}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{definition}  \label{DEFooIWZGooAinSOh}
    Le \defe{groupe diédral}{groupe!diédral} \( D_n\)\nomenclature[R]{\( D_n\)}{groupe diédral} est le groupe des isométries de \( \eR^2\) laissant invariant un polygone régulier à \( n\) côtés. 
\end{definition}
Le groupe diédral peut être vu comme le stabilisateur de l'ensemble
\begin{equation}
    \{  e^{2ik\pi/n},k=0,\ldots, n-1 \}
\end{equation}
dans le groupe des isométries affines de \( \eC^*\).
\index{groupe!agissant sur un ensemble!diédral}
\index{groupe!en géométrie}
\index{groupe!fini!diédral}
\index{groupe!permutation!diédral}
% TODO : prouver que les racines de l'unité forment un polygone régulier.

Si \( f\in D_n\), alors \( f( e^{2ik\pi/n}) \) doit être l'un des \(  e^{2ik'\pi/n}\), et vu que \( f\) conserve les longueurs dans \( \eC\), nous devons avoir
\begin{equation}
    1=d(0, e^{2ik\pi/n})=d\big( f(0), e^{2ik'\pi/n} \big).
\end{equation}
Donc \( f(0)\) est à l'intersection de tous les cercles de rayon \( 1\) centrés en les \(  e^{2ik\pi/n}\), ce qui montre que \( f(0))0\) (dès que \( n\geq 3\)). Par conséquent notre étude du groupe diédral ne doit prendre en compte que les isométries vectorielles de \( \eR^2\). En d'autres termes
\begin{equation}
    D_n\subset O(2,\eR).
\end{equation}

\begin{proposition}[\cite{tzHydF}]
    Le groupe \( D_n\) contient un sous groupe cyclique d'ordre \( 2\) et un sous groupe cyclique d'ordre \( n\).
\end{proposition}

\begin{proof}
    Si \( s\) est la réflexion d'axe \( \eR\), alors \( s\) est d'ordre \( 2\). De plus \( s\) est bien dans \( D_n\) parce que
    \begin{equation}    \label{EqSUshknP}
        s\big(  e^{2ki\pi/n} \big)= e^{2(n-k)i\pi/n}.
    \end{equation}

    De la même façon, la rotations d'angle \(2\pi/n\), que l'on note \( r\), agit sur les racines de l'unité et engendre un le groupe d'ordre \( n\) des rotations d'angle \(2 k\pi/n\).
\end{proof}

Notons que la conjugaison complexe ne fait pas spécialement partie du groupe \( D_n\). En effet pour \( n=3\) par exemple les points fixes sont \( A_1=(1,0)\), \( A_2=(-\frac{ 1 }{2},\frac{ \sqrt{3} }{2})\) et \( A_3=(\frac{ 1 }{2},-\frac{ \sqrt{3} }{2})\). La conjugaison complexe envoie évidemment \( A_1\) sur \( A_1\), mais pas du tout \( A_2\) sur \( A_3\).
%TODO : un dessin du triangle équilatéral serait pas mal ici.

\begin{proposition}[\cite{tzHydF}]
    Nous avons \( (sr)^2=\id\).
\end{proposition}

\begin{proof}
    Si \( z^n=1\), alors
    \begin{equation}
        (srsr)z=srs e^{2 i\pi/n}z=sr\big( e^{-2\pi i/n\bar z}\big)=s\bar z=z.
    \end{equation}
\end{proof}

\begin{proposition}[\cite{tzHydF}] \label{PropLDIPoZ}
    Le groupe diédral \( D_n\) est engendré par \( s\) et \( r\). De plus tous les éléments de \( D_n\) s'écrivent sous la forme \( s\circ r^m\).
\end{proposition}
\index{groupe!diédral!générateurs (preuve)}
\index{racine!de l'unité}
\index{géométrie!avec nombres complexes}
\index{géométrie!avec des groupes}
\index{isométrie!de l'espace euclidien \( \eR^2\)}

\begin{proof}
    Nous considérons les points \( A_0=1\) et \( A_k= e^{2ki\pi/n}\) avec \( k\in\{ 1,\ldots, n-1 \}\). Par convention, \( A_n=A_0\). L'action des éléments \( s\) et \( r\) sur ces points est
    \begin{subequations}
        \begin{align}
            r(A_k)&=A_{k+1}\\
            s(A_k)&=A_{n-k}.
        \end{align}
    \end{subequations}
    Cette dernière est l'équation \eqref{EqSUshknP}.
    
    Soit \( f\in D_n\). Étant donné que c'est une isométrie de \( \eR^2\) avec un point fixe (le point \( 0\)), \( f\) est soit une rotation soit une réflexion.
    %TODO : il faut démontrer ce point et mettre un lien vers ici.

    Supposons pour commencer que un des \( A_k\) est fixé par \( f\). Dans ce cas \( f\) a deux points fixes : \( O\) et \( A_k\) et est donc la réflexion d'axe \( (OA_k)\). Dans ce cas, nous avons \( f=s\circ r^{n-2k}\). En effet
    \begin{equation}
        s\circ r^{n-2k}(A_k)=s(A_{k+n-2k})=s(A_{n-k})=A_k.
    \end{equation}
    Donc \( O\) et \( A_k\) sont deux points fixes de l'isométrie \( f\); donc \( f\) est bien la réflexion sur le bon axe.

    Nous passons à présent au cas où \( f\) ne fixe aucun des \( A_k\). 
    \begin{enumerate}
        \item
            Supposons que \( f\) soit une rotation. Si \( f(A_k)=A_m\), alors l'angle de la rotation est 
            \begin{equation}
                \frac{ 2(m-k)\pi }{ n },
            \end{equation}
            et donc \( f=r^{m-k}\), qui est de la forme demandée.
        \item
            Supposons à présent que \( f\) soit une réflexion d'axe \( \Delta\). Cette fois, \( \Delta\) ne passe par aucun des points \( A_k\), par contre \( \Delta\) passe par \( 0\). Nous commençons par montrer que \( \Delta\) doit être la médiatrice d'un des côtés \( [A_p,A_{p+1}]\) du polygone. Vu que \( \Delta\) passe par \( O\) et n'est aucune des droites \( (OA_k)\), cette droite passe par l'intérieur d'un des triangles \( OA_pA_{p+1}\) et intersecte donc le côté correspondant.

            Notre tâche est de montrer que \( \Delta\) coupe \( [A_p,A_{p+1}]\) en son milieu. Dans ce cas, \( \Delta\) sera automatiquement perpendiculaire parce que le triangle \( OA_pA_{p+1}\) est isocèle en \( O\). Nommons \( l\) la longueur des côtés du polygone, \( P=\Delta\cap[A_p,A_{p+1}]\), \( x=d(A_p,P)\) et \( \delta=d(A_p,\Delta)\). Vu que \( f\) est la symétrie d'axe \( \Delta\), nous avons aussi \( d\big( f(A_p),\Delta \big)=\delta\) et \( d\big( A_p,f(A_p) \big)=2\delta\). D'autre part, par la définition de la distance, \( \delta<x\). Si \( x<\frac{ l }{2}\), alors \( \delta<\frac{ \delta }{2}\) et donc \( d\big( A_p,f(A_p) \big)<l\). Or cela est impossible parce que le polygone ne possède aucun sommet à distance plus courte que \( l\) de \( A_p\).

            De la même manière si \( x>\frac{ l }{2}\), nous raisonnons avec \( A_{p+1}\) pour obtenir une contradiction. Nous en concluons que la seule possibilité est \( x=\frac{ l }{2}\), et donc \( f(A_p)=A_{p+1}\). Montrons alors que \( f=s\circ r^{n-2p-1}\). Il faut montrer que c'est une réflexion qui envoie \( A_p\) sur \( A_{p+1}\). D'abord c'est une réflexion parce que
            \begin{equation}
                \det(sr^{n-2p-1})=\det(s)\det(r^{n-2p-1})=-1
            \end{equation}
            parce que \( \det(s)=-1\) alors que \( \det(r^k)=1\) parce que \( r\) est une rotation dans \( \SO(2)\). Ensuite nous avons
            \begin{equation}
                s\circ r^{n-2p-1}(A_p)=s(A_{p+n-2p-1})=s(A_{n-p-1})=A_{n-(n-p-1)}=A_{p+1}.
            \end{equation}

            Donc \( s\circ r^{n-2p-1}\) est bien une réflexion qui envoie \( A_p\) sur \( A_{p+1}\).

    \end{enumerate}
\end{proof}

\begin{corollary}   \label{CorWYITsWW}
La liste des éléments de \( D_n\) est 
\begin{equation}
    D_n=\{ 1,r,\ldots, r^{n-1},s,sr,\ldots, sr^{n-1} \}
\end{equation}
et \( | D_n |=2n\).
\end{corollary}

\begin{proof}
    Nous savons par la proposition \ref{PropLDIPoZ} que tous les élément de \( D_n\) s'écrivent sous la forme \( r^k\) ou \( sr^k\). Vu que \( r\) est d'ordre \( n\), il ne faut considérer que \( k\in\{ 1,\ldots, n-1 \}\). Les éléments \( 1\), \( r\),\ldots, \( r^{n-1}\) sont tous différents, et sont (pour des raisons de déterminant) tous différents des \( sr^k\). Les isométries \( sr^k\) sont toutes différentes entre elles pour essentiellement la même raison :
    \begin{equation}
        sr^k(A_p)=s(A_{p+k})=A_{n-p+k}
    \end{equation}
    donc si \( k\neq k'\), \( sr^k(A_p)\neq sr^{k'}(A_p)\). La liste des éléments de \( D_n\) est donc
    \begin{equation}
        D_n=\{ 1,r,\ldots, r^{n-1},s,sr,\ldots, sr^{n-1} \}
    \end{equation}
    et donc \( | D_n |=2n\).
\end{proof}

\begin{example}     \label{EXooHNYYooUDsKnm}
    Nous considérons le carré \( ABCD\) dans \( \eR^2\) et nous cherchons les isométries de \( \eR^2\) qui laissent le carré invariant. Nous nommons les points comme sur la figure \ref{LabelFigIsomCarre}. La symétrie d'axe vertical est nommée \( s\) et la rotation de \( 90\) degrés est notée \( r\).
    \newcommand{\CaptionFigIsomCarre}{Le carré dont nous étudions le groupe diédral.}
    \input{auto/pictures_tex/Fig_IsomCarre.pstricks}

    Il est facile de vérifier que toutes les symétries axiales peuvent être écrites sous la forme \( r^is\). De plus le groupe engendré par \( s\) agit sur le groupe engendré par \( r\) parce que
    \begin{equation}
        (srs^{-1})(A,B,C,D)=sr(B,A,D,C)=s(A,D,C,B)=(B,C,D,A),
    \end{equation}
    c'est à dire \( srs^{-1}=r^{-1}\). Nous sommes alors dans le cadre du corollaire \ref{CoroGohOZ} et nous pouvons écrire que
    \begin{equation}
        D_4=\gr(r)\times_{\sigma}\gr(s).
    \end{equation}
\end{example}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Générateurs : vue abstraite}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Nous allons montrer que \( D_n\) peut être décrit de façon abstraite en ne parlant que de ses générateurs. Nous considérons un groupe \( G\) engendré par des éléments \( a\) et \( b\) tels que
\begin{enumerate}
    \item
        \( a\) est d'ordre \( 2\),
    \item
        \( b\) est d'ordre \( n\) avec \( n\geq 3\),
    \item
        \( abab=e\).
\end{enumerate}
Nous allons prouver que ce groupe doit avoir la même liste d'éléments que celle du corollaire \ref{CorWYITsWW}.

\begin{proposition}[\cite{tzHydF}]
    Le groupe \( G\) n'est pas abélien.
\end{proposition}

\begin{proof}
    Nous savons que \( abab=e\), donc \( abab^{-1}=b^{-2}\), mais \( b^{-2}\neq e\) parce que \( b\) est d'ordre \( n>2\). Donc \( abab^{-1}\neq e\). En manipulant un peu :
    \begin{equation}
        e\neq abab^{-1}=(ab)(ba^{-1})^{-1}=(ab)(ba)^{-1}
    \end{equation}
    parce que \( a^{-1}=a\). Donc \( ab\neq ba\).
\end{proof}

\begin{lemma}[\cite{tzHydF}]        \label{LemKKXdqdL}
    Pour tout \( k\) entre \( 1\) et \( n-1\) nous avons
    \begin{equation}
        \AD(a)b^k=ab^ka^{-1}=ab^ka=b^{-k}.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous faisons la démonstration par récurrence. D'abord pour \( k=1\), nous devons avoir \( aba=b^{-1}\), ce qui est correct parce que par construction de \( G\) nous avons \( abab=e\). Ensuite nous supposons que le lemme tient pour \( k\) et nous regardons ce qu'il se passe avec \( k+1\) :
    \begin{equation}
            ab^{k+1}ba=ab^kba=\underbrace{ab^ka}_{b^{-k}}\underbrace{aba}_{b^{-1}}=b^{-k}b^{-1}=b^{-(k+1)}.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PROPooVQARooWuKHMZ}
    L'élément \( a\) n'est pas une puissance de \( b\).
\end{proposition}

\begin{proof}
    Supposons le contraire : \( a=b^k\). Dans ce cas nous aurions
    \begin{equation}
        e=(ab)(ab)=b^{k+1}b^{k+1}=b^{2k+2}=b^{2k}b^2=a^2b^2=b^2,
    \end{equation}
    ce qui signifierait que \( b\) est d'ordre \( 2\), ce qui est exclu par construction.
\end{proof}

\begin{proposition}[\cite{tzHydF}]      \label{PROPooEPVGooQjHRJp}
    La liste des éléments de \( G\) est donnée par
    \begin{equation}
        G=\{ 1,b,\cdots,b^{n-1},a,ab,\ldots, ab^{n-1} \}=\{ a^{\epsilon}b^k\}_{\substack{\epsilon=0,1\\k=0,\ldots, n-1}}
    \end{equation}
    Les éléments de ces listes sont distincts.
\end{proposition}

\begin{proof}
    Étant donné que \( a\) n'est pas une puissance de \( b\), les éléments \( 1\), \( a\), \( b\),\ldots, \( b^{n-1}\) sont distincts. De plus si \( k\) et \( m=k+p\) sont deux éléments distincts de \( \{ 1,\ldots, n-1 \}\), nous avons \( ab^k\neq ab^m\) parce que si \( ab^k=ab^{k+p}\), alors \( a=ab^p\) avec \( p<n\), ce qui est impossible. Pour la même raison, \( ab^k\neq e\), et \( ab^k\neq b^m\).

    Au final les éléments \( 1,a,b,\ldots, b^{n-1},ab,\ldots, ab^{n-1}\) sont tous différents. Nous devons encore voir qu'il n'y en a pas d'autres.

    Par définition le groupe \( G\) est engendré par \( a\) et \( b\), donc tout élément \( x\in G\) s'écrit $x=a^{m_1}b^{k_1}\ldots a^{m_r}b^{k_r}$ pour un certain \( r\) et avec pour tout \( i\), \( k_i\in\{ 1,\ldots, n-1 \}\) (sauf \( k_r\) qui peut être égal à zéro) et \( m_i=1\), sauf \( m_1\) qui peut être égal à zéro. Donc
    \begin{equation}
        x=a^mb^{k_1}ab^{k_2}a\ldots b^{k_{r-1}}ab^{k_r}
    \end{equation}
    où \( m\) et \( k_r\) peuvent éventuellement être zéro. En utilisant le lemme \ref{LemKKXdqdL} sous la forme \( b^{k_i}a=ab^{-k_i}\), quitte à changer les valeurs des exposants, nous pouvons passer tous les \( a \) à gauche et tous les \( b\) à droite pour finir sous la forme \( x=a^kb^m\). 

    Donc non, il n'existe pas d'autres éléments dans \( G\) que ceux déjà listés.
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LemooNFRIooPWuikH}
    Tout élément de \( G\) s'écrit de façon unique sous la forme \( a^{\epsilon}b^k\) ou \( b^ka^{\epsilon}\) avec \( \epsilon=0,1\) et \( k=0,\ldots, n-1\).
\end{lemma}

\begin{proof}
    Nous commençons par la forme \( a^{\epsilon}b^k\). L'existence est la proposition \ref{PROPooEPVGooQjHRJp}. Pour l'unicité nous supposons \( a^{\epsilon}b^k=a^{\sigma}b^l\) et nous décomposons en \( 4\).
    \begin{subproof}
        \item[\( \epsilon=0\), \( \sigma=0\)]
            Alors \( b^k=b^l\). Mais \( b\) étant d'ordre \( n\) et \( k,l\) étant égaux au maximum à \( n-1\), cette égalité implique \( k=l\).
        \item[\( \epsilon=0\), \( \sigma=1\)]
            Alors \( b^k=ab^l\), ce qui donne \( a=b^{k-l}\), ce qui est interdit par la proposition \ref{PROPooVQARooWuKHMZ}.
        \item[\( \epsilon=1\), \( \sigma=0\)]
            Même problème.
        \item[\( \epsilon=1\), \( \sigma=1\)]
            Encore une fois \( b^k=b^l\) implique \( k=l\).
    \end{subproof}
    En ce qui concerne la forme \( b^ka^{\epsilon}\), l'existence est à montrer. Soit l'élément \( g=a^{\epsilon}b^k\) et cherchons à le mettre sous la forme \( b^la^{\sigma}\). Si \( \epsilon=0\) c'est évident. Sinon \( \epsilon=1\) et nous avons par le lemme \ref{LemKKXdqdL}
    \begin{equation}
        ab^k=b^{-k}a^{-1}=b^{-k}b^na=b^{-k}a.
    \end{equation}
    En ce qui concerne l'unicité, nous refaisons \( 4\) cas pour \( b^ka^{\epsilon}=b^la^{\sigma}\) comme précédemment et ils se traitement exactement comme précédemment.
\end{proof}

\begin{theorem}
    Les groupes \( G\) et \( D_n\) sont isomorphes.
\end{theorem}

\begin{proof}
        Nous utilisons l'application
    \begin{equation}
        \begin{aligned}
            \psi\colon G&\to D_n \\
            a^kb^m&\mapsto s^kr^m. 
        \end{aligned}
    \end{equation}
    C'est évidemment bien défini et bijectif, mais c'est également un homomorphisme parce que si nous calculons \( \psi\) sur un produit, nous devons comparer
    \begin{equation}        \label{EqBULPilp}
        \psi\big( a^{k_1}b^{m_1}a^{k_2}b^{m_2} \big)
    \end{equation}
    avec
    \begin{equation}        \label{EqIVEIphI}
        \psi\big( a^{k_1}b^{m_1}\big)\psi\big(a^{k_2}b^{m_2} \big)= s^{k_1}r^{m_1}s^{k_2}r^{m_2}.
    \end{equation}
    Vu que \( D_n\) et \( G\) ont les mêmes propriétés qui permettent de permuter \( a\) et \( b\) ou \( s\) et \( r\), l'expression à l'intérieur du \( \psi\) dans \eqref{EqBULPilp} se simplifie en \( a^kb^m\) avec les même \( k\) et \( n\) que l'expression à droite dans \eqref{EqIVEIphI} ne se simplifie en \( s^kr^m\).
\end{proof}

\begin{corollary}
    Toutes les propriétés démontrées pour \( G\) sont vraies pour \( D_n\). En particulier, avec quelques redites :
    \begin{enumerate}
        \item
            Le groupe \( D_n\) peut être défini comme étant le groupe engendré par un élément \( s\) d'ordre \( 2\) et un élément \( r\) d'ordre \( n-1\) assujettis à la relation \( srsr=e\).
        \item
            Le groupe \( D_n\) n'est pas abélien.
        \item
            Pour tout \( k\in\{ 1,\ldots, n-1 \}\) nous avons \( sr^ks=r^{-k}\).
        \item
            L'élément \( s\) ne peut pas être obtenu comme une puissance de \( r\).
        \item
            La liste des éléments de \( D_n\) est
            \begin{equation}
                D_n=\{ 1,r,\ldots, r^{n-1},s,sr,\ldots, sr^{n-1} \}
            \end{equation}
        \item
            Le groupe diédral \( D_n\) est d'ordre \( 2n\).
    \end{enumerate}
\end{corollary}

\begin{proposition}
    En posant \( C_n=\{ r^k \}_{k=0,\ldots, n-1}\) et \( C_2=\{ a^{\epsilon} \}_{\epsilon=0,1}\), nous pouvons exprimer \( D_n\) comme le produit semi-direct
    \begin{equation}
        D_n=C_n\times_{\rho}C_2
    \end{equation}
    où \( \rho\) désigne l'action adjointe.
\end{proposition}

\begin{proof}
    L'isomorphisme est :
    \begin{equation}
        \begin{aligned}
            \psi\colon C_n\times_{\rho}C_2&\to D_n \\
            (b^k,a^{\epsilon})&\mapsto b^ka^{\epsilon}.
        \end{aligned}
    \end{equation}
    \begin{subproof}
        \item[Action adjointe]
            L'application \( \rho_{a^{\epsilon}}=\AD(a^{\epsilon})\) est toujours un homomorphisme. Vu que \( a^{\epsilon}\) est soit \( e\) soit \( a\), nous allons nous restreindre à \( a\) et oublier l'exposant \( \epsilon\). Il faut montrer que\( \AD(a)\in\Aut(C_n)\). En utilisant le lemme \ref{LemKKXdqdL},
            \begin{equation}
                \AD(a)b^k=ab^ka^{-1}=b^{-k}=b^{n-k}.
            \end{equation}
            L'application \( \AD(a)\colon C_n\to C_n\) est donc bijective et homomorphique. Ergo isomorphisme.
        \item[Injectif]
            Si \( \psi(b^k,a^{\epsilon})=\psi(b^l,a^{\sigma})\), alors par unicité du lemme \ref{LemooNFRIooPWuikH} nous avons \( k=l\) et \( \epsilon=\sigma\).
        \item[Surjectif]
            Par la partie «existence»  du lemme \ref{LemooNFRIooPWuikH}.
        \item[Homomorphisme]
            L'homomorphisme est toujours de mise lorsque l'on prend deux sous-groupes d'un même groupe (ici le groupe des isométries de \( \eR^2\)) et que l'on tente de faire un produit semi-direct en utilisant l'action adjointe. Dans notre cas, le calcul est : 
            \begin{equation}
                \psi\big( (b^k,a^{\epsilon})(b^l,a^{\sigma}) \big)=b^k\rho_{a^{\epsilon}}(b^l)a^{\epsilon+\sigma}=b^ka^{\epsilon}b^la^{-\epsilon}a^{\epsilon+\sigma}=b^ka^{\epsilon}b^la^{\sigma}=\psi(b^k,a^{\epsilon})\psi(b^l,a^{\sigma}).
            \end{equation}
    \end{subproof}
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Classes de conjugaison}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{subsubsecZQnBcgo}

Pour les classes de conjugaison du groupe diédral nous suivons \cite{HRIMAJJ}.

D'abord pour des raisons de déterminants\footnote{Vous notez qu'ici nous utilisons un argument qui utilise la définition de \( D_n\) comme isométries de \( \eR^2\). Si nous avions voulu à tout prix nous limiter à la définition «abstraite» en termes de générateurs, il aurait fallu trouver autre chose.}, les classes des éléments de la forme \( r^k\) et de la forme \( sr^k\) ne se mélangent pas. Nous notons \( C(x)\) la classe de conjugaison de \( x\), et \( y\cdot x=yxy^{-1}\).

Les relations que nous allons utiliser sont 
\begin{subequations}
    \begin{align}
        sr^ks=r^{-k}\\
        rs=sr^{-1}=sr^{n-1}.
    \end{align}
\end{subequations}

La classe de conjugaison qui ne rate jamais est bien entendu \( C(1)={1}\). Nous commençons les vraies festivités \( C(r^{m})\). D'abord \( r^k\cdot r^m=r^m\), ensuite
\begin{equation}
    (sr^k)\cdot r^m=sr^kr^mr^{-k}s^{-1}=sr^ms^{-1}=r^{-m}.
\end{equation}
Donc
\begin{equation}    \label{EqVFfFxgi}
    C(r^m)=\{ r^m,r^{-m} \}.
\end{equation}
À ce niveau il faut faire deux remarques. D'abord si \( m>\frac{ n }{2}\), alors \( C(r^m)\) est la classe de \( C^{n-m}\) avec \( n-m<\frac{ n }{2}\). Donc les classes que nous avons trouvées sont uniquement à lister avec \( m<\frac{ n }{2}\). Ensuite si \( m=\frac{ n }{2}\) alors \( r^m=r^{-m}\) et la classe est un singleton. Cela n'arrive que si \( n\) est pair.

Nous passons ensuite à \( C(s)\). Nous avons
\begin{equation}
    r^k\cdot s=r^ksr^{-k}=ssr^ksr^{-k}=sr^{-k}r^{-k}=sr^{n-2k},
\end{equation}
et
\begin{equation}
    (sr^k)\cdot s=\underbrace{sr^ks}_{r^{-k}}r^{-k}s^{-1}=r^{-2k}s=r^{n-2k}s=sr^{(n-1)(n-2k)}=sr^{n^2-2kn-n+2k}=sr^{2k}.
\end{equation}
donc
\begin{equation}
    C(s)=\{ sr^{n-2k},sr^{2k} \}_{k=0,\ldots, n-1}.
\end{equation}
Ici aussi l'écriture n'est pas optimale : peut-être que pour certains \( k\) il y a des doublons. Nous reportons l'écriture exacte à la discussion plus bas qui distinguera \( n\) pair de \( n\) impair. Notons juste que si \( n\) est pair, l'élément \( sr\) n'est pas dans la classe \( C(s)\).

Nous en faisons donc à présent le calcul en gardant en tête le fait qu'il n'a de sens que si \( n\) est pair. D'abord
\begin{equation}
    s\cdot (sr)=ssrs=rs=sr^{n-1}.
\end{equation}
Ensuite
\begin{equation}
    (sr^k)\cdot (sr)=sr^ksrr^{-k}s=r^{-2k+1}s=sr^{2k-1}.
\end{equation}
Avec \( k=\frac{ n }{2}\), cela rend \( s\cdot (sr)\), donc pas besoin de le recopier. Nous avons
\begin{equation}
    C(sr)=\{ sr^{2k-1} \}_{k=1,\ldots, n-1}.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Le compte pour $ n$ pair}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{SubsubsecROVmHuM}

Si \( n\) est pair, nous avons les classes
\begin{subequations}
    \begin{align}
        C(1)&=\{ 1 \}       &&& 1\text{ élément}\\
        C(r^m)&=\{ r^m,r^{m-1} \}&\text{ pour }&0<m<\frac{ n }{2}   & \frac{ n }{2}-1\text{ fois } 2\text{ éléments}\\
        C(r^{n/2})&=\{ r^{n/2} \}   &&&  1\text{ élément}\\ 
        C(s)&=\{ sr^{2k} \}_{k=0,\ldots, \frac{ n }{2}-1} &&&  \frac{ n }{2}\text{ éléments}\\
        C(sr)&=\{ sr^{2k+1} \}_{k=0,\ldots, \frac{ n }{2}-1} &&&  \frac{ n }{2}\text{ éléments}.
    \end{align}
\end{subequations}
Au total nous avons bien listé \( 2n\) éléments comme il se doit, dans \(  \frac{ n }{2}+3\) classes différentes.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Le compte pour $ n$ impair}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{GJIzDEP}

Si \( n\) est impair, nous avons les classes
\begin{subequations}
    \begin{align}
        C(1)&=\{ 1 \}       &&& 1\text{ élément}\\
        C(r^m)&=\{ r^m,r^{m-1} \}&\text{ pour }&0<m<\frac{ n-1 }{2}   & \frac{ n-1 }{2}\text{ fois } 2\text{ éléments}\\
        C(s)&=\{ sr^k \}_{k=0,\ldots, n-1} &&&  n\text{ éléments}
    \end{align}
\end{subequations}
Au total nous avons bien listé \( 2n\) éléments comme il se doit, dans \(  \frac{ n+3 }{2}\) classes différentes.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Applications : du dénombrement}
%---------------------------------------------------------------------------------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Le jeu de la roulette}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{pTqJLY}
\index{groupe!fini}
\index{groupe!de permutations}
\index{groupe!et géométrie}
\index{combinatoire}
\index{dénombrement}

Soit une roulette à \( n\) secteurs que nous voulons colorier en \( q\) couleurs\cite{HEBOFl}. Nous voulons savoir le nombre de possibilités à rotations près. Soit d'abord \( E\) l'ensemble des coloriages possibles sans contraintes; il y a naturellement \( q^n\) possibilités. Sur l'ensemble \( E\), le groupe cyclique \( G\) des rotations d'angle \( 2\pi/n\) agit. Deux coloriages étant identiques si ils sont reliés par une rotation, la réponse à notre problème est donné par le nombre d'orbites de l'action de \( G\) sur \( E\) qui sera donnée par la formule du théorème de Burnside \ref{THOooEFDMooDfosOw}. 

Nous devons calculer \( \Card\big( \Fix(g) \big)\) pour tout \( g\in G\). Soit \( g\), un élément d'ordre \( d\) dans \( G\). Si \( g\) agit sur la roulette, chaque secteur a une orbite contenant \( d\) éléments. Autrement dit, \( g\) divise la roulette en \( n/d\) secteurs. Un élément de \( E\) appartenant à \( \Fix(g)\) doit colorier ces \( n/d\) secteurs de façon uniforme; il y a \( q^{n/d}\) possibilités.

Il reste à déterminer le nombre d'éléments d'ordre \( d\) dans \( G\). Un élément de \( G\) est donné par un nombre complexe de la forme \(  e^{2ik\pi/n}\). Les éléments d'ordre \( d\) sont les racines primitives\footnote{Une racine non primitive \( 8\)ième de l'unité est par exemple \( i\). Certes \( i^8=1\), mais \( i^4=1\) aussi. Le nombre \( i\) est d'ordre \( 4\).} \( d\)ièmes de l'unité. Nous savons que --par définition-- il y a \( \varphi(d)\) telles racines primitives de l'unité. Bref il y a \( \varphi(d)\) éléments d'ordre \( d\) dans \( G\). 

La formule de Burnside nous donne maintenant le nombre d'orbites :
\begin{equation}
    \frac{1}{ n }\sum_{d|n}\varphi(d)q^{n/d}.
\end{equation}
Cela est le nombre de coloriage possibles de la roulette à \( n\) secteurs avec \( q\) couleurs.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{L'affaire du collier}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{siOQlG}

Nous avons maintenant des perles de \( q\) couleurs différentes et nous voulons en faire un collier à \( n\) perles. Cette fois non seulement les rotations donnent des colliers équivalents, mais en outre les symétries axiales (il est possible de retourner un collier, mais pas une roulette). Le groupe agissant sur \( E\) est maintenant le groupe diédral\footnote{Définition \ref{DEFooIWZGooAinSOh}.}\index{diédral}\index{groupe!diédral} \( D_n\) conservant un polygone a \( n\) sommets.

Nous devons séparer le cas \( n\) impair du cas \( n\) pair.

Si \( n\) est impair, alors les axes de symétries passent par un sommet par le milieu du côté opposé. Le groupe \( D_n\) contient \( n\) symétries axiales. Nous avons donc maintenant
\begin{equation}
    | G |=2n.
\end{equation}
Nous écrivons la formule de Burnside
\begin{equation}
    \Card(\Omega)=\frac{1}{ 2n }\sum_{g\in G}\Card\big( \Fix(g) \big).
\end{equation}
Si \( g\) est une rotation, le travail est déjà fait. Si \( g\) est une symétrie, nous avons le choix de la couleur du sommet par lequel passe l'axe et le choix de la couleur des \( (n-1)/2\) paires de sommets. Cela fait
\begin{equation}
    qq^{(n-1)/2}=q^{\frac{ n+1 }{2}}
\end{equation}
possibilités. Nous avons donc
\begin{equation}
    \Card(\Omega)=\frac{1}{ 2n }\left( \sum_{d|n}q^{n/d}\varphi(d)+nq^{\frac{ n+1 }{2}} \right).
\end{equation}

Si \( n\) est pair, le choses se compliquent un tout petit peu. En plus de symétries axiales passant par un sommet et le milieu du côté opposé, il y a les axes passant par deux sommets opposés. Pour colorier un collier en tenant compte d'une telle symétrie, nous pouvons choisir la couleur des deux perles par lesquelles passe l'axe ainsi que la couleur des \( (n-2)/2\) paires de perles. Cela fait en tout
\begin{equation}
    q^2q^{\frac{ n-2 }{2}}=q^{\frac{ n+2 }{2}}.
\end{equation}
Le groupe \( G\) contient \( n/2\) tels axes.

Notons que cette fois \( G\) ne contient plus que \( n/2\) symétries passant par un sommet et un côté. L'ordre de $G$ est donc encore \( 2n\). La formule de Burnside donne
\begin{equation}
    \Card(\Omega)=\frac{1}{ 2n }\left( \sum_{d\divides n}\varphi(d)q^{n/d}+\frac{ n }{2}q^{(n+2)/2}+\frac{ n }{2}q^{n/2} \right).
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Un peu de structure de \texorpdfstring{\( \gO(n)\)}{O(n)}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Angles orientés, rotations}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons défini les rotations planes en \ref{DEFooFUBYooHGXphm}, et montré que pour les rotations vectorielles, nous avions en réalité le groupe \( \SO(2)\) en le corollaire \ref{CORooVYUJooDbkIFY}. Il est temps de donner une forme matricielle aux rotations et de lier cela aux fonctions trigonométriques.

\begin{lemma}       \label{LEMooAJMAooXPSKtS}
    Si \( A\in \gO(2)\) alors il existe un unique \( \theta\in\mathopen[ 0 , 2\pi \mathclose[\) et un unique \( \epsilon=\pm 1\) tels que
    \begin{equation}
        A=\begin{pmatrix}
            \cos(\theta)    &   -\epsilon\sin(\theta)    \\ 
            \sin(\theta)    &   \epsilon\cos(\theta)    
        \end{pmatrix}
    \end{equation}
\end{lemma}

\begin{proof}
    Soit une matrice \( A=\begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}\) et imposons qu'elle soit dans \( \gO(2)\). Le fait que \( A\) soit orthogonale impose
    \begin{equation}
        \begin{pmatrix}
            a    &   c    \\ 
            b    &   d    
        \end{pmatrix}\begin{pmatrix}
            a    &   b    \\ 
            c    &   d    
        \end{pmatrix}=\begin{pmatrix}
            a^2+c^2    &   ab+cd    \\ 
            ab+cd    &   b^2+d^2    
        \end{pmatrix}=\begin{pmatrix}
            1    &   0    \\ 
            0    &   1    
        \end{pmatrix}.
    \end{equation}
    Nous avons alors le système
    \begin{subequations}
        \begin{numcases}{}
            a^2+b^2=1\\
            b^2+d^2=1\\
            ab+cd=0
        \end{numcases}
    \end{subequations}
    La proposition \ref{PROPooKSGXooOqGyZj} nous permet de déduire qu'il existe un unique \( \theta\in\mathopen[ 0 , 2\pi \mathclose[\) tel que \( a=\cos(\theta)\), \( c=\sin(\theta)\), ainsi que plusieurs \( \alpha\in \eR\) tel que \( b=\cos(\alpha)\), \( d=\sin(\alpha)\).

        Note : si nous voulons \( \alpha\in\mathopen[ 0 , 2\pi \mathclose[\), alors il y a unicité. Ici nous ne nous attachons pas à cette contrainte; nous savons qu'il en existe plusieurs, et nous allons en fixer un en fonction de \( \theta\). Le \( \alpha\) ainsi fixé ne sera peut-être pas dans \( \mathopen[ 0 , 2\pi \mathclose[\), mais ce ne sera pas grave.

        Les angles \( \theta\) et \( \alpha\) sont alors liés par la contrainte
        \begin{equation}
            \cos(\theta)\cos(\alpha)+\sin(\theta)\sin(\alpha)=0.
        \end{equation}
        Utilisant l'identité \eqref{EQooCVZAooQfocya} cela signifie que \( \cos(\theta-\alpha)=0\). Donc
        \begin{equation}
            \alpha\in\{ \theta+\frac{ \pi }{2}+k\pi \}_{k\in \eZ}.
        \end{equation}
        Si \( k\) est pair, ça donne 
        \begin{subequations}
            \begin{align}
                \cos(\alpha)=-\sin(\theta)\\
                \sin(\alpha)=\cos(\theta)
            \end{align}
        \end{subequations}
        et alors
        \begin{equation}        \label{EQooNAMKooKACIfd}
            A=\begin{pmatrix}
                \cos(\theta)    &   -\sin(\theta)    \\ 
                \sin(\theta)    &   \cos(\theta)    
            \end{pmatrix}.
        \end{equation}
        Si au contraire \( k\) est impair, alors
        \begin{subequations}
            \begin{align}
                \cos(\alpha)=\sin(\theta)\\
                \sin(\alpha)=-\cos(\theta),
            \end{align}
        \end{subequations}
        et
        \begin{equation}        \label{EQooJMYFooGgAiMJ}
            A=\begin{pmatrix}
                \cos(\theta)    &   \sin(\theta)    \\ 
                \sin(\theta)    &   -\cos(\theta)    
            \end{pmatrix}.
        \end{equation}

        Nous avons démontré qu'une matrice de \( \gO(2)\) était forcément d'une des deux formes \eqref{EQooNAMKooKACIfd} ou \eqref{EQooJMYFooGgAiMJ}. Il est maintenant facile de vérifier que ces deux matrices sont effectivement dans \( \gO(2)\).
\end{proof}


\begin{lemma}       \label{LEMooHRESooQTrpMz}
    Toute rotation admet une matrice de la forme (dans la base canonique)
    \begin{equation}
        \begin{pmatrix}
            \cos(\theta)    &   -\sin(\theta)    \\ 
            \sin(\theta)    &   \cos(\theta)    
        \end{pmatrix}
    \end{equation}
    avec \( \theta\in\mathopen[ 0 , 2\pi \mathclose[\).
\end{lemma}

\begin{proof}
    Vu que \( \SO(2)\) est la partie de \( \gO(2)\) constitué des matrices de déterminant \( 1\), nous pouvons reprendre la forme donnée par le lemme \ref{LEMooAJMAooXPSKtS} et fixer \( \epsilon\) par la contrainte sur le déterminant.

    Nous avons, en utilisant la relation du lemme \ref{LEMooAEFPooGSgOkF},
    \begin{equation}
        \det\begin{pmatrix}
            \cos(\theta)    &   -\epsilon\sin(\theta)    \\ 
            \sin(\theta)    &   \epsilon\cos(\theta)    
        \end{pmatrix}=\epsilon,
    \end{equation}
    et donc il faut et suffit de fixer \( \epsilon=1\).
\end{proof}

\begin{proposition}[\cite{ooGEXYooMTrOdH}]      \label{PROPooDWIMooQPkobw}
    Si \( u\) et \( v\) sont des vecteurs unitaires\footnote{De norme \( 1\).} de \( \eR^2\) alors il existe une unique rotation\footnote{Définition \ref{DEFooFUBYooHGXphm}.} \( f\) telle que \( f(u)=v\).
\end{proposition}

\begin{proof}
    C'est la proposition \ref{PROPooNXJKooEDOczh} appliquée à \( O=(0,0)\).
\end{proof}

Notons l'unicité. Nous ne faisons pas de différences entre \( R_{\theta}\) et \( R_{\theta+2\pi}\) et les autres \( R_{\theta+2k\pi}\). En particulier si une rotation \( T\) est donnée, dire «\( T=R_{\theta}\)» ne définit pas un nombre \( \theta\) de façon univoque. Par contre ça définit une classe modulo \( 2\pi\), c'est à dire un élément \( \theta\in \eR/2\pi\).

Nous avons déjà défini le groupe \( \SO(2)\) en la définition \ref{DEFooJLNQooBKTYUy} et nous avons déterminé ses matrices dans \( \eR^2\) en le lemme \ref{LEMooHRESooQTrpMz}. 

La proposition \ref{PROPooDWIMooQPkobw} donne une application
\begin{equation}
    T\colon S^1\times S^1\to \SO(2).
\end{equation}
Et nous avons une relation d'équivalence sur \( S^1\times S^1\) donnée par \( (u,v)\sim(u',v')\) si et seulement si il existe \( g\in\SO(2)\) telle que \( g(u)=u'\) et \( g(v)=v'\). 

\begin{definition}[Angle orienté\cite{ooGEXYooMTrOdH}]      \label{DEFooVBKIooWlHvod}
    Les classes de \( S^1\times S^1\) pour cette relation d'équivalence sont les \defe{angles orientés de vecteurs}{angle!orienté de vecteurs}. Nous notons \( [u,v]\) la classe de \( (u,v)\).
\end{definition}

\begin{proposition}     \label{PROPooIWJQooGQJBWR}
    Nous avons \( T(u,v)=T(u',v')\) si et seulement si \( (u,v)\sim(u',v')\).
\end{proposition}

\begin{proof}
    En utilisant la commutativité du groupe \( \SO(2)\) nous avons équivalence entre les affirmations suivantes :
    \begin{itemize}
        \item \( (u,v)\sim (u',v')\)
        \item \( T(u,u')=T(v',v')\)
        \item \( T(u,u')\circ T(u',v)=T(v,v')\circ T(u',v)\)
        \item
            \( T(u,v)=T(u',v')\).
    \end{itemize}
\end{proof}

\begin{proposition}
    Nous avons une bijection
    \begin{equation}
        \begin{aligned}
            S\colon \frac{ S^1\times S^1 }{ \sim }&\to \SO(2) \\
            [u,v]&\mapsto T(u,v). 
        \end{aligned}
    \end{equation}
\end{proposition}

\begin{proof}
    En plusieurs points.
    \begin{subproof}
    \item[\( S\) est bien définie]
        En effet si \( [u,v]=[z,t]\) alors \( T(u,v)=T(z,t)\).
    \item[Injectif]
        Si \( S[u,v]=S[z,t]\) alors \( T(u,v)=T(z,t)\), qui implique \( (u,v)\sim (z,t)\) par la proposition \ref{PROPooIWJQooGQJBWR}.
    \item[Surjectif]
        Nous avons \( R_{\theta}=T(u,R_{\theta}u)\).
    \end{subproof}
\end{proof}

\begin{definition}[Somme d'angles orientés\cite{ooGEXYooMTrOdH}]
    Si \( [u,v]\) et \( [z,t]\) sont des angles orientés, nous définissons la somme par
    \begin{equation}
        [u,v]+[z,t]=S^{-1}\Big( S[u,v]\circ S[z,t] \Big).
    \end{equation}
\end{definition}

\begin{lemma}       \label{LEMooWISVooYsStJp}
    Quelque propriétés des angles plats liées à la somme.
    \begin{enumerate}
        \item
            \( (S^1\times S^1)/\sim\) est un groupe commutatif.
        \item       \label{ITEMooBKTFooWbEvIU}
            Relations de Chasles :
            \begin{equation}
                [u,v]+[v,w]=[u,w].
            \end{equation}
        \item
            \( -[u,v]=[v,u]\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Pour la relation de Chasles, ça se base sur la propriété correspondante sur \( T\) :
    \begin{subequations}
        \begin{align}
            [u,v]+[v,w]&=S^{-1}\Big( T(u,v)\circ T(v,w) \Big)\\
            &=S^{-1}\big( T(u,w) \big)\\
            &=[u,w].
        \end{align}
    \end{subequations}
    Pour l'inverse, la vérification est que
    \begin{equation}
        [u,v]+[v,u]=[u,u]=0.
    \end{equation}
\end{proof}

\begin{definition}
    La \defe{mesure}{mesure!angle entre vecteurs} de l'angle orienté \( [u,v]\) est \( [\theta]_{2\pi}\) si \( T[u,v]=R_{\theta}\).
\end{definition}
Notons dans cette définition qu'écrire \( T[u,v]=R_{\theta}\) dans \( \SO(2)\) ne définit pas \( \theta\), mais seulement sa classe modulo \( 2\pi\). C'est pour cela que la mesure de l'angle orienté n'est également définie que modulo \( 2\pi\).

Pour la suite nous allons nous intéresser à des vecteurs qui ont, dans l'idée, un point de départ et un point d'arrivée. Si \( A,B\in \eR^2\) nous notons
\begin{equation}
    \vect{ AB }=\frac{ B-A }{ \| B-A \| }.
\end{equation}
C'est le vecteur unitaire dans la direction «de \( B\) vers $A$».

\begin{theorem}[Théorème de l'angle inscrit\cite{ooRGSCooNgALYH}]       \label{THOooQDNKooTlVmmj}
    Soit un cercle \( \Gamma\) de centre \( O\) et trois points distincts \( A,B,M\in \Gamma\). Alors
    \begin{equation}
        2(\vect{ MA },\vect{ MB })\in (\vect{ OA },\vect{ OB })_{2\pi}
    \end{equation}
    où l'indice \( 2\pi\) indique la classe modulo \( 2\pi\).
\end{theorem}

\begin{proof}
    Le triangle \( MOA\) est isocèle en \( O\), donc les angles à la base sont égaux. Et de plus la somme des angles est dans \( [\pi]_{2\pi}\). Bon, entre nous, nous savons que la somme des angles est exactement \( \pi\), mais comme nous n'avons pas défini les angles autrement que modulo \( \pi\), nous ne pouvons pas dire mieux. Donc
    \begin{equation}
        2(\vect{ AB },\vect{ AO })+(\vect{ OB },\vect{ OA })\in [\pi]_{2\pi}.
    \end{equation}
    Il faut être sûr de l'orientation de tout cela. Le nombre \( (\vect{ AB },\vect{ AO })\) est l'angle qui sert à amener \( \vect{ AB }\) sur \( \vect{ AO }\). Vu que nous l'avons choisit dans le sens trigonométrique, il faut bien prendre les autres dans le sens trigonométrique et utiliser \( (\vect{ OA }, \vect{ OB })\) et non \( (\vect{ OB },\vect{ OA })\).

\begin{center}
   \input{auto/pictures_tex/Fig_YQIDooBqpAdbIM.pstricks}
\end{center}

De la même manière sur le triangle \( MOB\) nous écrivons
\begin{equation}
    2(\vect{ MB },\vect{ MO })+(\vect{ OM },\vect{ OB })\in[\pi]_{2\pi}.
\end{equation}
Nous faisons la différence entre les deux équations en nous souvenant que \( -(\vect{ MB },\vect{ MO })=(\vect{ MO },\vect{ MB })\) et les relations de Chasles du lemme \ref{LEMooWISVooYsStJp}\ref{ITEMooBKTFooWbEvIU} nous avons :
\begin{equation}
    2(\vect{ MA },\vect{ MB })+(\vect{ OB },\vect{ OA })\in[0]_{2\pi}.
\end{equation}
\end{proof}

\begin{normaltext}
    Comment exprimer le fait qu'un angle orienté soit égal à \( \theta\) modulo \( \pi\) alors que les angles orientés sont des classes modulo \( 2\pi\) ? Nous ne pouvons certainement pas écrire
    \begin{equation}
        (u,v)=[\theta]_{\pi}
    \end{equation}
    parce que \( (u,v)\) est un élément de \( S^1\times S^1\) alors que \( [\theta]_{\pi}\) est un ensemble de nombres. Nous pouvons écrire
    \begin{equation}
        [u,v]\subset [\theta]_{\pi}.
    \end{equation}
    C'est cohérent parce que nous avons des deux côtés des ensembles de nombres. Les opérations permises sont l'égalité ou l'inclusion. L'égalité entre les deux ensembles n'est pas possible parce que la différence minimale ente deux éléments dans \( [u,v]\) est \( 2\pi\) alors que celle dans \( [\theta]_{\pi}\) est \( \pi\).

    Si \( u\) et \( v\) forment un angle droit, nous avons
    \begin{equation}
        [u,v]=\{ \frac{ \pi }{2}+2k\pi \}_{k\in \eZ}.
    \end{equation}
    Et cela est bien un sous-ensemble de \( [\pi/2]_{\pi}\).

    Pour exprimer que la différence entre deux angles orientés diffèrent de \( \pi\) nous devrions écrire :
    \begin{equation}
        [u,v]\subset[a,b]_{\pi}
    \end{equation}
    où le membre de droite signifie la classe modulo \( \pi\) d'un représentant de \( [a,b]\). 

    Nous allons cependant nous permettre d'écrire
    \begin{equation}
        [u,v]=[a,b]_{\pi}
    \end{equation}
    voire carrément
    \begin{equation}
        (u,v)=(a,b)_{\pi}.
    \end{equation}
    Cette dernière égalité devant être comprise comme voulant dire que l'angle pour passer de \( u\) à \( v\) est soit le même que celui pour alle de \( a\) à \( b\) soit ce dernier plus \( \pi\).
\end{normaltext}

\begin{theorem}[\cite{ooRGSCooNgALYH}]      \label{THOooUDUGooTJKDpO}
    Soient \( 4\) points distincts du plan \( A,B,C,D\). Ils sont alignés ou cocycliques\footnote{C'est à dire sur un même cercle.} si et seulement si
    \begin{equation}
        (\vect{ CA },\vect{ CB })=(\vect{ DA },\vect{ DB })_{\pi}.
    \end{equation}
\end{theorem}

Nous allons seulement démontrer l'implication directe.
\begin{proof}
    Si les quatre points sont alignés nous avons \( [\vect{ CA },\vect{ CB }]=[0]_{2\pi}\) et \( [\vect{ DA },\vect{ DB }]=[0]_{2\pi}\). En particulier nous avons
    \begin{equation}
        [\vect{ CA },\vect{ CB }]=[\vect{ DA },\vect{ DB }]
    \end{equation}
    et a fortiori l'égalité modulo \( \pi\) au lieu de \( 2\pi\).

    Nous nous relâchons en termes de notations. Si les quatre points sont cocycliques, nous pouvons utiliser le théorème de l'angle inscrit \ref{THOooQDNKooTlVmmj} dans les triangles \( ABC\) et \( ADB\) :
    \begin{subequations}
        \begin{align}
            2(\vect{ CA },\vect{ CB })=(\vect{ OA },\vect{ OB })_{2\pi}\\
            2(\vect{ DA },\vect{ DB })=(\vect{ OA },\vect{ OB })_{2\pi},
        \end{align}
    \end{subequations}
    ce qui donne \(  2(\vect{ CA },\vect{ CB })=2(\vect{ DA },\vect{ DB })_{2\pi}  \) et donc
    \begin{equation}
        (\vect{ CA },\vect{ CB })=(\vect{ DA },\vect{ DB })_{\pi}.
    \end{equation}

    Comme annoncé, nous ne faisons pas la preuve dans l'autre sens; elle peut être trouvée dans~\cite{ooRGSCooNgALYH}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Angles et nombres complexes}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooKNUVooUBKaWm}

Les nombres complexes peuvent être repérés par une norme et un angle, ce qui en fait un terrain propice à l'utilisation des angles orientés. Nous en ferons d'ailleurs usage dans \( \hat\eC=\eC\cup\{ \infty \}\) pour parler d'alignement, de cocyclicité et de birapport dans la proposition \ref{PROPooSGCJooLnOLCx}.

Soient deux éléments \( z_1,z_2\in \eC\). Nous les écrivons sous la forme \( z_1=r_1 e^{i\theta_1}\) et \( z_2=r_2 e^{i\theta_2}\); remarquons que cela ne définit \( \theta_i\) qu'à \( 2\pi\) près. Nous avons
\begin{equation}
    [z_1,z_2]=[\theta_2-\theta_1]_{2\pi}.
\end{equation}

Soient maintenant \( a,b,c,d\in \eC\). Nous écrivons \( \vect{ ab }\) le vecteur unitaire dans le sens «de \( a\) vers \( b\)», c'est à dire un multiple positif bien choisi du nombre \( b-a\). Nous notons \( \theta_{ab}\) l'argument du nombre complexe \( b-a\), et nous avons encore
\begin{equation}
    [\vect{ ab },\vect{ cd }]=[\theta_{ab}-\theta_{cd}].
\end{equation}

Avec toutes ces notations, ce qui est bien est que les produits et quotients de nombres complexes se comportent très bien par rapport aux angles : l'argument de \( a/b\) est \( \theta_a-\theta_b\) et en particulier l'argument de 
\begin{equation}
    \frac{ a-b }{ c-d }
\end{equation}
est dans la classe de l'angle orienté
\begin{equation}
    [\vect{ ba },\vect{ dc }].
\end{equation}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Valeurs propres dans \( \gO(n)\)}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{ooPWOHooHwgPzO}]      \label{PROPooVEJGooWnqtMm}
    Soit une matrice \( A\in O(n)\). Si \( \lambda\in \eC\) est une valeur propre de \( A\), alors \( \bar\lambda\) est également une valeur propre de \( A\), et de plus \( | \lambda |=1\).
\end{proposition}

\begin{proof}
    Dire que \( \lambda\in \eC\) est une valeur propre de \( A\) signifie qu'il exist \( x\in \eC^n\) (non nul) tel que \( Ax=\lambda x\). Vu que les éléments de la matrice \( A\) sont réels,
    \begin{equation}
        A\bar x=\bar A\bar x=\overline{ Ax }=\overline{ \lambda x }=\bar \lambda\bar x.
    \end{equation}
    Donc \( \bar \lambda\) est une valeur propre de \( A\) pour le vecteur propre \( \bar x\).

    Soit \( \lambda\)  une valeur propre de \( A\) de vecteur propre \( x\). Alors nous avons d'une part
    \begin{equation}
        \langle \overline{ Ax }, Ax\rangle =\langle A^tA\bar x, x\rangle =\langle x, \bar x\rangle ,
    \end{equation}
    et d'autre part
    \begin{equation}
        \langle \overline{ Ax }, Ax\rangle =\langle \bar \lambda \bar x, \lambda x\rangle =| \lambda |^2\langle \bar x, x\rangle .
    \end{equation}
    Vu que \( x\neq 0\) nous avons aussi \( \langle \bar x, x\rangle \neq 0\). Par conséquent \( | \lambda |^2=1\) et \( | \lambda |=1\).
\end{proof}

\begin{lemma}[\cite{ooPWOHooHwgPzO}]        \label{LEMooNEDQooNRmASH}
    Soit un espace vectoriel euclidien \( E\) de dimension finie et une isométrie \( f\) de \( E\). Soit \( F\) un sous-espace de \( E\) stable par \( f\). Alors \( F^{\perp}\) est stable par \( f\).
\end{lemma}

\begin{proof}
    La restriction \( f_F\colon F\to F\) est encore une isométrie; elle est donc inversible : pour tout $y\in F$, il existe \( x\in F\) tel que \( y=f(x)\). Soit \( a\in F^{\perp}\); nous montrons que \( f(a)\in F^{\perp}\). Soit donc \( y\in F\) et calculons :
    \begin{equation}
        \langle y, f(a)\rangle =\langle f(x), f(a)\rangle =\langle x,a, \rangle =0
    \end{equation}
    parce que \( x\in F^{\perp}\).
\end{proof}

\begin{proposition}[\cite{ooPWOHooHwgPzO}]      \label{PROPooOMORooWzsrDB}
    Soit une isométrie \( f\colon \eR^3\to \eR^3\).
    \begin{enumerate}
        \item
            L'application linéaire \( f\) possède au moins une valeur propre réelle qui vaut \( \pm 1\).
        \item
            Il existe une base orthonormée de \( \eR^3\) dans laquelle la matrice de \( f\) est de la forme
            \begin{equation}
                \begin{pmatrix}
                    \lambda    &   0    &   0    \\
                    0    &   \cos(\theta)    &   -\epsilon\cos(\theta)    \\
                    0    &   \sin(\theta)    &   \epsilon\cos(\theta)
                \end{pmatrix}
            \end{equation}
            avec \( \epsilon,\lambda=\pm 1\) et \( \theta\in \mathopen[ 0 , 2\pi \mathclose[\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le polynôme caractéristique de \( f\), donné par \( \det(f-\lambda\id)\), est à coefficients réels et de degré \( 3\). Il possède dont au moins une solution réelle par le corollaire \ref{CORooKKNWooWEQukb}. Soit donc une valeur propre réelle \( \lambda\) de \( \chi_f\); par le lemme \ref{PROPooVEJGooWnqtMm} nous avons \( \lambda=\pm 1\). Soit \( u_1\) le vecteur propre correspondant. Nous notons \( F\) l'espace engendré par \( u_1\).

    Nous avons \( f(F)=F\) et donc \( f(F^{\perp})=F^{\perp}\) par le lemme \ref{LEMooNEDQooNRmASH}. Soit une base orthonormée \( \{ u_2,u_3 \}\) de \( F^{\perp}\) et la matrice \( B\) de la restriction \( f_{p}\) à \( F^{\perp}\). Vu que l'application \( f_p\) est une isométrie de \( F^{\perp}\), la matrice \( B\) est, par le lemme \ref{LEMooAJMAooXPSKtS}, de la forme
    \begin{equation}
        B=\begin{pmatrix}
            \cos(\theta)    &   -\epsilon\sin(\theta)    \\ 
            \sin(\theta)    &   \epsilon\cos(\theta)    
        \end{pmatrix}
    \end{equation}
    pour un certain \( \theta\in\mathopen[ 0 , 2\pi \mathclose[\) et \( \epsilon=\pm 1\).

    Dans la base \( \{u_1,u_2,u_3\}\) de \( \eR^3\), la matrice de \( f\) est alors
    \begin{equation}
        \begin{pmatrix}
            \lambda    &   0    \\ 
            0    &   B    
        \end{pmatrix},
    \end{equation}
    comme annoncé.
\end{proof}

Pour classifier les isométries de \( \eR^3\), nous pouvons nous baser sur les possibilités de la matrice donne dans le lemme \ref{PROPooOMORooWzsrDB}. Il y a essentiellement quatre possibilités suivant les valeurs de \( \lambda=\pm 1\) et \( \epsilon=\pm 1\).

\begin{subproof}
    \item[Si \( \epsilon=\lambda=1\)] Alors la matrice est
    \begin{equation}
        A=\begin{pmatrix}
            1    &   0    &   0    \\
            0    &   \cos(\theta)    &   -\sin(\theta)    \\
            0    &   \sin(\theta)    &   \cos(\theta)
        \end{pmatrix}
    \end{equation}
    et l'isométrie correspondante est la rotation d'angle \( -\theta\) autour de la droite de \( u_1\).

    \item[Si \( \epsilon=\lambda=-1\)]
    Alors la matrice est
    \begin{equation}
        A=\begin{pmatrix}
            -1    &  0     &   0    \\
            0    &   \cos(\theta)    &   \sin(\theta)    \\
            0    &   \sin(\theta)    &   -\cos(\theta)
        \end{pmatrix}
    \end{equation}
    Cette application est plus subtile, parce que même dans le plan \( \Span(u_2,u_3)\), ce n'est pas une rotation. Nous allons montrer qu'il s'agit d'une réflexion autour de la droite d'angle \( \theta/2\) dans le plan \( \Span(u_2,u_3)\). Nous nommons \( D\) cette droite. Dans la base \( \{ u_1,u_2,u_3 \}\), les points de cette droite sont de la forme\footnote{Les plus acharnés remarquerons que \( \{ u_1,u_2,u_3 \}\) est un ensemble, qui est une base. Mais un ensemble n'est pas ordonné, alors que pour écrire l'équation de droite qui suit, nous supposons un ordre. Je laisse au tel lecteur le soin de trouver une bonne notation.}
    \begin{equation}
        \big( 0,\cos(\theta/2),\sin(\theta/2) \big).
    \end{equation}
    
    L'image de \( u_1\) par cette réflexion est \(-u_1\), c'est clair.

    Faisons en détail l'image de \( u_3\). Nous devons démontrer que la droite \( D\) coupe le segment \( \mathopen[ u_3 , A(u_3) \mathclose]\) perpendiculairement en son milieu.
    
    Dans le plan \( \Span(u_2,u_3)\) nous avons \( u_3=\begin{pmatrix}
        0    \\ 
        1    
    \end{pmatrix}\) et \( A(u_3)=\begin{pmatrix}
        \sin(\theta)    \\ 
        \cos(\theta)    
    \end{pmatrix}\). Le milieu du segment \( \mathopen[ u_3 , A(u_3) \mathclose]\) est le point
    \begin{equation}
        M=\left( \frac{ \sin(\theta) }{2},\frac{ 1-\cos(\theta) }{2} \right).
    \end{equation}
    Les formules de duplication d'angle du corollaire \ref{CORooQZDQooWjMXTF} nous permettent d'écrire \( \sin(\theta)\) et \( \cos(\theta)\) en fonction de \( \sin(\theta/2)\) et \( \cos(\theta/2)\), et donc d'exprimer le point \( M\) de la façon suivante :
    \begin{subequations}
        \begin{align}
            M&=\left( \cos(\theta/2)\sin(\theta/2),\frac{ 1-\big( \cos^2(\theta/2)-\sin^2(\theta/2) \big) }{2} \right)\\
            &=\big( \cos(\theta/2)\sin(\theta/2),\sin^2(\theta/2) \big)\\
            &=\sin(\theta/2)\big( \cos(\theta/2),\sin(\theta/2) \big).
        \end{align}
    \end{subequations}
    Ce point fait donc partie de la droite \( D\). La droite \( D\) coupe le segment \( \mathopen[ u_3 , A(u_3) \mathclose]\) en son milieu.

    En ce qui concerne l'orthogonalité, nous calculons le produit scalaire
    \begin{equation}
            \big( A(u_3)-u_3 \big)\cdot\begin{pmatrix}
                \cos(\theta/2)    \\ 
                \sin(\theta/2)    
            \end{pmatrix}
            =\sin(\theta)\cos(\theta/2)-\big( 1+\cos(\theta) \big)\sin(\theta/2)=0
    \end{equation}
    où nous avons encore utilisé les duplications d'angles et le fait que \( 1=\cos^2(\theta/2)+\sin^2(\theta/2)\) (lemme \ref{LEMooAEFPooGSgOkF}).

    \item[Si \( \epsilon=-1\) et \( \lambda=1\)] Alors la matrice est
        \begin{equation}
            A=\begin{pmatrix}
                1    &   0    &   0    \\
                0    &   \cos(\theta)    &   \sin(\theta)    \\
                0    &   \sin(\theta)    &   -\cos(\theta)
            \end{pmatrix}.
        \end{equation}
        C'est la symétrie orthogonale par le plan engendré par \( u_1\) et \( v=\cos(\theta/2)u_2+\sin(\theta/2)u_3\).

        Le vecteur \( u_1\) est bien évidemment préservé par \( A\). En ce qui concerne le vecteur \( v\),
        \begin{equation}
            A(v)=\cos(\theta/2)\begin{pmatrix}
                0    \\ 
                \cos(\theta)    \\ 
                \sin(\theta)    
            \end{pmatrix}+\sin(\theta/2)\begin{pmatrix}
                0    \\ 
                -\sin(\theta)    \\ 
                \cos(\theta)    
            \end{pmatrix}=
            \begin{pmatrix}
                0    \\ 
                \cos(\theta/2)    \\ 
                \sin(\theta/2)    
            \end{pmatrix}=v.
        \end{equation}
        Nous avons sauté quelque étapes de calcul mettant en scène les formules de duplication d'angle : exprimer \( \cos(\theta)=\cos^2(\theta/2)-\sin^2(\theta/2)\) et \( \sin(\theta)=2\cos(\theta/2)\sin(\theta/2)\).

        Pour achever, nous devons trouver un vecteur \( w\) perpendiculaire au plan, et montrer qu'il est envoyé par \( A\) sur \( -w\). Un vecteur \( w=xu_1+yu_2+zu_3\) est perpendiculaire au plan si les deux égalités suivantes sont satisfaites :
        \begin{subequations}
            \begin{align}
                \big( \cos(\theta/2)u_2+\sin(\theta/2)u_3 \big)\cdot (xu_1+yu_2+zu_3)=0\\
                u_1\cdot(xu_1+yu_2+zu_3)=0.
            \end{align}
        \end{subequations}
        Nous avons immédiatement \( x=0\) et ensuite la relation
        \begin{equation}        \label{EQooXQMDooTvwrWk}
            y\cos(\theta/2)+z\sin(\theta/2)=0.
        \end{equation}
        En ne regardant que les deux dernières composantes pour alléger l'écriture,
        \begin{equation}
            A(w)=y\begin{pmatrix}
                \cos(\theta)    \\ 
                \sin(\theta)    
            \end{pmatrix}+z\begin{pmatrix}
                \sin(\theta)    \\ 
                -\cos(\theta)    
            \end{pmatrix}.
        \end{equation}
        Le but est de montrer que cela est égal à \( -y\cos(\theta/2)-z\sin(\theta/2)\).

        Notons \( c=\cos(\theta/2)\) et \( s=\sin(\theta/2)\). Alors \( A(w)_2=y(c^2-s^2)+2zcs\). Évacuons tout de suite les deux cas limite : si \( c=0\) alors \( A(w)_2=-y\) (parce que \( s=\pm1\)) et c'est bon. Si \( s=0\), alors \( A(w)_2=y\), mais la relation \eqref{EQooXQMDooTvwrWk} donne \( y=0\), donc c'est bon aussi. Dans la cas générique, \( z=-yc/2\) et
        \begin{equation}
            A(w)_2=y(c^2-s^2)-2cs\frac{ yc }{ s }=-y(c^2+s^2)=-y.
        \end{equation}

        En ce qui concerne \( A(w)_3\), c'est très similaire :
        \begin{equation}
            A(w)_3=2ysc-z(c^2-s^2).
        \end{equation}
        Avec \( z=0\) c'est \( -z\), donc c'est bon. Avec \( c=0\) c'est \( z\) mais \( z=0\). Et pour le cas générique, la substitution \( y=-zs/c\) donne le résultat.
        

    \item[Si \( \epsilon=1\) et \( \lambda=-1\)] Alors la matrice est
        \begin{equation}
            A=\begin{pmatrix}
                -1    &   0    &   0    \\
                0    &   \cos(\theta)    &   -\sin(\theta)    \\
                0    &   \sin(\theta)    &   \cos(\theta)
            \end{pmatrix}.
        \end{equation}
        Cela est la composition entre la symétrie de plan \( \Span(u_2,u_3)\) et la rotation d'angle \( \theta\) dans ce plan.
\end{subproof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Sous-groupes finis de $\SO(2)$}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}
    Tout sous groupe fini de \( \SO(2)\) est cyclique.
\end{lemma}

\begin{proof}
    Soit un sous-groupe fini \( G\) dans \( \SO(2)\). Soit \( g\in G\); l'ensemble \( \{ g^k \}_{k\in \eZ}\) est un sous-groupe fini de \( G\).
\end{proof}
<++>

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Sous-groupes finis de \( \SO(3)\)}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{MonCerveau}]       \label{LEMooWIMMooXOCfSt}
    Points fixes pour \( \SO(3)\).
    \begin{enumerate}
        \item
            Tout élément de \( \SO(3)\) possède une droite de point fixes.
        \item
            Tout élément non trivial de \( \SO(3)\) possède une seule droite de points fixes.
    \end{enumerate}
\end{lemma}

\begin{proof}
    Le polynôme caractéristique d'un élément de \( \SO(3)\) est de degré trois et possède dont (en comptant les multiplicités) trois racines dont une réelle par le corollaire \ref{CORooKKNWooWEQukb}. Vu que nous sommes en dimension impaire, le coefficient du terme de degré \( 3\) est \( -1\) et le polynôme caractéristique de \( g\in\SO(3)\) s'écrit
    \begin{equation}
        \chi_g(X)=-(X-\lambda_1)(X-\bar\lambda_1)(X-s)
    \end{equation}
    avec \( s=\pm1 \) que nous allons tout de suite fixer. Nous savons que \( \det(g)=\chi_g(0)\) mais aussi que \( \det(g)=1\). Donc
    \begin{equation}
        1=\det(g)=\chi_g(0)=\lambda_1\bar\lambda_1 s=s.
    \end{equation}
    Tout cela pour dire que tout élément de \( \SO(3)\) possède une valeur propre égale à \( 1\), et donc une droite de points fixes.

    Pour continuer, supposons que \( g\) possède deux droites distinctes de points fixes. En particulier \( g\) fixe un plan. Une base orthonormée de \( \eR^3\) peut être choisie en prenant deux vecteurs \( e_1\), \( e_2\) dans ce plan et un vecteur \( e_3\) perpendiculaire au plan.

    Vu que \( g\) est une isométrie, la base reste orthonormée sous l'action de \( g\). Donc \( g\) a pour matrice
    \begin{equation}
        \begin{pmatrix}
            1    &   0    &   0    \\
            0    &   1    &   0    \\
            0    &   0    &   \pm 1
        \end{pmatrix}.
    \end{equation}
    Pour que le déterminant soit \( 1\), il faut que la matrice soit l'identité.
\end{proof}

\begin{proposition}[\cite{ooYODPooHeNKiQ,fJhCTE,ooBWVZooJiWRvf}]
    Les sous-groupes finis de \( \SO(3)\) sont :
    \begin{multicols}{2}
        \begin{enumerate}
            \item
                les groupes cycliques \( \eZ/n\eZ\),
            \item
                les groupes diédraux \( D_n\),
            \item
                le groupe alterné \( A_4\),
            \item
                le groupe alterné \( A_5\)
            \item
                le groups symétrique \( S_4\).
        \end{enumerate}
    \end{multicols}
\end{proposition}

\begin{proof}
    Soit \( G\), un sous-groupe fini de \( \SO(3)\). Par la proposition \ref{PropKBCXooOuEZcS}, les éléments de \( G\) sont des isométries de \( \eR^3\), et le lemme \ref{LEMooWIMMooXOCfSt} dit que tout élément de \( G\) possède une droite de points fixes.

    Un point de la sphère unité fixé par \( g\in G\) est un \defe{pôle}{pôle} de \( g\). Nous nommons \( \Omega\) l'ensemble des pôles des éléments non triviaux de \( G\).
    \begin{subproof}
        \item[Une action]
            Le groupe \( G\) agit sur \( \Omega\). En effet si \( x\in \Omega\), alors \( x\) est fixé par un élément \( g\). Montrons que \( h(x)\) est également fixé par un élément de \( G\). Par dur : \( (h^{-1}gh)h(x)=h(x)\); donc \( h(x)\) est un pôle de \( h^{-1} gh\).

        \item[Les fixateurs sont cycliques]

            Nous montrons à présent que pour tout \( x\in\Omega\), le sous-groupe \( \Fix(x)\) est cyclique. Soit donc \( x\in\Omega\), le plan orthogonal \( \sigma=\Span(x)^{\perp}\) et \( h\in \Fix(x)\). Nous avons \( h(\sigma)=\sigma\). En effet si \( y\in \sigma\) nous avons 
            \begin{equation}
                0=y\cdot x=h(y)\cdot h(x)=h(y)\cdot x,
            \end{equation}
            donc \( h(y)\) est perpendiculaire à \( x\). L'inclusion inverse se démontre de même : si \( y\in \sigma\) alors \( y=h\big( h^{-1}(y) \big)\) alors que \( h^{-1}(y)\in \sigma\).

            La restriction de \( h\) à \( \sigma\) est une isométrie de \( \sigma\). Prenant une isométrie \( f\colon \sigma\to \eR^2\), l'application
            \begin{equation}
                \begin{aligned}
                    \varphi\colon \Fix(x)&\to \SO(2) \\
                    h&\mapsto f\circ h\circ f^{-1}. 
                \end{aligned}
            \end{equation}
            est un morphisme injectif de groupes. En effet nous avons d'une part
            \begin{equation}
                \varphi(hh')=f\circ h\circ h'\circ f^{-1}=fhf^{-1}fh'f^{-1}=\varphi(h)\varphi(h'),
            \end{equation}
            d'où le morphisme. Et d'autre part, si \( \varphi(h)=\varphi(h')\) alors \( f\circ h\circ f^{-1}=g\circ h'\circ f^{-1}\), qui donne immédiatement \( h=h'\).

            Nous en déduisons que \( \Fix(x)\) est isomorphe à un sous-groupe de \( SO(2)\) (l'image de \( \varphi\)).
    \end{subproof}
    <++>
\end{proof}
<++>

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Coordonnées polaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Les coordonnées polaires}
%---------------------------------------------------------------------------------------------------------------------------

On a vu qu'un point $M$ dans $\eR^2$ peut être représenté par ses abscisses $x$ et ses ordonnées $y$. Nous pouvons également déterminer le même point $M$ en donnant un angle et une distance comme montré sur la figure \ref{LabelFigJWINooSfKCeA}.
\newcommand{\CaptionFigJWINooSfKCeA}{Un point en coordonnées polaires est donné par sa distance à l'origine et par l'angle qu'il faut avec l'horizontale.}
\input{auto/pictures_tex/Fig_JWINooSfKCeA.pstricks}


Le même point $M$ peut être décrit indifféremment avec les coordonnées $(x,y)$ ou bien avec $(r,\theta)$.

\begin{remark}
	L'angle $\theta$ d'un point n'étant a priori défini qu'à un multiple de $2\pi$ près, nous convenons de toujours choisir un angle $0\leq\theta<2\pi$. Par ailleurs l'angle $\theta$ n'est pas défini si $(x,y)=(0,0)$.

	La coordonnée $r$ est toujours positive.
\end{remark}

En utilisant la trigonométrie, il est facile de trouver le changement de variable qui donne $(x,y)$ en fonction de $(r,\theta)$:
\begin{subequations}		\label{EqrthetaxyPoal}
	\begin{numcases}{}
		x=r\cos(\theta)\\
		y=r\sin(\theta).
	\end{numcases}
\end{subequations}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Transformation inverse : théorie}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Voyons la question inverse : comment retrouver $r$ et $\theta$ si on connait $x$ et $y$ ? Tout d'abord,
\begin{equation}
	r=\sqrt{x^2+y^2}
\end{equation}
parce que la coordonnée $r$ est la distance entre l'origine et $(x,y)$. Comment trouver l'angle ? Nous supposons $(x,y)\neq (0,0)$. Si $x=0$, alors le point est sur l'axe vertical et nous avons
\begin{equation}
	\theta=\begin{cases}
		\pi/2	&	\text{si }y>0\\
		3\pi/2	&	 \text{si }y<0
	\end{cases}
\end{equation}
Notez que si $y<0$, conformément à notre convention $\theta\geq 0$, nous avons noté $\frac{ 3\pi }{2}$ et non $-\frac{ \pi }{ 2 }$.

Supposons maintenant le cas général avec $x\neq 0$. Les équations \eqref{EqrthetaxyPoal} montrent que
\begin{equation}
	\tan(\theta)=\frac{ y }{ x }.
\end{equation}
Nous avons donc
\begin{equation}
	\theta=\tan^{-1}\left( \frac{ y }{ x } \right).
\end{equation}
La fonction inverse de la fonction tangente est celle définie plus haut.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Transformation inverse : pratique}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le code suivant utilise \href{http://www.sagemath.org}{Sage}.

\lstinputlisting{tex/frido/calculAngle.py}

Son exécution retourne :
\begin{verbatim}
(sqrt(2), 1/4*pi)
(sqrt(5), pi - arctan(1/2))
(6, 1/6*pi)
\end{verbatim}
Notez que ce sont des valeurs \emph{exactes}. Ce ne sont pas des approximations, Sage travaille de façon symbolique.

Voici un tableau qui rappelle les valeurs à retenir pour les fonctions sinus, cosinus et tangente.\label{PGooIMQFooTnBdIl}
\begin{equation*}
    \begin{array}[]{|c|c|c|c|}
      \hline
      x&\sin(x)&\cos(x)&\tan(x)\\
      \hline
      0&0&1&0\\
      \hline
      \pi/6&1/2&\sqrt{3}/2&\sqrt{3}/3\\
      \hline
      \pi/6&1/2&\sqrt{3}/2&\sqrt{3}/3\\
      \hline
      \pi/4&\sqrt{2}/2&\sqrt{2}/2&1\\
      \hline
      \pi/3&\sqrt{3}/2&1/2&\sqrt{3}\\
      \hline
      \pi/2&1&0&\text{N.D.}\\
      \hline
    \end{array}
\end{equation*}
où «N.D.»  signifie «non défini».

Rappelons le graphe de la fonction sinus :
\begin{center}
   \input{auto/pictures_tex/Fig_TWHooJjXEtS.pstricks}
\end{center}
celui de la fonction cosinus :
\begin{center}
   \input{auto/pictures_tex/Fig_JJAooWpimYW.pstricks}
\end{center}


\begin{lemma}
  Pour toute valeur de $x\in \eR$ on a $|\sin(x)|\leq |x|$. 
\end{lemma}

\begin{proof}
        Nous séparons des cas en fonction des valeurs.
    \begin{itemize}
    \item Si $0\leq x\leq \pi/2$ alors le sinus de $x$ est la longueur du cathète verticale du triangle rectangle de sommets $O = (0,0)$, $A = (\cos(x), \sin(x))$ et $B = (\cos(x), 0)$. Le triangle de sommets $A$, $B$ et $C = (1, 0)$ est aussi rectangle et nous savons que chacun des cathètes ne peut pas être plus long que l'hypoténuse. Donc $\sin(x)$ est inférieur à la longueur du segment $AC$. À son tour le segment $AC$ ne peut pas être plus long que l'arc de cercle $\wideparen{A0C}$, car le chemin le plus court entre deux points du plan est toujours donné par un morceau de droite. La longueur de l'arc de cercle $\frown{AC}$ est \emph{par définition} la mesure en radiants de l'angle $\widehat{AOC}$, qui est $x$ et on a l'inégalité $\sin(x)\leq x$. 
    \item Si $-\pi/2\leq x\leq 0$ le m\^eme raisonnement que au point précédent permet de conclure que $\sin(x)\leq |x|$.
    \item Nous savons par ailleurs que la fonction sinus prend ses valeurs dans l'intervalle $[-1,1]$ et donc pour tout $x$ tel que $|x|\geq \pi/2 \equiv 1,57\ldots$ on a forcement $|\sin(x)|\leq |x|$.  
    \end{itemize}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Coordonnées polaires comme changement de variables pour intégrales}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooUUAKooMSJHsL}

Le théorème \ref{THOooUMIWooZUtUSg} manque un peu d'exemples. Nous allons en voir quelque uns maintenant.

\begin{example} 
\textbf{Coordonnées polaires : }On veut évaluer l'intégrale de la fonction $f(x,y)= x^2+y^2$ sur la région $V$ suivante :
\[
V=\{(x,y) \in \eR^2\,\vert\, x^2+y^2\leq 1,\, x>0,\, y>0\}.
\]
On peut faire le calcul directement,
\[
\int_{V}f(x,y)\, dV=\int_0^1\int_0^{\sqrt{1-x^2}}x^2+y^2\, dy\,dx=\int_0^1x^2\sqrt{1-x^2} + \frac{(1-x^2)^{3/2}}{3}\, dx  
\] 
mais c'est un peu ennuyeux. On peut simplifier beaucoup les calculs avec un changement de variables vers les coordonnées polaires. Dans ce cas, on sait bien que le difféomorphisme à utiliser est $\phi(r,\theta)=(r\cos \theta, r\sin\theta)$. Le jacobien  $J_{\phi}$ est
\begin{equation}
 J_{\phi}(r, \theta)= \left\vert\begin{array}{cc}
\cos \theta & \sin \theta \\
-r\sin \theta  & r\cos \theta
\end{array}\right\vert= r,
\end{equation}
qui est toujours positif. La fonction $f$ peut s'écrire comme $f(\phi(r,\theta))=r^2$ et $\phi^{-1}(V)=]0,1]\times]0, \pi/2[$.  
La formule du changement de variables nous donne
\[
\int_{V}f(x,y)\, dV=\int_0^{\pi/2}\int_0^{1}r^3 dr\,d\theta=\int_0^{\pi/2}\frac{1}{4}\,d\theta=\frac{\pi}{8}.  
\] 
\end{example}

\begin{example}
\textbf{Coordonnées cylindriques : }On veut calculer le volume de la région $A$ définie par  l'intersection entre la boule unité et le cylindre qui a pour base un disque de rayon $1/2$ centré en $(0, 1/2)$
\[
A=\{(x,y,z) \in\eR^3 \,\vert\, x^2+y^2+z^1\leq 1\}\cap\{(x,y,z) \in \eR^3\,\vert\, x^2+(y-1/2)^2\leq 1/4\}.
\]
On peut décrire $A$ en coordonnées cylindriques
\begin{equation}
  \begin{aligned}
    A=\Big\{(r,\theta,z) &\in ]0, +\infty[\times [-\pi,\pi[\times \eR\,\vert\,\\
& -\pi/2<\theta<\pi, \, 0<r\leq \sin\theta, \, -\sqrt{1-r^2}\leq z\leq\sqrt{1-r^2} \Big\}.
  \end{aligned}
\end{equation}
Le jacobien de ce changement de variables,  $J_{cyl}$, est
\begin{equation}
 J_{cyl}(r, \theta), z= \left\vert\begin{array}{ccc}
\cos \theta & \sin \theta & 0\\
-r\sin \theta  & r\cos \theta &0 \\
0&0&
\end{array}\right\vert= r,
\end{equation}
qui est toujours positif. Le volume de $A$ est donc
\[
\int_{\eR^3}\chi_{A}(x,y,z)\, dV=\int_{-\pi/2}^{\pi/2}\int_0^{\sin\theta}\int_{-\sqrt{1-r^2}}^{\sqrt{1-r^2}} r dz\,dr\,d\theta=\frac{2\pi}{8}+\frac{8}{9}.  
\] 
\end{example}

\begin{example}
\textbf{Volume d'un solide de révolution : }Soit $g:[a,b]\to\eR_+$ une fonction continue et positive. On dit que le solide $A$ décrit par
\[
A=\left\{(x,y,z)\in\eR^3\, \vert \, z\in[a,b], \,\sqrt{x^2+y^2}\leq g^2(z) \right\}
\]
est un solide de révolution. Afin de calculer son volume, on peut décrire $A$ en coordonnées cylindriques, 
\[
A=\left\{(r,\theta,z) \in ]0, +\infty[\times [-\pi,\pi[\times \eR\,\vert\, a\leq z\leq b, \, 0<r^2\leq g^2(z) \right\}.
\]
Le jacobien de ce changement de variables est  $J_{cyl}=r$, comme dans l'exemple précédent. Le volume de $A$ est donc
\[
\int_{\eR^3}\chi_{A}(x,y,z)\, dV=\int_a^{b}\int_{-\pi}^{\pi}\int_{0}^{g(z)} r  \,dr\,d\theta\, dz=\int_a^{b} \pi g^2(z) \, dz.
\] 
Cette formule peut être utilisée pour tout solide de révolution. 
\end{example}

\begin{example}
\textbf{Coordonnées sphériques : }On veut calculer le volume du cornet de glace  $A$ 
\[
A=\left\{(x,y,z)\in\eR^3\, \vert \, (x,y)\in \mathbb{S}^2, \,\sqrt{x^2+y^2}\leq z\leq \sqrt{1-x^2-y^2} \right\}. 
\]
On peut décrire $A$ en coordonnées sphériques. 
\[
A=\{(\rho,\theta,\phi) \in ]0, +\infty[\times [-\pi,\pi[\times [0,\pi[\,\vert\, 0<\phi\leq\pi/4, \, 0<\rho\leq 1 \}.
\]
Le jacobien de ce changement de variables  $J_{sph}$ est
\begin{equation}
 J_{sph}(\rho, \theta, \phi)= \left\vert\begin{array}{ccc}
\cos \theta \sin\phi & \sin \theta\sin\phi & \cos\phi\\
-\rho\sin \theta\sin\phi  & \rho\cos \theta\sin\phi & 0 \\
\rho\cos\theta\cos\phi&\rho\sin\theta\cos\phi& -\rho\sin\phi
\end{array}\right\vert= \rho^2\sin\phi,
\end{equation}
Le volume de $A$ est donc
\[
\int_{\eR^3}\chi_{A}(x,y,z)\, dV=\int_{-\pi}^{\pi}\int_0^{\pi/4}\int_{0}^{1}\rho^2\sin\phi \,d\rho\,d\phi\,d\theta=\frac{2\pi}{3}\left(1-\frac{1}{\sqrt{2}}\right).  
\] 
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Changement de variables}
%---------------------------------------------------------------------------------------------------------------------------

Le domaine $E=\{ (x,y)\in\eR^2\tq x^2+y^2<1 \}$ s'écrit plus facilement $E=\{ (r,\theta)\tq r<1 \}$ en coordonnées polaires. Le passage aux coordonnées polaire permet de transformer une intégration sur un domaine rond à une intégration sur le domaine rectangulaire $\mathopen]0,2\pi\mathclose[\times\mathopen]0,1\mathclose[$. La question est évidement de savoir si nous pouvons écrire
\begin{equation}
	\int_Ef=\int_{0}^{2\pi}\int_0^1f(r\cos\theta,r\sin\theta)drd\theta.
\end{equation}
Hélas, non; la vie n'est pas aussi simple.

\begin{theorem}
Soit $g\colon A\to B$ un difféomorphisme. Soient $F\subset B$ un ensemble mesurable et borné et $f\colon F\to \eR$ une fonction bornée et intégrable. Supposons que $g^{-1}(F)$ soit borné et que $Jg$ soit borné sur $g^{-1}(F)$. Alors
\begin{equation}
	\int_Ff(x)dy=\int_{g^{-1}(F)f\big( g(x) \big)}| Jg(x) |dx
\end{equation}
\end{theorem}
Pour rappel, $Jg$ est le déterminant de la matrice \href{http://fr.wikipedia.org/wiki/Matrice_jacobienne}{jacobienne} (aucun lien de \href{http://fr.wikipedia.org/wiki/Jacob}{parenté}) donnée par
\begin{equation}
	Jg=\det\begin{pmatrix}
	\partial_xg_1	&	\partial_yg_1	\\ 
	\partial_xg_2	&	\partial_tg_2	
\end{pmatrix}.
\end{equation}
Un \defe{difféomorphisme}{difféomorphisme} est une application $g\colon A\to B$ telle que $g$ et $g^{-1}\colon B\to A$ soient de classe $C^1$.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Coordonnées polaires}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Les coordonnées polaires sont données par le difféomorphisme
\begin{equation}
	\begin{aligned}
		g\colon \mathopen]0,\infty\mathclose[\times\mathopen]0,2\pi\mathclose[ &\to\eR^2\setminus D\\
		(r,\theta)&\mapsto \big( r\cos(\theta),r\sin(\theta) \big)
	\end{aligned}
\end{equation}
où $D$ est la demi droite $y=0$, $x\geq 0$. Le fait que les coordonnées polaires ne soient pas un difféomorphisme sur tout $\eR^2$ n'est pas un problème pour l'intégration parce que le manque de difféomorphisme est de mesure nulle dans $\eR^2$. Le jacobien est donné par
\begin{equation}
	Jg=\det\begin{pmatrix}
	\partial_rx	&	\partial_{\theta}x	\\ 
	\partial_ry	&	\partial_{\theta}y
\end{pmatrix}=\det\begin{pmatrix}
	\cos(\theta)	&	-r\sin(\theta)	\\ 
	\sin(\theta)	&	r\cos(\theta)	
\end{pmatrix}=r.
\end{equation}

\begin{example}    
    Montrons comment intégrer la fonction $f(x,y)=\sqrt{1-x^2-y^2}$ sur le domaine délimité par la droite $y=x$ et le cercle $x^2+y^2=y$, représenté sur la figure \ref{LabelFigQXyVaKD}. Pour trouver le centre et le rayon du cercle $x^2+y^2=y$, nous commençons par écrire $x^2+y^2-y=0$, et ensuite nous reformons le carré : $y^2-y=(y-\frac{ 1 }{2})^2-\frac{1}{ 4 }$.
    \newcommand{\CaptionFigQXyVaKD}{Passage en polaire pour intégrer sur un morceau de cercle.}
\input{auto/pictures_tex/Fig_QXyVaKD.pstricks}

    Le passage en polaire transforme les équations du bord du domaine en
    \begin{equation}
        \begin{aligned}[]
            \cos(\theta)&=\sin(\theta)\\
            r^2&=r\sin(\theta).
        \end{aligned}
    \end{equation}
    L'angle $\theta$ parcours donc $\mathopen] 0 , \pi/4 \mathclose[$, et le rayon, pour chacun de ces $\theta$ parcours $\mathopen] 0 , \sin(\theta) \mathclose[$. La fonction à intégrer se note maintenant $f(r,\theta)=\sqrt{1-r^2}$. Donc l'intégrale à calculer est
    \begin{equation}		\label{PgRapIntMultFubiniBoutCercle}
        \int_{0}^{\pi/4}\left( \int_0^{\sin(\theta)}\sqrt{1-r^2}r\,rd \right).
    \end{equation}
    Remarquez la présence d'un $r$ supplémentaire pour le jacobien.

    Notez que les coordonnées du point $P$ sont $(1,1)$.
\end{example}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Coordonnées sphériques}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Les coordonnées sphériques sont données par
\begin{equation}		\label{EqChmVarSpherique}
	\left\{
\begin{array}{lllll}
x=r\cos\theta\sin\varphi	&			&r\in\mathopen] 0 , \infty \mathclose[\\
y=r\sin\theta\sin\varphi	&	\text{avec}	&\theta\in\mathopen] 0 , 2\pi \mathclose[\\
z=r\cos\varphi			&			&\phi\in\mathopen] 0 , \pi \mathclose[.
\end{array}
\right.
\end{equation}
Le jacobien associé est $Jg(r,\theta,\varphi)=-r^2\sin\varphi$. Rappelons que ce qui rentre dans l'intégrale est la valeur absolue du jacobien.

Si nous voulons calculer le volume de la sphère de rayon $R$, nous écrivons donc
\begin{equation}
	\int_0^Rdr\int_{0}^{2\pi}d\theta\int_0^{\pi}r^2 \sin(\phi)d\phi=4\pi R=\frac{ 4 }{ 3 }\pi R^3.
\end{equation}
Ici, la valeur absolue n'est pas importante parce que lorsque $\phi\in\mathopen] 0,\pi ,  \mathclose[$, le sinus de $\phi$ est positif.

Des petits malins pourraient remarquer que le changement de variable \eqref{EqChmVarSpherique} est encore une paramétrisation de $\eR^3$ si on intervertit le domaine des angles : 
\begin{equation}
	\begin{aligned}[]
		\theta&\colon 0 \to \pi\\
		\phi	&\colon 0\to 2\pi,
	\end{aligned}
\end{equation}
alors nous paramétrons encore parfaitement bien la sphère, mais hélas
\begin{equation}		\label{EqVolumeIncorrectSphere}
	\int_0^Rdr\int_{0}^{\pi}d\theta\int_0^{2\pi}r^2 \sin(\phi)d\phi=0.
\end{equation}
Pourquoi ces «nouvelles» coordonnées sphériques sont-elles mauvaises ? Il y a que quand l'angle $\phi$ parcours $\mathopen] 0 , 2\pi \mathclose[$, son sinus n'est plus toujours positif, donc la \emph{valeur absolue} du jacobien n'est plus $r^2\sin(\phi)$, mais $r^2\sin(\phi)$ pour les $\phi$ entre $0$ et $\pi$, puis $-r^2\sin(\phi)$ pour $\phi$ entre $\pi$ et $2\pi$. Donc l'intégrale \eqref{EqVolumeIncorrectSphere} n'est pas correcte. Il faut la remplacer par
\begin{equation}
	\int_0^Rdr\int_{0}^{\pi}d\theta\int_0^{\pi}r^2 \sin(\phi)d\phi- \int_0^Rdr\int_{0}^{\pi}d\theta\int_{\pi}^{2\pi}r^2 \sin(\phi)d\phi = \frac{ 4 }{ 3 }\pi R^3
\end{equation}

\subsection{Coordonnées polaires}
Soit $T$ la fonction de $]0, +\infty[\times \eR$ dans $\eR^2\setminus\{(0,0)\}$ définie par
\begin{equation}
  \begin{array}{lccc}
    T: &]0, +\infty[\times \eR & \to & \eR^2\setminus\{(0,0)\}\\
 & (r, \theta)&\mapsto& (r\cos \theta, r \sin \theta),
  \end{array}
\end{equation}
Cette fonction est surjective. Elle est bijective sur chaque bande de la forme  $]0, +\infty[\times [a-\pi,a+\pi[$. Si $a=0$ l'inverse de $T$  est la fonction $T^{-1}(x,y)= (\sqrt{x^2+y^2}, \arctg (y/x))$. Soit $P=(x,y)$ un élément dans $\eR^2$, on dit que $r=\sqrt{x^2+y^2}$ est le rayon de $P$ et que $\theta=\arctg (y/x) $ est son argument principal. L'origine ne peut pas être décrite en coordonnées polaires parce que si son rayon est manifestement zéro, on ne peut pas lui associer une valeur univoque de l'angle $\theta$. 

\begin{example}
L'équation du cercle de rayon $a$ et centre $(0, 0)$ en coordonnées polaires est $r=a$. 
\end{example}

\begin{example}
	Une équation possible pour la demi-droite $x=y$, $x>0$,  est $\theta=\pi/4$.         
\end{example}

%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++   
\subsection{Coordonnées cylindriques}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Soit $T$ la fonction de $]0, +\infty[\times \eR^2$ dans $\eR^3\setminus\{(0,0,0)\}$ définie par
\begin{equation}
  \begin{array}{lccc}
    T: &]0, +\infty[\times \eR\times \eR & \to & \eR^3\setminus\{(0,0,0)\}\\
 & (r, \theta, z)&\mapsto& (r\cos \theta, r \sin \theta, z),
  \end{array}
\end{equation}
Cette fonction est surjective. Elle est bijective sur chaque bande de la forme  $]0, +\infty[\times [a-\pi,a+\pi[\times \eR$, $a$ dans $\eR$. Il n'y a presque rien de nouveau par rapport aux coordonnées polaires. Les coordonnées  cylindriques sont intéressantes si on décrit un objet invariant par rapport aux rotations autour de l'axe des $z$. 

\begin{example}
Il faut savoir ce que décrivent les équations les plus simples en coordonnées cylindriques, 
\begin{itemize}
\item $r\leq a$, pour $a$ constant dans  $]0, +\infty[$, est le cylindre de hauteur infinie qui a pour axe l'axe des $z$ et pour base le disque de rayon $a$ centré à l'origine, 
\item $r= a$ est  la surface du cylindre,
\item $\theta = b$ est un demi-plan ouvert et sa fermeture contient l'axe des $z$,
\item $z=c$ est un plan parallèle au plan $x$-$y$. 
\end{itemize}
\end{example}

\begin{example}
  Un demi-cône qui a  son sommet en l'origine et  pour axe l'axe des $z$ est décrit par $z=d r$.  Si $d$ est positif  il s'agit  de la moitié supérieure du cône, si $d<0$ de la moitié inférieure.
\end{example}

\begin{example}
 De même,  la sphère de rayon $a$ et centrée à l'origine est l'assemblage des calottes $z=\sqrt{a^2-r^2}$ et $z=-\sqrt{a^2-r^2}$. 
\end{example}

%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++   
\subsection{Coordonnées sphériques}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit $T$ la fonction de $]0, +\infty[\times \eR^2$ dans $\eR^3\setminus\{(0,0,0)\}$ définie par
\begin{equation}
  \begin{array}{lccc}
    T: &]0, +\infty[\times \eR\times \eR & \to & \eR^3\setminus\{(0,0,0)\}\\
 & (\rho, \theta, \phi)&\mapsto& (\rho\cos \theta\sin \phi, \rho \sin \theta\sin \phi, \rho\cos \phi),
  \end{array}
\end{equation}
Cette fonction est surjective. Elle est bijective sur chaque bande de la forme  $]0, +\infty[\times [a-\pi,a+\pi[\times [b-\pi/2, b+\pi/2[$, $a$ et $b$ dans $\eR$.  Si $a =0$ et $b=-\pi/2$ la fonction inverse $T^{-1}$ est donnée donnée
\begin{equation}
  \begin{array}{lccc}
    T: &\eR^3\setminus\{(0,0,0)\} & \to & ]0, +\infty[\times [-\pi,\pi[\times [0, \pi[\\
 & (x,y,z)&\mapsto& \left(\sqrt{x^2+y^2+z^2}, \arctg \frac{y}{x}, \arccos \left(\frac{z}{\sqrt{x^2+y^2+z^2}}\right)\right). 
  \end{array}
\end{equation}
Soit $ P$ un point dans $\eR^3$. L'angle $\phi$ est l'angle entre le demi-axe positif des $z$ et le vecteur $\overrightarrow{OP}$, $\rho$ est la norme de $\overrightarrow{OP}$ et $\theta$ est l'argument en coordonnées polaires de la projection de $\overrightarrow{OP}$ sur le plan $x$-$y$.  

\begin{remark}
	Dans la littérature, les angles $\theta$ et $\phi$ sont parfois inversés (voire, changent de nom, par exemple $\varphi$ au lieu de $\phi$). Il faut donc être très prudent lorsqu'on veut utiliser dans un cours des formules données dans un autre cours.
\end{remark}

\begin{example}
Il faut connaître le sens des équations plus simples, 
\begin{itemize}
\item $\rho\leq a$, pour $a$ constant dans  $]0, +\infty[$, est la boule fermée de rayon $a$ centrée à l'origine, 
\item $\rho= a$ est  la sphère de rayon $a$ centrée à l'origine,
\item $\theta = b$ est un demi-plan ouvert et sa fermeture contient l'axe des $z$,
\item $\phi= c$ est un demi-cône qui a  son sommet à l'origine et  pour axe l'axe des $z$.  Si $c$ est positif  il s'agit  de la moitié supérieure du cône, si $d<0$ de la moitié inférieure. 
\end{itemize}
 \end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
 \subsection{Coordonnées cylindriques et sphériques}
%---------------------------------------------------------------------------------------------------------------------------

Les \defe{coordonnées cylindriques}{coordonnées!cylindrique} sont un perfectionnement des coordonnées polaires. Il s'agit simplement de donner le point $(x,y,z)$ en faisant la conversion $(x,y)\mapsto(r,\theta)$ et en gardant le $z$. Les formules de passage sont
\begin{subequations}
	\begin{numcases}{}
		x=r\cos(\theta)\\
		y=r\sin(\theta)\\
		z=z.
	\end{numcases}
\end{subequations}

Les \defe{coordonnées sphériques}{coordonnées!sphériques} sont ce qu'on appelle les «méridiens» et «longitudes» en géographie. Les formules de transformation sont 
\begin{subequations}		%\label{SubEqsCoordSphe}
	\begin{numcases}{}
		x=\rho\sin(\theta)\cos(\varphi)\\
		y=\rho\sin(\theta)\sin(\varphi)\\
		z=\rho\cos(\theta)
	\end{numcases}
\end{subequations}
avec $0\leq\theta\leq\pi$ et $0\leq\varphi<2\pi$.

\begin{remark}
	Attention : d'un livre à l'autre les conventions sur les noms des angles changent. N'essayez donc pas d'étudier par cœur des formules concernant les coordonnées sphériques trouvées autre part. Par exemple sur le premier dessin de \href{http://fr.wikipedia.org/wiki/Coordonnées_sphériques}{wikipédia}, l'angle $\varphi$ est noté $\theta$ et l'angle $\theta$ est noté $\Phi$. Mais vous noterez que sur cette même page, les convention de noms de ces angles changent plusieurs fois.
\end{remark}

Trouvons le changement inverse, c'est à dire trouvons $\rho$, $\theta$ et $\varphi$ en termes de $x$, $y$ et $z$. D'abord nous avons
\begin{equation}
	\rho=\sqrt{x^2+y^2+z^2}.
\end{equation}
Ensuite nous savons que
\begin{equation}
	\cos(\theta)=\frac{ z }{ \rho }
\end{equation}
détermine de façon unique\footnote{Le problème $\rho=0$ ne se pose pas; pourquoi ?} un angle $\theta\in\mathopen[ 0 , \pi \mathclose]$. Dès que $\rho$ et $\theta$ sont connus, nous pouvons poser $r=\rho\sin\theta$ et alors nous nous trouvons avec les équations
\begin{subequations}
	\begin{numcases}{}
		x=r\cos(\varphi)\\
		y=r\sin(\varphi),
	\end{numcases}
\end{subequations}
qui sont similaires à celles déjà étudiées dans le cas des coordonnées polaires.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Récapitulatif des changements de variables}
%---------------------------------------------------------------------------------------------------------------------------

En pratique, nous retiendrons les formules suivantes:
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Coordonnées polaires}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{subequations}
    \begin{numcases}{}
        x=r\cos\theta\\
        y=r\sin\theta
    \end{numcases}
\end{subequations}
avec \( r\in\mathopen] 0 , \infty \mathclose[\) et \( \theta\in\mathopen[ 0 , 2\pi [\). Le jacobien vaut \( r\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Coordonnées cylindriques}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{subequations}
    \begin{numcases}{}
        x=r\cos\theta\\
        y=r\sin\theta\\
        z=z
    \end{numcases}
\end{subequations}
avec \( r\in\mathopen] 0 , \infty \mathclose[\), \( \theta\in\mathopen[ 0 , 2\pi [\) et \( z\in\eR\). Le jacobien vaut \( r\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Coordonnées sphériques}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{subequations}
    \begin{numcases}{}
        x=\rho\cos\theta\sin\phi\\
        y=\rho\sin\theta\sin\phi\\
        z=\rho\cos\phi
    \end{numcases}
\end{subequations}
avec \( \rho\in\mathopen] 0 , \infty \mathclose[\), \( \theta\in\mathopen[ 0 , 2\pi [\) et \( \phi\in\mathopen[ 0 , \pi [\). Le jacobien vaut \( -\rho^2\sin(\phi)\). 

N'oubliez pas que lorsqu'on effectue un changement de variables dans une intégrale, la \emph{valeur absolue} du jacobien apparaît.

Cependant notre convention de coordonnées sphériques fait venir \( \sin(\phi)\) avec \( \phi\in\mathopen[ 0 , \pi [\); vu que le signe de \( \sin(\phi)\) y est toujours positif, cette histoire de valeur absolue est sans grandes conséquent. Ce n'est pas le cas de toutes les conventions possibles.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{une limite}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}
    Nous avons la limite
    \begin{equation}\label{sinsurx}
      \lim_{x\to 0} \frac{\sin(x)}{x} = 1.
    \end{equation}
\end{proposition}

\begin{proof}
    On commence par observer que la fonction $g(x)=\frac{\sin(x)}{x}$ est un rapport entre deux fonction impaires et est donc une fonction paire. Nous pouvons alors nous réduire à considérer le cas où $x$ est positif. La première étape de cette preuve nous dit que $g(x)\leq 1$ pour tout $x\in\eR^{+,*}$. 

    Nous voulons étudier le comportement de $g$ dans un voisinage de $0$. Nous pouvons alors supposer que $x$ soit inférieur à $\pi/2$. Soit $D = (1, \tan (x))$. On voit très bien dans le dessin que l'aire du triangle de sommets $O$, $D$ et $C$ est supérieure à l'aire du secteur circulaire de sommets $O$, $A$ et $C$. Ces deux aires peuvent \^etre calculées très facilement et nous obtenons
    \begin{equation*}
      \frac{\sin(x)}{2\cos(x)} \geq \frac{x}{2}.
    \end{equation*}
    À partir de cette dernière inégalité nous pouvons écrire 
    \begin{equation*}
      1\geq \frac{\sin(x)}{x}\geq \cos(x).
    \end{equation*}
    En prenant la limite lorsque $x$ tend vers $0$ dans les trois membres de l'inégalité la règle de l'étau nous permet d'obtenir la limite remarquable  \eqref{sinsurx}. 
\end{proof}


