% This is part of Mes notes de mathématique
% Copyright (c) 2008-2017
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Valeur propre et vecteur propre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}      \label{DefooMMKZooVcskCc}
    Soit un \( \eK\)-espace vectoriel \( E\) et un endomorphisme \( A\colon V\to V\). Un \defe{vecteur propre}{vecteur!propre} de \( A\) est un vecteur \( v \neq 0\) tel que \( Av=\lambda v\) pour un certain \( \lambda\in \eK\). Dans ce cas, \( \lambda\) est la \defe{valeur propre}{valeur!propre} de \( v\).

    L'\defe{espace propre}{espace!propre} de \( A\) pour la valeur \( \lambda\)\footnote{Nous laissons au lecteur le soin de vérifier que c'est bien un sous-espace vectoriel de \( E\).} est l'ensemble des vecteurs propres de \( A\) pour la valeur propre \( \lambda\) et zéro.
\end{definition}
L'ensemble de valeurs propres de l'endomorphisme \( u\) est son \defe{spectre}{spectre!d'un endomorphisme} et est noté \( \Spec(u)\).

\begin{remark}
    Le nombre zéro peut être une valeur propre; c'est le vecteur zéro qui ne peut pas être vecteur propre. La matrice nulle est une matrice diagonalisable.
\end{remark}

\begin{lemma}       \label{LemjcztYH}
    Soit \( u\) un endomorphisme et \( E_{\lambda}(u)\)\nomenclature[A]{\( E_{\lambda}(u)\)}{Espace propre de \( u\)} ses espaces propres. La somme des \( V_{\lambda}\) est directe.
\end{lemma}

\begin{proof}
    Soit \( v_i\in V_{\lambda_i}\) un choix de vecteurs propres de \( u\). Si la somme n'est pas directe, nous pouvons considérer une combinaison linéaire des \( v_i\) qui soit nulle :
    \begin{equation}
        v_1+\cdots+v_p=0.
    \end{equation}
    Appliquons \( (A-\lambda_1\mtu)\) à cette égalité :
    \begin{equation}
        (\lambda_2-\lambda_1)v_1+\cdots+(\lambda_p-\lambda_1)v_p=0.
    \end{equation}
    En appliquant encore successivement les opérateurs \( (A-\lambda_i\mtu)\) nous réduisons le nombre de termes jusqu'à obtenir \( v_p=0\).
\end{proof}

\begin{example} \label{ExICOJcFp}
    Sur \( \eR^2\), nous considérons la matrice \( A=\begin{pmatrix}
        1    &   0    \\ 
        1    &   1    
    \end{pmatrix}\) qui a pour polynôme caractéristique le polynôme \( \chi_A=(X-1)^2\). Le nombre \( \lambda=1\) est une racine double de ce polynôme, et pourtant il n'y a qu'une seule dimension d'espace propre :
    \begin{equation}
        \begin{pmatrix}
            1    &   0    \\ 
            1    &   1    
        \end{pmatrix}\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}=\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}
    \end{equation}
    entraine \( x=0\).

    Ici la multiplicité algébrique est différente de la multiplicité géométrique.
\end{example}

\begin{proposition}[\cite{RombaldiO}]   \label{PropTVKbxU}
    Soit \( E\), un espace vectoriel sur un corps infini et \( (F_k)_{k=1,\ldots, r}\), des sous-espaces vectoriels propres\footnote{Définition \ref{DefooMMKZooVcskCc}.} de \( E\) tels que \( \bigcup_{i=1}^rF_i=E\). Alors \( E=F_k\) pour un certain \( k\).

    Autrement dit, l'union finie de sous-espaces propres ne peut être égal à l'espace complet.
\end{proposition}

La proposition suivante donne une utilisation amusante de la notion de polynôme caractéristique
\begin{proposition}[\cite{ooNGUJooPphdsT}]
    Soit un espace vectoriel \( V\) de dimension finie pour lequel il existe un endomorphisme \( f\colon V\to V\) tel que \( (f\circ f)(v)=-v\) pour tout \( v\in V\). Alors la dimension de \( V\) est paire.
\end{proposition}

\begin{proof}
    Cherchons les valeurs propres de \( f\) en résolvant l'équation \( f(v)=\lambda v\). Nous appliquons \( f\) à cette égalité :
    \begin{equation}
        -v=\lambda f(v)=\lambda^2v.
    \end{equation}
    Donc \( \lambda\) ne peut pas être réel. Nous avons montré que \( f\) n'a pas de valeurs propres réelles. Or le polynôme caractéristique de \( f\) est de degré égal à la dimension. Si la dimension est impaire, le polynôme caractéristique est de degré impair, et possède donc une racine réelle. Autrement dit, l'absence de racines réelles au polynôme caractéristique indique une dimension paire.
\end{proof}

Une autre preuve possible est d'utiliser le déterminant : si la dimension de \( V\) est \( n\) nous avons :
\begin{equation}
    \det(f^2)=\det(-\id)=(-1)^n.
\end{equation}
Donc \( (-1)^n\) est positif, ce qui montre que \( n\) est pair.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Polynôme d'endomorphismes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit \( A\) un anneau commutatif et \( \eK\), un corps commutatif. L'injection canonique \( A\to A[X]\) se prolonge en une injection
\begin{equation}
   \eM(A)\to\eM\big( A[X] \big).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes d'endomorphismes}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( u\in\End(E)\) où \( E\) est un \( \eK\)-espace vectoriel. Nous considérons l'application
\begin{equation}    \label{EqOVKooeMJuv}
    \begin{aligned}
        \varphi_u\colon \eK[X]&\to \End(E) \\
        P&\mapsto P(u). 
    \end{aligned}
\end{equation}
L'image de \( \varphi_u\) est un sous-espace vectoriel. En effet si \( A=\varphi_u(P)\) et \( B=\varphi_u(Q)\), alors \( A+B=\varphi_u(P+Q)\) et \( \lambda A=(\lambda P)(u)\). En particulier c'est un espace fermé.

Soit \( u\) un endomorphisme d'un \( \eK\)-espace vectoriel \( E\) et \( P\), un polynôme. Nous disons que \( P\) est un polynôme \defe{annulateur}{polynôme!annulateur} de \( u\) si \( P(u)=0\) en tant que endomorphisme de \( E\).

\begin{lemma}       \label{LemQWvhYb}
    Si \( P\) et \( Q\) sont des polynômes dans \( \eK[X]\) et si \( u\) est un endomorphisme d'un \( \eK\)-espace vectoriel \( E\), nous avons
    \begin{equation}
        (PQ)(u)=P(u)\circ Q(u).
    \end{equation}
\end{lemma}

\begin{proof}
    Si \( P=\sum_i a_iX^i\) et \( Q=\sum_j b_jX^j\), alors le coefficient de \( X^k\) dans \( PQ\) est
    \begin{equation}        \label{EqCoefGPyVcv}
        \sum_la_lb_{k-l}.
    \end{equation}
    Par conséquent \( (PQ)(u)\) contient \( \sum_la_lb_{k-l}u^k\). Par ailleurs \( P(u)\circ Q(u)\) est donné par
    \begin{equation}
        \sum_ia_iu^i\left( \sum_jb_ju^j \right)(x)=\sum_{ij}a_ib_ju^{i+j}(x).
    \end{equation}
    Le coefficient du terme en \( u^k\) est bien le même que celui donné par \eqref{EqCoefGPyVcv}.
\end{proof}

\begin{theorem}[Décomposition des noyaux ou lemme des noyaux]       \label{ThoDecompNoyayzzMWod}
    Soit \( u\) un endomorphisme du \( \eK\)-espace vectoriel \( E\). Soit \( P\in\eK[X]\) un polynôme tel que \( P(u)=0\). Nous supposons que \( P\) s'écrive comme le produit \( P=P_1\ldots P_n\) de polynômes deux à deux étrangers\footnote{Définition \ref{DefDSFooZVbNAX}.}. Alors
    \begin{equation}
        E=\ker P_1(u)\oplus\ldots\oplus\ker P_n(u).
    \end{equation}
    De plus les projecteurs associés à cette décomposition sont des polynômes en \( u\).
\end{theorem}
\index{lemme!des noyaux}
Ce résultat est utilisé pour prouver que toute représentation est décomposable en représentations irréductibles, proposition \ref{PropHeyoAN} ainsi que pour le théorème \ref{ThoDigLEQEXR} qui dit que si le polynôme minimal d'un endomorphisme est scindé à racine simple alors il est diagonalisable.

\begin{proof}
    Nous posons 
    \begin{equation}
        Q_i=\prod_{j\neq i}P_i.
    \end{equation}
    Par le lemme \ref{LemuALZHn} ces polynômes sont étrangers entre eux et le théorème de Bézout (théorème \ref{ThoBezoutOuGmLB}) donne l'existence de polynômes \( R_i\) tels que
    \begin{equation}
        R_1Q_1+\cdots+R_nQ_n=1.
    \end{equation}
    Si nous appliquons cette égalité à \( u\) et ensuite à \( x\in E\) nous trouvons
    \begin{equation}        \label{EqqVcpUy}
        \sum_{i=1}^n(R_iQ_i)(u)(x)=x,
    \end{equation}
    et en particulier si nous posons \( E_i=\Image\big(P_iQ_i(u)\big)\) nous avons
    \begin{equation}
        E=\sum_{i=1}^nE_i.
    \end{equation}
    Cette dernière somme n'est éventuellement pas une somme directe. Si \( i\neq j\), alors \( Q_iQ_j\) est multiple de \( P\) et nous avons, en utilisant le lemme \ref{LemQWvhYb}, 
    \begin{equation}
        (R_iQ_i)(u)\circ (R_jQ_j)(u)=\big( R_iQ_iR_jQ_j \big)(u)=S_{ij}(u)\circ P(u)=0
    \end{equation}
    où \( S_{ij}\) est un polynôme. 

    Nous pouvons voir \( E\) comme un \( \eK\)-module et appliquer le théorème \ref{ThoProjModpAlsUR}. Les opérateurs \( R_iQ_i(u)\) ont l'identité comme somme et sont orthogonaux, et nous avons donc la décomposition en somme directe :
    \begin{equation}
        E=\bigoplus_{i=1}^nR_iQ_i(u)E.
    \end{equation}

    Afin de terminer la preuve, nous devons montrer que \( R_iQ_i(u)E=\ker P_i(u)\). D'abord nous avons
    \begin{equation}
        P_iR_iQ_i(u)=(R_iP)(u)=R_i(u)\circ P(u)=0,
    \end{equation}
    par conséquent \( \Image(R_iQ_i(u))\subset \ker P_i(u)\). Pour obtenir l'inclusion inverse, nous reprenons l'équation \eqref{EqqVcpUy} avec \( x\in\ker P_i(u)\). Elle se réduit à
    \begin{equation}
        (R_iQ_i)(u)x=x.
    \end{equation}
    Par conséquent \( x\in\Image\big( R_iQ_i(u) \big)\).
\end{proof}

\begin{corollary}   \label{CorKiSCkC}
    Soit \( E\), un \( \eK\)-espace vectoriel de dimension finie et \( f\), un endomorphisme semi-simple dont la décomposition du polynôme minimal \( \mu_f\) en facteurs irréductibles sur \( \eK[X]\) est \( \mu_f=M_1^{\alpha_1}\cdots M_r^{\alpha_r}\). Si \( F\) est un sous-espace stable par \( f\), alors
    \begin{equation}
        F=\bigoplus_{i=1}^r\ker M_i^{\alpha_i}(f)\cap F
    \end{equation}
\end{corollary}

\begin{proof}
    Nous posons \( E_i=\ker M_i^{\alpha_i}(f)\) et \( F_i=E_i\cap F\). Les polynômes \( M_i^{\alpha_i}\) sont deux à deux étrangers et \( \mu_f(f)=0\), donc le lemme des noyaux (\ref{ThoDecompNoyayzzMWod}) s'applique et
    \begin{equation}
        E=E_1\oplus\ldots\oplus E_r.
    \end{equation}
    Nous pouvons décomposer \( x\in F\) en termes de cette somme :
    \begin{equation}     \label{EqbBbrdi}
        x=x_1+\cdots +x_r
    \end{equation}
    avec \( x_i\in E_i\). Toujours selon le lemme des noyaux, les projections sur les espaces \( E_i\) sont des polynômes en \( f\). Par conséquent \( F\) est stable sous toutes ces projections \( \pr_i\colon E\to E_i\), et en appliquant \( \pr_i\) à \eqref{EqbBbrdi}, \( \pr_i(x)=x_i\). Vu que \( x\in F\), le membre de gauche est encore dans \( F\) et \( x_i\in E_i\cap F\). Nous avons donc
    \begin{equation}
        F\subset\bigoplus_{i=1}^rF_i.
    \end{equation}
    L'inclusion inverse est immédiate parce que \( F_i\subset F\) pour chaque \( i\).
\end{proof}

\begin{lemma}   \label{LemVISooHxMdbr}
    Si \( x\) est un vecteur propre de valeur propre \( \lambda\) pour l'endomorphisme \( u\) et si \( P\) est un polynôme, alors \( x\) est vecteur propre de \( u\) pour la valeur propre \( P(\lambda)\).
\end{lemma}

\begin{proof}
    C'est un simple calcul de \( P(u)x\) en ayant noté \( P(X)=\sum_{k=0}^nc_kX^n\) :
    \begin{equation}
        P(u)x=\sum_{k=0}^nc_ku^k(x)=\sum_{k=0}^nc_k\lambda^ku=P(\lambda)x.
    \end{equation}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Calcul effectif de l'exponentielle d'une matrice}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooGAHVooBRUFub}

Nous reprenons l'exemple de \cite{MneimneReduct}. Soit \( A\) une matrice dont le polynôme minimum s'écrit
\begin{equation}
    P(X)=(X-1)^2(X-2).
\end{equation}
Par le théorème \ref{ThoDecompNoyayzzMWod} de décomposition des noyaux nous avons
\index{théorème!décomposition des noyaux!et exponentielle de matrice}
\begin{equation}
    E=\ker(A-1)^2\oplus\ker(A-2).
\end{equation}
En suivant les notations de ce théorème nous avons \( P_1(X)=(X-1)^2\), \( P_2(X)=X-2\) et
\begin{subequations}
    \begin{align}
        Q_1(X)&=X-2\\
        Q_2(X)&=(X-1)^2.
    \end{align}
\end{subequations}
Les polynômes \( R_i\) dont l'existence est assurée par le théorème de Bézout sont
\begin{equation}
    \begin{aligned}[]
        R_1(X)&=-X\\
        R_2(X)&=1.
    \end{aligned}
\end{equation}
Nous avons
\begin{equation}
    R_1Q_1+R_2Q_2=1.
\end{equation}
Le projecteur \( p_i\) sur \( \ker P_i\) est \( R_iQ_i\) :
\begin{equation}
    \begin{aligned}[]
        p_1&=-A(A-2)=\pr_{\ker(u-1)^2}\\
        p_2&=(A-1)^2=\pr_{\ker(u-2)}.
    \end{aligned}
\end{equation}
Passons maintenant au calcul de l'exponentielle. Nous avons évidemment
\begin{equation}
    e^A=e^Ap_1+e^Ap_2.
\end{equation}
Étant donné que \( p_1\) est le projecteur sur le noyau de \( (A-1)^2\), nous avons
\begin{equation}
    e^Ap_1=ee^{A-1}p_1=ep_1+e(u-1)1=ep_1=-Ae(A-2).
\end{equation}
En effet \( e^{A-1}p_1=\sum_{k=0}^{\infty}(A-1)^k\circ p_1\). De la même façon nous avons
\begin{equation}
    e^Ap_2=e^2e^{A-2}p_2=e^2p_2=e^2(A-1)^2.
\end{equation}
Au final,
\begin{equation}
    e^A=-Ae(A-2)+e^2(A-1)^2.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme minimal et minimal ponctuel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemmaDef}        \label{DefooOHUXooNkPWaB}
    Soit un endomorphisme \( f\colon E\to E\) d'un \( \eK\)-espace vectoriel de dimension finie. Il existe un unique polynôme annulateur normalisé de degré minimum.

    Il est nommé le \defe{polynôme minimal}{polynôme!minimal} de \( f\) et il est noté \( \mu_f\) ou simplement \( \mu\) lorsque la dépendance en \( f\) est claire.
\end{lemmaDef}

\begin{proof}
    Pour l'unicité, soient \( P\) et \( Q\) deux polynômes annulateur de \( f\) de même degré \( N\) et ayant tous deux \( 1\) comme coefficient de \( x^N\). Alors \( P-Q\) est de degré \( N-1\) tout en étant encore annulateur.

    Pour l'existence, les endomorphismes \( \id\), \( f\), \( f^2\), \ldots ne peuvent pas être tous linéairement indépendants parce que la dimension de \( \End(E)\) est finie. Il existe donc un nombre \( N\) et des coefficients \( a_k\) tels que \( \sum_{k=0}^Na_kf^k=0\). Le polynôme \( P(X)=\sum_{k=0}^Na_kX^k\) est donc annulateur de \( f\).

    Une autre façon de le dire est que l'application linéaire \( \varphi\colon \eK[X]\to \End(E)\) donnée par \( \varphi(P)=P(f)\) est un endomorphisme d'un espace vectoriel de dimension infinie vers un espace vectoriel de dimension finie. Il ne peut donc pas être injectif et possède donc un noyau non réduit à zéro.
\end{proof}

\begin{remark}
    La preuve donnée ci-dessus montre que \( \deg(\mu)\leq \dim(E)^2\). Comme conséquence du théorème de Caley-Hamilton \ref{ThoCalYWLbJQ} nous verrons qu'en réalité le degré du polynôme minimal est majoré par la dimension de l'espace.
\end{remark}

\begin{example}[Pas en dimension infinie]
    L'endomorphisme de dérivation
\end{example}


Dans la suite, l'endomorphisme \( f\) du \( \eK\)-espace vectoriel \( E\) de dimension \( n\) est fixé. Pour \( x\in E\) nous notons
\begin{equation}            \label{EqooOAYDooEpZELo}
    E_x=\{ P(f)x\tq P\in \eK[X] \}.
\end{equation}
Nous considérons le morphisme d'algèbres
\begin{equation}
    \begin{aligned}
        \varphi\colon \eK[X]&\to \End(E) \\
        P&\mapsto P(f) 
    \end{aligned}
\end{equation}
et si \( x\in E\) est donné nous considérons le morphisme de \( \eK\)-espaces vectoriels
\begin{equation}
    \begin{aligned}
        \varphi_x\colon \eK[X]&\to E \\
        P&\mapsto P(f)x. 
    \end{aligned}
\end{equation}
Les noyaux de ces applications sont des idéaux, entre autres par le lemme \ref{LemQWvhYb}. Ils ont donc un unique générateur unitaire (chacun) par le théorème \ref{ThoCCHkoU}. En termes de vocabulaire, l'ensemble
\begin{equation}
    \ker(\phi)=\{  Q\in\eK[X]\tq Q(f)=0  \}
\end{equation}
est l'\defe{idéal annulateur}{polynôme!annulateur} de \( f\) et un polynôme \( Q\) tel que \( Q(f)=0\) est une polynôme annulateur de \( f\).

\begin{definition}      \label{DEFooUICRooBGYhqQ}
    Le générateur unitaire de \( \ker(\varphi_x)\) est le \defe{polynôme minimal ponctuel}{polynôme!minimal!ponctuel} de \( f\) en \( x\). Il sera noté \( \mu_{f,x}\) ou \( \mu_x\) lorsque la dépendance en \( f\) est claire dans le contexte.
\end{definition}
Nous notons \( \mu\) le générateur unitaire du noyau de \( \varphi\) et \( \mu_x\) celui de \( \varphi_x\). Vu que \( \mu\in\ker(\varphi_x)\) pour tout \( x\) nous avons \( \mu_x\divides \mu\) pour tout \( x\).

\begin{example}[Pas en dimension infinie]       \label{ExooDTUJooIMqSKn}
    En dimension infinie, il n'y a pas toujours de polynôme annulateur. Si \( E\) est un espace vectoriel de dimension infine ayant une base dénombrable \( \{ e_i \}_{i\in \eN}\) alors l'opérateur donné par \( f(e_i)=e_{i+1}\) n'a pas de polynôme annulateur. Même pas ponctuel en quel que point que ce soir.

    De même l'opérateur donné par \( g(e_1)=0\) et \( g(e_i)=e_{i-1}\) si \( i\neq 1\) n'a pas de polynôme annulateur, mais il a un polynôme annulateur ponctuel évident en \( x=e_1\). L'exemple \ref{ExooLRHCooMYLQTU} donnera un habillage à peine subtil à cet exemple.
\end{example}

\begin{proposition}     \label{PropAnnncEcCxj}
    Si \( P\) est un polynôme tel que \( P(f)=0\), alors le polynôme minimal \( \mu_f\) divise \( P\). Autrement dit, le polynôme minimal engendre l'idéal des polynômes annulateurs.
\end{proposition}

\begin{proof}
    L'ensemble \( \ker(\varphi)=\{ Q\in \eK[X]\tq Q(u)=0 \} \) est un idéal par le lemme \ref{LemQWvhYb}. Le polynôme minimal de \( u\) est un élément de degré plus bas dans \( I\) et par conséquent \( I=(\mu_u)\) par le théorème \ref{ThoCCHkoU}. Nous concluons que \( \mu_u\) divise tous les éléments de \( I\).
\end{proof}

La proposition suivante permet de caractériser le polynôme minimal.
\begin{proposition}[\cite{ooEPEFooQiPESf}]      \label{PROPooVUJPooMzxzjE}
    Soit une application linéaire \( f\) sur un \( \eK\)-espace vectoriel. Il existe un unique polynôme unitaire\quext{À mon avis, «unitaire» manque dans \cite{ooEPEFooQiPESf}.} \( P\in \eK[X]\) tel que
    \begin{enumerate}
        \item
            \( P(f)=0\);
        \item
            l'application
            \begin{equation}        \label{EQooIBMDooVTaEhf}
                \begin{aligned}
                    \varphi\colon \frac{ \eK[X] }{ (P) }&\to \End(E) \\
                    \bar Q&\mapsto Q(f) 
                \end{aligned}
            \end{equation}
            est injective.
    \end{enumerate}
\end{proposition}

\begin{proof}
    En ce qui concerne l'existence, il existe le polynôme minimal de \( f\) qui satisfait les conditions. Pour l'unicité nous travaillons maintenant.

    Supposons que l'application \eqref{EQooIBMDooVTaEhf} soit injective. Alors pour tout \( Q\in \eK[X]\) tel que \( Q(f)=0\) nous avons \( \bar Q=0\), c'est à dire \( Q=PR\) pour un certain \( R\in \eK[X]\). Autrement dit : \( P\) est un générateur unitaire de l'idéal annulateur de \( f\). Le théorème \ref{ThoCCHkoU}\ref{ITEMooASHKooZqkiCH} nous dit alors que \( P=\mu\) parce que \( \mu\) est également générateur unitaire.
\end{proof}

\begin{lemma}[\cite{ooRJDSooXpVtMD}]\label{LemSYsJJj}
    Soit \( f\colon E\to E\) un endomorphisme de l'espace vectoriel \( E\). Il existe un élément \( x\in E\) tel que \( \mu_{f,x}=\mu_f\).
\end{lemma}

\begin{proof}
    Soit une décomposition en irréductibles du polynôme minimal \( \mu=P_1^{\alpha_1}\ldots P_r^{\alpha_r}\). Nous notons \( E_i=\ker\big( P_i^{\alpha_i}(f) \big)\). Les polynômes \( P_i\) sont étrangers deux à deux (un diviseur commun aurait a fortiori été un diviseur et aurait contredit l'irréductibilité). Le lemme des noyaux \ref{ThoDecompNoyayzzMWod} nous donne la somme directe
    \begin{equation}
        E=\bigoplus_{i=1}^r\ker\big( P_i^{\alpha_i}(f) \big).
    \end{equation}
    Si \( x_i\in E_i\) alors \( \mu_{x_i}\) est une puissance de \( P_i\). En effet \( \mu_{x_i}\divides \mu\) et est donc un produit des puissances des \( P_j\). Or si \( (QP_j)(f)x_i=0\) alors \( (P_jQ)(f)x_i=0\), ce qui donne \( Q(f)x_i\in E_j\cap E_i=\{ 0 \}\). Donc \( \mu_{x_i}\) n'est pas de la forme \( QP_j\) pour \( j\neq i\). Nous en déduisons que \( \mu_{x_i}\) est une puissance de \( P_i\) dès que \( x_i\in E_i\). Nous choisissons \( x_i\in E_i\) tel que \( \mu_{x_i}=P_i^{\alpha_i}\).

    Nous posons enfin \( a=x_1+\cdots +x_r\); par définition du polynôme annulateur \( \mu_a\), nous avons
    \begin{equation}        \label{EqooVIGGooSfuvwB}
        0=\mu_a(f)a=\mu_a(f)x_1+\cdots +\mu_a(f)x_r.
    \end{equation}
    Mais \( m_a(f)x_j\in E_i\), et la somme des \( E_j\) est directe, donc l'annulation de la somme \eqref{EqooVIGGooSfuvwB} implique l'annulation de chacun des termes : \( \mu_a(f)x_i=0\) pour tout \( i\). Cela prouve que \( \mu_{x_i}\divides \mu_a\). Mais comme les \( \mu_{x_i}\) sont premiers deux à deux (parce que ce sont les \( P_i^{\alpha_i}\)), nous avons que le produit divise encore \( \mu_a\) :
    \begin{equation}
        \prod_{i=1}^r\mu_{x_i}\divides \mu_a,
    \end{equation}
    c'est à dire \( \mu\divides \mu_a\). Comme nous avons aussi \( \mu_a\divides \mu\), nous déduisons \( \mu_a=\mu\).
\end{proof}

\begin{definition}[Matrices, endomorphismes et vecteurs cycliques]      \label{DEFooFEIFooNSGhQE}
    Une matrice est \defe{cyclique}{cyclique!matrice}\index{matrice!cyclique} si elle est semblable à une matrice compagnon. Un endomorphisme \( f\colon E\to E\) est \defe{cyclique}{cyclique!endomorphisme}\index{endomorphisme!cyclique} s'il existe un vecteur \( x\in E\) tel que \( \{ f^k(x) \}_{k=0,\ldots, n-1} \) est une base de \( E\). Un vecteur ayant cette propriété est un \defe{vecteur cyclique}{vecteur!cyclique} pour \( f\).
\end{definition}

\begin{lemma}   \label{LemAGZNNa}
    Soit \( E\) un espace vectoriel de dimension finie et un endomorphisme cyclique\footnote{Voir la définition \ref{DEFooFEIFooNSGhQE}.} \( f\) de \( E\). Soit un vecteur cyclique \( v\) de \( f\), alors le polynôme minimal de \( f\) est égal au polynôme minimal de \( f\) au point \( v\) : \( \mu_{f}=\mu_{f,v}\).
\end{lemma}

\begin{proof}
    Montrons que \( \mu_{f,v}\) est un polynôme annulateur de \( f\), ce qui prouvera que \( \mu_f\) divise \( \mu_{f,v}\) par la proposition \ref{PropAnnncEcCxj}. Étant donné que \( v\) est cyclique, tout élément de \( E\) s'écrit sous la forme \( x=Q(f)v\). Prenons un polynôme \( P\) annulateur de \( f\) en \( v\) : \( P(f)v=0\). Nous montrons que \( P\) est alors un polynôme annulateur de \( f\). En effet, nous avons
    \begin{equation}
        P(f)x=\big( P(f)\circ Q(f) \big)v=\big( Q(f)\circ P(f) \big)v=0
    \end{equation}
    où nous avons utilisé le lemme \ref{LemQWvhYb}.
\end{proof}

\begin{lemma}[\cite{ooRJDSooXpVtMD}]
    Soit \( a\in E\) tel que \( \mu_a=\mu\). Alors \( E_a\) est un sous-espace stable pour \( f\) pour lequel il existe un supplémentaire stable.
\end{lemma}

\begin{proof}
    Soit \( l=\deg(\mu)=\deg(\mu_a)\). L'espace \( E_a\) étant engendré par les \( f^k(a)\) nous savons que \( e_1=a\), \( e_2=f(a)\),\ldots, \( e_l=k^{l-1}(a)\) forment une base de \( E_a\). Nous pouvons la compléter en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Et nous posons\footnote{ici, comme presque partout, \( e^*_{l}\) est le dual de \( e_l\), c'est à dire l'application linéaire sur \( E\) donnée par \( e^*_l(e_i)=\delta_{li}\). }
    \begin{subequations}
        \begin{align}
            G&=\{ x\in E\tq e^*_l\big( f^k(x) \big)=0\,\forall k\geq 0 \}\\
            &=\bigcap_{k\geq 0}\ker\{ e^*_l\circ f^k \}\\
            &=\bigcap_{k=0}^{l-1}\ker(  e^*_l\circ f^k ).
        \end{align}
    \end{subequations}
    La dernière égalité est due au fait que \( l\) soit le degré de \( \mu\). Du coup \( f^l\) est une combinaison linéaire des \( f^i\) avec \( i\leq l-1\).

    Nous avons \( f(G)\subset G\) et de plus \( E_a\cap G=\{ 0 \}\) parce qu'un élément de \( E_a\) est une combinaison linéaire d'éléments de la forme \( f^j(a)\) (\( j\leq l\)). Après application de \( f^{l-j}\), ces éléments obtiennent une composante \( f^l(a)=e_l\). De plus \( G\) est un sous-espace vectoriel du fait que \( e^*_l\circ f^i\) est une application linéaire. 
    
    Montrons enfin que \( \dim(G)=n-l\). Pour cela nous remarquons que \( G\) est une intersection d'hyperplans, et nous montrons que les équations définissant ces hyperplans sont linéairement indépendantes. Soit donc 
    \begin{equation}        \label{EqooOHESooRtBUfc}
        \sum_{j=0}^{l-1}\lambda_j\big( e^*_l\circ f^j \big)=0
    \end{equation}
    et montrons que \( \lambda_j=0\) pour tout $j$ est l'unique solution. Soit \( x\in E\) et appliquons l'opération \eqref{EqooOHESooRtBUfc} au vecteur \( f^i(x)\); le résultat est zéro :
    \begin{equation}
        0=\sum_{j=0}^{l-1}\lambda_j(e^*_l\circ f^i\circ f^j)=(e^*_l\circ f^i)P(u)
    \end{equation}
    où nous avons posé \( P(X)=\sum_{j=0}^{l-1}\lambda_jX^j\). Appliquons cela à \( a\) : pour tout \( i\) nous avons
    \begin{equation}
        (e^*_l\circ f^i)\big( P(f)a \big)=0.
    \end{equation}
    Mais par définition de \( E_a\), l'élément \(P(f)a \) est dans \( E_a\). Nous en déduisons que 
    \begin{equation}
        P(f)a\in G\cap E_a=\{ 0 \},
    \end{equation}
    c'est à dire que \( P\) est un polynôme annulateur de \( a\). Mais \( P\) est de degré \( l-1\) alors que le polynôme minimal de \( a\) est de degré \( l\). Par conséquent \( P=0\) et \( \lambda_j=0\) pour tout \( j\).
\end{proof}

\begin{definition}  \label{DEFooBOHVooSOopJN}
    Un endomorphisme d'un espace vectoriel est \defe{semi-simple}{semi-simple!endomorphisme} si tout sous-espace stable par \( u\) possède un supplémentaire stable.
\end{definition}

\begin{lemma}   \label{LemrFINYT}
    Si le polynôme minimal d'un endomorphisme est irréductible, alors il est semi-simple\footnote{Définition \ref{DEFooBOHVooSOopJN}.}.
\end{lemma}

\begin{proof}
    Soit \( f\), un endomorphisme dont le polynôme minimal est irréductible et \( F\), un sous-espace stable par \( f\). Nous devons en trouver un supplémentaire stable. Si \( F=E\), il n'y a pas de problèmes. Sinon nous considérons \( u_1\in E\setminus F\) et
    \begin{equation}
        E_{u_1}=\{ P(f)u_1\tq P\in \eK[X] \},
    \end{equation}
    qui est un espace stable par \( f\). 

    Montrons que \( E_{u_1}\cap F=\{ 0 \}\). Pour cela nous regardons l'idéal
    \begin{equation}
        I_{u_1}=\{ P\in \eK[X]\tq P(f)u_1=0 \}.
    \end{equation}
    Cela est un idéal non réduit à \( \{ 0 \}\) parce que le polynôme minimal de \( f\) par exemple est dans \( I_{u_1}\). Soit \( P_{u_1}\) un générateur unitaire de \( I_{u_1}\). Étant donné que \( \mu_f\in I_{u_1}\), nous avons que \( P_{u_1}\) divise \( \mu_f\) et donc \( P_{u_1}=\mu_f\) parce que \( \mu_f\) est irréductible par hypothèse.

    Soit \( y\in E_{u_1}\cap F\). Par définition il existe \( P\in\eK[X]\) tel que \( y=P(f)u_1\) et si \( y\neq 0\), ce la signifie que \( P\notin I_{u_1}\), c'est à dire que \( P_{u_1} \) ne divise pas \( P\). Étant donné que \( P_{u_1}\) est irréductible cela implique que \( P_{u_1}\) et \( P\) sont premiers entre eux (ils n'ont pas d'autre \( \pgcd\) que \( 1\)).

    Nous utilisons maintenant Bézout (théorème \ref{ThoBezoutOuGmLB}) qui nous donne \( A,B\in \eK[X]\) tels que 
    \begin{equation}
        AP+BP_{u_1}=1.
    \end{equation}
    Nous appliquons cette égalité à \( f\) et puis à \( u_1\):
    \begin{equation}
        u_1=A(f)\circ \underbrace{P(f)u_1}_{=y}+B(f)\circ \underbrace{P_{u_1}(u_1)}_{=0}=A(f)y.
    \end{equation}
    Mais \( y\in F\), donc \( A(f)y\in F\). Nous aurions donc \( u_1\in F\), ce qui est impossible par choix. Nous avons maintenant que l'espace \( E_{u_1}\oplus F\) est stable sous \( f\). Si cet espace est \( E\) alors nous arrêtons. Sinon nous reprenons le raisonnement avec \( E_{u_1}\oplus F\) en guise de \( F\) et en prenant \( u_2\in E\setminus(E_{u_1}\oplus F)\). Étant donné que \( E\) est de dimension finie, ce procédé s'arrête à un certain moment et nous aurons
    \begin{equation}
        E=F\oplus E_{u_1}\oplus\ldots\oplus E_{u_k}
    \end{equation}
    où chacun des \( E_{u_i}\) sont stables.
\end{proof}

\begin{theorem} \label{ThoFgsxCE}
    Un endomorphisme est semi-simple si et seulement si son polynôme minimal est produit de polynômes irréductibles distincts deux à deux.
\end{theorem}
\index{anneau!principal}

\begin{proof}

    Supposons que \( f\) soit semi-simple et que son polynôme minimal soit donné par \( \mu_f=M_1^{\alpha_1}\ldots M_r^{\alpha_r}\) où les \( M_i\) sont des polynômes irréductibles deux à deux distincts. Nous devons montrer que \( \alpha_i=1\) pour tout \( i\). Soit \( i\) tel que \( \alpha_i\geq 1\) et \( N\in \eK[X]\) tel que \( \mu_f=M^2N\) où l'on a noté \( M=M_i\). Nous étudions l'espace
    \begin{equation}
        F=\ker M(f)
    \end{equation}
    qui est stable par \( f\), et qui possède donc un supplémentaire \( S\) également stable par \( f\). Nous allons montrer que \( MN\) est un polynôme annulateur de \( f\).

    D'abord nous prenons \( x\in S\). Étant donné que \( F\) est le noyau de \( M(f)\),
    \begin{equation}
        M(f)\big( MN(f)x \big)=\mu_f(f)x=0,
    \end{equation}
    ce qui signifie que \( MN(f)x\in F\). Mais vu que \( S\) est stable par \( f\) nous avons aussi que \( MN(f)x\in S\). Finalement \( MN(f)x\in F\cap S=\{ 0 \}\). Autrement dit, \( MN(f)\) s'annule sur \( S\).

    Prenons maintenant \( y\in F\). Nous avons
    \begin{equation}
        MN(f)=N(f)\big( M(f)y \big)=0
    \end{equation}
    parce que \( y\in F=\ker M(f)\).

    Nous avons prouvé que \( MN(f)\) s'annule partout et donc que \( MN(f)\) est un polynôme annulateur de \( f\), ce qui contredit la minimalité de \( \mu_f=M^2N\).

    Nous passons au sens inverse. Soit \( m_f=M_1\ldots M_r\) une décomposition du polynôme minimal de l'endomorphisme \( f\) en irréductibles distincts deux à deux. Soit \( F\) un sous-espace vectoriel stable par \( f\). Nous notons
    \begin{equation}
        E_i=\ker(M_i(f))
    \end{equation}
    et \( f_i=f|_{E_i}\). Par le lemme \ref{CorKiSCkC} nous avons
    \begin{equation}
        F=\bigoplus_{i=1}^r(F\cap E_i).
    \end{equation}
    Les espaces \( E_i\) sont stables par \( f\) et étant donné que \( M_i\) est irréductible, il est le polynôme minimal de \( f_i\). En effet, \( M_i\) est annulateur de \( f_i\), ce qui montre que le minimal de \( f_i\) divise \( M_i\). Mais \( M_i\) étant irréductible, \( M_i\) est le polynôme minimal. Étant donné que \( \mu_{f_i}=M_i\), l'endomorphisme \( f_i\) est semi-simple par le lemme \ref{LemrFINYT}.

    L'espace \( F\cap E_i\) étant stable par l'endomorphisme semi-simple \( f_i\), il possède un supplémentaire stable que nous notons \( S_i\)~:
    \begin{equation}
        E_i=S_i\oplus(F\cap E_i).
    \end{equation}
    Étant donné que sur chaque \( S_i\) nous avons \( f|_{S_i}=f_i\), l'espace \( S=S_1\oplus\ldots\oplus S_r\) est stable par \( f\). Du coup nous avons
    \begin{subequations}
        \begin{align}
            E&=E_1\oplus\ldots\oplus E_r\\
            &=\big( S_1\oplus(F\cap E_1) \big)\oplus\ldots\oplus\big( S_r\oplus(F\cap E_r) \big)\\
            &=\big( \bigoplus_{i=1}^rS_i \big)\oplus\big( \bigoplus_{i=1}^rF\cap E_i \big)\\
            &=S\oplus F,
        \end{align}
    \end{subequations}
    ce qui montre que \( F\) a bien un supplémentaire stable par \( f\) et donc que \( f\) est semi-simple.
\end{proof}

\begin{example}[L'espace engendré par \( \mtu\), \( A\), \( A^2\),\ldots]
    Soit \( A\) une matrice, et 
    \begin{equation}
        V=\Span\{A^k\tq k\in \eN \}.
    \end{equation}
    Nous montrons que \( \dim(V)\) est le degré du polynôme minimal de \( A\).

    D'abord l'idéal annulateur de \( A\) est engendré par le polynôme minimal\footnote{Proposition \ref{PropAnnncEcCxj}.} que nous notons
        $\mu=\sum_{k=0}^pa_kX^k$.
    La partie \( \{ \mtu,\ldots, A^{p-1} \}\) est libre parce qu'une combinaison linéaire nulle de cela serait un polynôme annulateur en \( A\) de degré plus petit que \( p\). Donc \( \dim(V)\geq p\).

    La partie \( \{ \mtu,A,\ldots, A^p \}\) est liée à cause du polynôme minimal. Isoler \( A^p\) dans \( \mu(A)=0\) donne un polynôme \( f\) de degré \( p-1\) tel que \( A^p=f(A)\).

    Nous allons montrer à présent que la famille \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice (alors \( \dim(V)\leq p\)). Soit un entier \( q\geq p\)et de division euclidienne\footnote{Théorème \ref{ThoDivisEuclide}.} \( np+r=q\) avec \( r<p\). Nous avons \( A^q=A^{np}A^r\). D'une part
    \begin{equation}
        A^{np}=(A^p)^n=f(A)^n
    \end{equation}
    est de degré \( n(p-1)\). Par conséquent
    \begin{equation}
        A^q=f(A)^nA^r
    \end{equation}
    qui est de degré \( n(p-1)+r=q-n\). Autrement dit il existe un polynôme \( g_1\) de degré \( q-n\) tel que \( A^q=g_1(A)\). Si \( q-n>p-1\) alors nous pouvons recommencer et obtenir un polynôme \( g_2\) de degré strictement inférieur à celui de \( g_1\) tel que \( A^q=g_2(A)\). Au bout du compte, il existe un polynôme \( g\) de degré au maximum \( p-1\) tel que \( A^q=g(A)\). Cela prouve que la partie \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice de \( V\).

    La dimension de \( V\) est donc \( p\), le degré du polynôme minimal.
\end{example}

\begin{proposition}     \label{PropooCFZDooROVlaA}
    Soit \( f\) un endomorphisme d'un espace vectoriel de dimension finie. Nous avons l'isomorphisme d'espace vectoriel
    \begin{equation}
        \eK[f]\simeq\frac{ \eK[X] }{ (\mu_f) }
    \end{equation}
    La dimension en est \( \deg(\mu_f)\).
\end{proposition}

\begin{proof}
    Notons avant de commencer que \( (\mu)\) est l'idéal engendré par \( \mu\). Les classes dont il est question dans le quotient \( \eK[X]/(\mu)\) sont 
    \begin{equation}
        \bar P=\{ P+S\mu \}_{S\in \eK[X]}.
    \end{equation}
    Nous allons montrer que l'application suivante fournit l'isomorphisme : 
    \begin{equation}
        \begin{aligned}
            \psi\colon \frac{ \eK[X] }{ (\mu) }&\to \eK[f] \\
            \bar P&\mapsto P(f). 
        \end{aligned}
    \end{equation}
    \begin{subproof}
        \item[\( \psi\) est bien définie]
            Si \( Q\in \bar P\) alors \( Q=P+S\mu\) pour un certain \( S\in \eK[X]\). Du coup nous avons
            \begin{equation}
                \psi(\bar Q)=P(f)+(S\mu)(f).
            \end{equation}
            Mais \( \mu(f)=0\) donc le deuxième terme est nul. Donc \( \psi(\bar P)\) est bien définit.
        \item[Injectif]
            Si \( \psi(\bar P)=0\) nous avons \( P(f)=0\), ce qui signifie que \( P=S\mu\) pour un polynôme \( S\). Par conséquent \( P\in (\mu)\) et donc \( \bar P=0\).
        \item[Surjectif]
            Soit \( P\in \eK[X]\). L'élément \( P(f) \) de \( \eK[f]\) est dans l'image de \( \psi\) parce que c'est \( \psi(\bar P)\).
    \end{subproof}
    En ce qui concerne la dimension, le corollaire \ref{CorsLGiEN} en parle déjà : une base est donné par les projections de \( 1,X,\ldots, X^{\deg(\mu_a)-1}\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Polynôme caractéristique}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefOWQooXbybYD}
    Soit un anneau commutatif \( A\). Si \( u\in\eM_n(A)\), nous définissons le \defe{polynôme caractéristique de \( u\)}{polynôme!caractéristique}\index{caractéristique!polynôme} :
    \begin{equation}    \label{Eqkxbdfu}
        \chi_u(X)=\det(X\mtu_n-u).
    \end{equation} 
    Nous définissons de même le polynôme caractéristique d'un endomorphisme \( u\colon E\to E\).
\end{definition}

\begin{lemma}       \label{LemooWCZMooZqyaHd}
    Le polynôme caractéristique \( \chi_u\) est unitaire et a pour degré la dimension de l'espace vectoriel \( E\)..
\end{lemma}

\begin{theorem}     \label{ThoNhbrUL}
    Soit \( E\) un \(\eK\)-espace vectoriel de dimension finie \( n\) et un endomorphisme \( u\in\End(E)\). Alors
    \begin{enumerate}
        \item
            Le polynôme caractéristique divise \( (\mu_u)^n\) dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes facteurs irréductibles dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes racines dans \(\eK[X]\).
        \item
            Le polynôme caractéristique est scindé si et seulement si le polynôme minimal est scindé.
    \end{enumerate}
\end{theorem}

\begin{theorem} \label{ThoWDGooQUGSTL}
    Soit \( u\in\End(E)\) et \( \lambda\in\eK\). Les conditions suivantes sont équivalentes
    \begin{enumerate}
        \item\label{ItemeXHXhHi}
            \( \lambda\in\Spec(u)\)
        \item\label{ItemeXHXhHii}
            \( \chi_u(\lambda)=0\)
        \item\label{ItemeXHXhHiii}
            \( \mu_u(\lambda)=0\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    \ref{ItemeXHXhHi} \( \Leftrightarrow\) \ref{ItemeXHXhHii}. Dire que \( \lambda\) est dans le spectre de \( u\) signifie que l'opérateur \( u-\lambda\mtu\) n'est pas inversible, ce qui est équivalent à dire que \( \det(u-\lambda\mtu)\) est nul par la proposition \ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} ou encore que \( \lambda\) est une racine du polynôme caractéristique de \( u\). 

    \ref{ItemeXHXhHii} \( \Leftrightarrow\) \ref{ItemeXHXhHiii}. Cela est une application directe du théorème \ref{ThoNhbrUL} qui précise que le polynôme caractéristique a les mêmes racines dans \(\eK\) que le polynôme minimal.
\end{proof}

\begin{definition}
    Si \( \lambda\in\eK\) est une racine de \( \chi_u\), l'ordre de l'annulation est la \defe{multiplicité algébrique}{multiplicité!valeur propre!algébrique} de la valeur propre \( \lambda\) de \( u\). À ne pas confondre avec la \defe{multiplicité géométrique}{multiplicité!valeur propre!géométrique} qui sera la dimension de l'espace propre.
\end{definition}

\begin{proposition}[\cite{RombaldiO}]\label{PropNrZGhT}
    Soit \( f\), un endomorphisme de \( E\) et \( x\in E\). Alors
    \begin{enumerate}
        \item
            L'espace \( E_{f,x}\) est stable par \( f\).
        \item\label{ItemfzKOCo}
            L'espace \( E_{f,x}\) est de dimension
            \begin{equation}
                p_{f,x}=\dim E_{f,x}=\deg(\mu_{f,x})
            \end{equation}
            où \( \mu_{f,x}\) est le générateur unitaire de \( I_{f,x}\).
        \item   \label{ItemKHNExH}
            Le polynôme caractéristique de \( f|_{E_{f,x}}\) est \( \mu_{f,x}\).
        \item   \label{ItemHMviZw}
            Nous avons
            \begin{equation}
                \chi_{f|_{E_{f,x}}}(f)x=\mu_{f,x}(f)x=0.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le fait que \( E_{f,x}\) soit stable par \( f\) est classique. Le point \ref{ItemHMviZw} est un une application du point \ref{ItemKHNExH}. Les deux gros morceaux sont donc les points \ref{ItemfzKOCo} et \ref{ItemKHNExH}.

    Étant donné que \( \mu_{f,x}\) est de degré minimal dans \( I_{f,x}\), l'ensemble
    \begin{equation}
        B=\{ f^k(x)\tq 0\leq k\leq p_{f,x}-1 \}
    \end{equation}
    est libre. En effet une combinaison nulle des vecteurs de \( B\) donnerait un polynôme en \( f\) de degré inférieur à \( p_{f,x}\) annulant \( x\). Nous écrivons
    \begin{equation}
        \mu_{f,x}(X)=X^{p_{f,x}}-\sum_{i=0}^{p_{f,x}-1}a_iX^k. 
    \end{equation}
    Étant donné que \( \mu_{f,x}(f)x=0\) et que la somme du membre de droite est dans \( \Span(B)\), nous avons \( f^{p_{f,x}}(x)\in\Span(B)\). Nous prouvons par récurrence que \( f^{p_{f,x}+k}(x)\in\Span(B)\). En effet en appliquant \( f^k\) à l'égalité
    \begin{equation}
        0=f^{p_{f,x}}(x)-\sum_{i=0}^{p_{f,x}-1}a_if^i(x)
    \end{equation}
    nous trouvons
    \begin{equation}
        f^{p_{f,x}+k}(x)=\sum_{i=0}^{p_{f,x}-1}a_if^{i+k}(x),
    \end{equation}
    alors que par hypothèse de récurrence le membre de droite est dans \( \Span(B)\). L'ensemble \( B\) est alors générateur de \( E_{f,x}\) et donc une base d'icelui. Nous avons donc bien \( \dim(E_{f,x})=p_{f,x}\).

    Nous montrons maintenant que \( \mu_{f,x}\) est annulateur de \( f\) au point \( x\). Nous savons que
    \begin{equation}
        \mu_{f,x}(f)x=0.
    \end{equation}
    En y appliquant \( f^k\) et en profitant de la commutativité des polynômes sur les endomorphismes (proposition \ref{LemQWvhYb}), nous avons
    \begin{equation}
        0=f^k\big( \mu_{f,x}(f)x \big)=\mu_{f,x}(f)f^k(x),
    \end{equation}
    de telle sorte que \( \mu_{f,x}(f)\) est nul sur \( B\) et donc est nul sur \( E_{f,x}\). Autrement dit,
    \begin{equation}
        \mu_{f,x}\big( f|_{E_{f,x}} \big)=0.
    \end{equation}
    Montrons que \( \mu_{f,x}\) est même minimal pour \( f|_{E_{f,x}}\). Sot \( Q\), un polynôme non nul de degré \( p_{f,x}-1\) annulant \( f|_{E_{f,x}}\). En particulier \( Q(f)x=0\), alors qu'une telle relation signifierait que \( B\) est un système lié, alors que nous avons montré que c'était un système libre. Nous concluons que \( \mu_{f,x}\) est le polynôme minimal de \( f|_{E_{f,x}}\).
\end{proof}

Cette histoire de densité permet de donner une démonstration alternative du théorème de Cayley-Hamilton.
\begin{theorem}[Cayley-Hamlilton]   \label{ThoCalYWLbJQ}
    Le polynôme caractéristique est un polynôme annulateur.
\end{theorem}
\index{théorème!Cayley-Hamilton}

Une démonstration plus simple via la densité des diagonalisables est donnée en théorème \ref{ThoHZTooWDjTYI}.
\begin{proof}
    Nous devons prouver que \( \chi_f(f)x=0\) pour tout \( x\in E\). Pour cela nous nous fixons un \( x\in E\), nous considérons l'espace \( E_{f,x}\) et \( \chi_{f,x}\), le polynôme caractéristique de \( f|_{E_{f,x}}\). Étant donné que \( E_{f,x}\) est stable par \( f\), le polynôme caractéristique de \( f|_{E_{j,x}}\) divise \( \chi_f\), c'est à dire qu'il existe un polynôme \( Q_x\) tel que
    \begin{equation}
        \chi_f=Q_x\chi_{f,x},
    \end{equation}
    et donc aussi
    \begin{equation}
        \chi_f(f)x=Q_x(f)\big( \chi_{f,x}(f)x \big)=0
    \end{equation}
    parce que la proposition \ref{PropNrZGhT} nous indique que \( \chi_{f,x}\) est un polynôme annulateur de \( f|_{E_{f,x}}\).
\end{proof}

\begin{corollary}
    Le degré du polynôme minimal est majoré par la dimension de l'espace.
\end{corollary}

\begin{proof}
    Le polynôme minimal engendre l'idéal des polynôme annulateurs (proposition \ref{PropAnnncEcCxj}), et divise donc le polynôme caractéristique. Or le degré du polynôme caractéristique est la dimension de l'espace par le lemme \ref{LemooWCZMooZqyaHd}.
\end{proof}

\begin{example}[Calcul de l'inverse d'un endomorphisme]
    Le polynôme de Cayley-Hamilton donne un moyen de calculer l'inverse d'un endomorphisme inversible pourvu que l'on sache son polynôme caractéristique. En effet, supposons que
    \begin{equation}
        \chi_f(X)=\sum_{k=0}^na_kX^k.
    \end{equation}
    Nous aurons alors
    \begin{equation}
        0=\chi_f(f)=\sum_{k=0}^na_kf^k.
    \end{equation}
    Nous appliquons \( f^{-1}\) à cette dernière égalité en sachant que \( f^{-1}(0)=0\) :
    \begin{equation}
        0=a_0f^{-1}+\sum_{k=1}^na_kf^{k-1},
    \end{equation}
    et donc
    \begin{equation}
        u^{-1}=-\frac{1}{ \det(f) }\sum_{k=1}^na_kf^{k-1}
    \end{equation}
    où nous avons utilisé le fait que \( a_0=\chi_f(0)=\det(f)\).
\end{example}

\begin{proposition}\label{PropooBYZCooBmYLSc}
    Si \( (X-z)^l\) (\( l\geq 1\)) est la plus grande puissance de \( (X-z)\) dans le polynôme caractéristique d'un endomorphisme \( u\) alors 
    \begin{equation}
        1\leq \dim(E_e)\leq l.
    \end{equation}
    C'est à dire que nous avons au moins un vecteur propre pour chaque racine du polynôme caractéristique.
\end{proposition}

\begin{proof}
    Si $(X-z)$ divise \( \chi_u\) alors en posant \( \chi_u=(X-z)P(X)\) nous avons
    \begin{equation}
        \det(u-X\mtu)=(X-z)P(X),
    \end{equation}
    ce qui, évalué en \( X=z\), donne \( \det(u-z\mtu)=0\). L'annulation du déterminant étant équivalente à l'existence d'un noyau non trivial, nous avons \( v\neq 0\) dans \( E\) tel que \( (u-z\mtu)v=0\). Cela donne \( u(v)=zv\) et donc que \( v\) est vecteur propre de \( u\) pour la valeur propre \( z\). Donc aussi \( \dim(E_z)\geq 1\).

    Si \( \dim(E_z)=k\) alors le théorème de la base incomplète \ref{ThonmnWKs} nous permet d'écrire une base de \( E\) dont les \( k\) premiers vecteurs forment une base de \( E_z\). Dans cette base, la matrice de \( u\) est de la forme
    \begin{equation}
        \begin{pmatrix}
             z   &       &       &   *    \\
                &   \ddots    &       &   \vdots    \\
                &       &   z    &   *    \\ 
                &       &       &   *     
         \end{pmatrix}
    \end{equation}
    où les étoiles représentent des blocs a priori non nuls. En tout cas il est vu sous cette forme que \( (X-z\mtu)^k\) divise \( \chi_u\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Diagonalisation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Ici encore \( \eK\) est un corps commutatif.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices semblables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[matrices semblables] \label{DefCQNFooSDhDpB}
    Sur l'ensemble \( \eM_n(\eK)\) des matrices \( n\times n\) à coefficients dans \(\eK\) nous introduisons la relation d'équivalence \( A\sim B\) si et seulement s'il existe une matrice \( P\in\GL(n,\eK)\) telle que \( B=P^{-1}AP\). Deux matrices équivalentes en ce sens sont dites \defe{semblables}{semblables!matrices}.
\end{definition}

Le polynôme caractéristique est un invariant sous les similitudes. En effet si \( P\) est une matrice inversible,
\begin{subequations}
    \begin{align}
        \chi_{PAP^{-1}}&=\det(PAP^{-1}-\lambda X)\\
        &=\det\big( P^{-1}(PAP^{-1}-\lambda X)P^{-1} \big)\\
        &=\det(A-\lambda X).
    \end{align}
\end{subequations}

La permutation de lignes ou de colonnes ne sont pas de similitudes, comme le montrent les exemples suivants :
\begin{equation}
    \begin{aligned}[]
        A&=\begin{pmatrix}
            1    &   2    \\ 
            3    &   4    
        \end{pmatrix}&
        B&=\begin{pmatrix}
            2    &   1    \\ 
            4    &   3    
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Nous avons \( \chi_A=x^2-5x-2\) tandis que \( \chi_B=x^2-5x+2\) alors que le polynôme caractéristique est un invariant de similitude.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes diagonalisables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefCNJqsmo}
    Une matrice est \defe{diagonalisable}{diagonalisable} si elle est semblable à une matrice diagonale.
\end{definition}

\begin{lemma}
    Une matrice triangulaire supérieure avec des \( 1\) sur la diagonale n'est diagonalisable que si elle est diagonale (c'est à dire si elle est la matrice unité).
\end{lemma}

\begin{proof}
    Si \( A\) est une matrice triangulaire supérieure de taille \( n\) telle que \( A_{ii}=1\), alors \( \det(A-\lambda\mtu)=(1-\lambda)^n\), ce qui signifie que \( \Spec(A)=\{ 1 \}\). Pour la diagonaliser, il faudrait une matrice \( P\in\GL(n,\eK)\) telle que \( \mtu=P^{-1}AP\), ce qui est uniquement possible si \( A=\mtu\).
\end{proof}

\begin{lemma}       \label{LemgnaEOk}
    Soit \( F\) un sous-espace stable par \( u\). Soit une décomposition du polynôme minimal
    \begin{equation}
        \mu_u=P_1^{n_1}\ldots P_r^{n_r}
    \end{equation}
    où les \( P_i\) sont des polynômes irréductibles unitaires distincts. Si nous posons \( E_i=\ker P_i^{n_i}\), alors
    \begin{equation}
        F=(F\cap E_1)\oplus\ldots \oplus(F\cap E_r).
    \end{equation}
\end{lemma}

\begin{theorem}     \label{ThoDigLEQEXR}
    Soit \( E\), un espace vectoriel de dimension \( n\) sur le corps commutatif \( \eK\) et \( u\in\End(E)\). Les propriétés suivantes sont équivalentes.
    \begin{enumerate}
        \item\label{ItemThoDigLEQEXRiv}
            L'endomorphisme \( u\) est diagonalisable.
        \item       \label{ItemThoDigLEQEXRi}
            Il existe un polynôme \( P\in\eK[X]\) non constant, scindé sur \(\eK\) dont toutes les racines sont simples tel que \( P(u)=0\).
        \item\label{ItemThoDigLEQEXRii}
            Le polynôme minimal \( \mu_u\) est scindé sur \(\eK\) et toutes ses racines sont simples\footnote{Le polynôme \emph{caractéristique}, lui, n'a pas spécialement ses racines simples; il peut encore être de la forme
            \begin{equation}
                \chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i},
        \end{equation}
        mais alors \( \dim(E_{\lambda_i})=\alpha_i\). }.
        \item\label{ItemThoDigLEQEXRiii}
            Tout sous-espace de \( E\) possède un supplémentaire stable par \( u\).
        \item       \label{ITEMooZNJFooEiqDYp}
            Dans une base adaptée, la matrice de \( u\) est diagonale et les éléments diagonaux sont ses valeurs propres.
    \end{enumerate}
\end{theorem}
\index{diagonalisable!et polynôme minimum scindé}

\begin{proof}
    Plein d'implications à prouver.
    \begin{subproof}
    \item[\ref{ItemThoDigLEQEXRi} implique \ref{ItemThoDigLEQEXRii}] Étant donné que \( P(u)=0\), il est dans l'idéal des polynôme annulateurs de \( u\), et le polynôme minimal \( \mu_u\) le divise parce que l'idéal des polynôme annulateurs est généré par \( \mu_u\) par le théorème \ref{ThoCCHkoU}.

    \item[\ref{ItemThoDigLEQEXRii} implique \ref{ItemThoDigLEQEXRiv}] Étant donné que le polynôme minimal est scindé à racines simples, il s'écrit sous forme de produits de monômes tous distincts, c'est à dire
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots(X-\lambda_r)
    \end{equation}
    où les \( \lambda_i\) sont des éléments distincts de \( \eK\). Étant donné que \( \mu_u(u)=0\), le théorème de décomposition des noyaux (théorème \ref{ThoDecompNoyayzzMWod}) nous enseigne que
    \begin{equation}
        E=\ker(u-\lambda_1)\oplus\ldots\oplus\ker(u-\lambda_r).
    \end{equation}
    Mais \( \ker(u-\lambda_i)\) est l'espace propre \( E_{\lambda_i}(u)\). Donc \( u\) est diagonalisable.

\item[\ref{ItemThoDigLEQEXRiv} implique \ref{ItemThoDigLEQEXRiii}] Soit \( \{ e_1,\ldots, e_n \}\) une base qui diagonalise \( u\), soit \( F\) un sous-espace de \( E\) un \( \{ f_1,\ldots, f_r \}\) une base de \( F\). Par le théorème \ref{ThoBaseIncompjblieG} (qui généralise le théorème de la base incomplète), nous pouvons compléter la base de \( F\) par des éléments de la base \( \{ e_i \}\). Le complément ainsi construit est invariant par \( u\).

\item[\ref{ItemThoDigLEQEXRiii} implique \ref{ItemThoDigLEQEXRiv}] En dimension un, tout endomorphisme est diagonalisable, nous supposons donc que \( \dim E=n\geq 2\). Nous procédons par récurrence sur le nombre de vecteurs propres connus de \( u\). Supposons avoir déjà trouvé \( p\) vecteurs propres \( e_1,\ldots, e_p\) de \( u\). Considérons \( H\), un hyperplan qui contient les vecteurs \( e_1,\ldots, e_p\). Soit \( F\) un supplémentaire de \( H\) stable par \( u\); par construction \( \dim F=1\) et si \( e_{p+1}\in F\), il doit être vecteur propre de \( u\).

\item[\ref{ItemThoDigLEQEXRiv} implique \ref{ItemThoDigLEQEXRi}] Nous supposons maintenant que \( u\) est diagonalisable. Soient \( \lambda_1,\ldots, \lambda_r\) les valeurs propres deux à deux distinctes, et considérons le polynôme
    \begin{equation}
        P(x)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Alors \( P(u)=0\). En effet si \( e_i\) est un vecteur propre pour la valeur propre \( \lambda_i\), 
    \begin{equation}
        P(u)e_i=\prod_{j\neq i}(u-\lambda_j)\circ(u-\lambda_i)e_i=0
    \end{equation}
    par le lemme \ref{LemQWvhYb}. Par conséquent \( P(u)\) s'annule sur une base.

\item[\ref{ITEMooZNJFooEiqDYp} implique \ref{ItemThoDigLEQEXRi}]
    Si la matrice \( A\) est diagonale alors le polynôme \( P=\prod_{i=1}^n(A-A_{ii}\mtu)\) est annulateur de \( A\).
        \item[\ref{ItemThoDigLEQEXRii} implique \ref{ITEMooZNJFooEiqDYp}]
            le polynôme minimal de \( u\) s'écrit 
            \begin{equation}
                \mu=(X-\lambda_1)\ldots(X-\lambda_r),
            \end{equation}
            et les espaces $E_i$ du lemme \ref{LemgnaEOk} sont les espaces propres \( E_i=\ker(u-\lambda_i)\). Nous avons donc une somme directe
            \begin{equation}
                E=E_1\oplus\ldots\oplus E_r.
            \end{equation}
            Dans chacun des espaces propres, $u$ a une matrice diagonale avec la valeur propre correspondante sur la diagonale. Une base de \( E\) constituée d'une base de chacun des espaces propres est donc une base comme nous en cherchons.
    \end{subproof}
\end{proof}

\begin{corollary}       \label{CorQeVqsS}
    Si \( u\) est diagonalisable et si \( F\) est une sous-espace stable par \( u\), alors
    \begin{equation}
        F=\bigoplus_{\lambda}E_{\lambda}(u)\cap F
    \end{equation}
    où \( E_{\lambda}(u)\) est l'espace propre de \( u\) pour la valeur propre \( \lambda\). En particulier la restriction de \( u\) à \( F\), \( u|_F\) est diagonalisable.
\end{corollary}

\begin{proof}
    Par le théorème \ref{ThoDigLEQEXR}, le polynôme \( \mu_u\) est scindé et ne possède que des racines simples. Notons le
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Les espaces \( E_i\) du lemme \ref{LemgnaEOk} sont maintenant les espaces propres.

    En ce qui concerne la diagonalisabilité de \( u|_F\), notons que nous avons une base de \( F\) composée de vecteurs dans les espaces \( E_{\lambda}(u)\). Cette base de \( F\) est une base de vecteurs propres de \( u\).
\end{proof}

\begin{lemma}
    Soit \( E\) un \( \eK\)-espace vectoriel et \( u\in\End(E)\). Si \( \Card\big( \Spec(u) \big)=\dim(E)\) alors \( u\) est diagonalisable.
\end{lemma}

\begin{proof}
    Soient \( \lambda_1,\ldots, \lambda_n\) les valeurs propres distinctes de \( u\). Nous savons que les espaces propres correspondants sont en somme directe (lemme \ref{LemjcztYH}). Par conséquent \( \Span\{ E_{\lambda_i}(u) \}\) est de dimension \( n\) est \( u\) est diagonalisable.
\end{proof}

Voici un résultat de diagonalisation simultanée. Nous donnerons un résultat de trigonalisation simultanée dans le lemme \ref{LemSLGPooIghEPI}.
\begin{proposition}[Diagonalisation simultanée]     \label{PropGqhAMei}
    Soit \( (u_i)_{i\in I}\) une famille d'endomorphismes qui commutent deux à deux.
    \begin{enumerate}
        \item       \label{ItemGqhAMei}
            Si \( i,j\in I\) alors tout sous-espace propre de \( u_i\) est stable par \( u_j\). Autrement dit \( u_j\big(E_{\lambda}(u)\big)\subset E_{\lambda}(u)\).
        \item
            Si les \( u_i\) sont diagonalisables, alors ils le sont simultanément.
    \end{enumerate}
\end{proposition}
\index{diagonalisation!simultanée}

\begin{proof}
    Supposons que \( u_i\) et \( u_j\) commutent et soit \( x\) un vecteur propre de \( u_i\) : \( u_ix=\lambda x\). Nous montrons que \( u_jx\in E_{\lambda}(u)\). Nous avons
    \begin{equation}
        u_i\big( u_j(x) \big)=u_j\big( u_i(x) \big)=\lambda u_j(x).
    \end{equation}
    Par conséquent \( u_j(x)\) est vecteur propre de \( u_i\) de valeur propre \( \lambda\).

    Montrons maintenant l'affirmation à propos des endomorphismes simultanément diagonalisables. Si \( \dim E=1\), le résultat est évident. Nous supposons également qu'aucun des \( u_i\) n'est multiple de l'identité. Nous effectuons une récurrence sur la dimension.

    Soit \( u_0\) un des \( u_i\) et considérons ses valeurs propres deux à deux distinctes \( \lambda_1,\ldots, \lambda_r\). Pour chaque \( k\) nous avons
    \begin{equation}
        E_{\lambda_k}(u_0)\neq E,
    \end{equation}
    sinon \( u_0\) serait un multiple de l'identité. Par contre le fait que \( u_0\) soit diagonalisable permet de décomposer \( E\) en espaces propres de \( u_0\) :
    \begin{equation}
        E=\bigoplus_{k}E_{\lambda_k}(u_0).
    \end{equation}
    Ce que nous allons faire est de simultanément diagonaliser les \( (u_i)_{i\in I}\) sur chacun des \( E_{\lambda_k}\) séparément. Par le point \ref{ItemGqhAMei}, nous avons \( u_i\colon E_{\lambda_k}(u_0)\to E_{\lambda_k}(u_0)\), et nous pouvons considérer la famille d'opérateurs
    \begin{equation}
        \left( u_i|_{E_{\lambda_k}(u_0)} \right)_{i\in I}.
    \end{equation}
    Ce sont tous des opérateurs qui commutent et qui agissent sur un espace de dimension plus petite. Par hypothèse de récurrence nous avons une base de \( E_{\lambda_k}(u_0)\) qui diagonalise tous les \( u_i\).
\end{proof}

\begin{example}     \label{ExewINgYo}
    Soit un espace vectoriel sur un corps \( \eK\). Un opérateur \defe{involutif}{involution} est un opérateur différent de l'identité dont le carré est l'identité. Typiquement une symétrie orthogonale dans \( \eR^3\). Le polynôme caractéristique d'une involution est \( X^2-1=(X+1)(X-1)\).
    
    Tant que \( 1\neq -1\), \( X^1-1\) est donc scindé à racines simples et les involutions sont diagonalisables (\ref{ThoDigLEQEXR}). Cependant si le corps est de caractéristique \( 2\), alors \( X^2-1=(X+1)^2\) et l'involution n'est plus diagonalisable.

    Par exemple si le corps est de caractéristique \( 2\), nous avons
    \begin{subequations}
        \begin{align}
            A&=\begin{pmatrix}
                1    &   1    \\ 
                0    &   1    
            \end{pmatrix}\\
            A^1&=\begin{pmatrix}
                1    &   2    \\ 
                0    &   1    
            \end{pmatrix}=\begin{pmatrix}
                1    &   0    \\ 
                0    &   1    
            \end{pmatrix}.
        \end{align}
    \end{subequations}
    Ce \( A\) est donc une involution mais n'est pas diagonalisable.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe, pas toujours}
%---------------------------------------------------------------------------------------------------------------------------

Il n'est pas vrai qu'une matrice de \( \eM(n,\eC)\) soit toujours diagonalisable. En effet le théorème \ref{ThoDigLEQEXR}\ref{ItemThoDigLEQEXRii} dit qu'une matrice est diagonalisable si et seulement si son polynôme minimal est scindé à racines simples. Certes sur \( \eC\) le polynôme minimal sera scindé, mais il ne sera pas spécialement à racines simples.

\begin{example}
    La matrice
    \begin{equation}
        A=\begin{pmatrix}
            0    &   1    \\ 
            0    &   0    
        \end{pmatrix}
    \end{equation}
    a pour polynôme caractéristique \( \chi_A(X)=X^2\). Cela est également son polynôme minimal, et ce n'est pas à racine simple. 

    Il est par ailleurs facile de voir que le seul espace propre de \( A\) est \( \Span\{ (1,0) \}\) (ici le span est sur \( \eC\)). Donc l'espace \( \eC^2\) ne possède pas de base de vecteurs propres de \( A\).
\end{example}

Ce qui est vrai, c'est que le polynôme caractéristique a des racines, et que ces racines correspondent à des vecteurs propres. Mais il n'y a pas toujours autant de vecteurs propres que la multiplicité des racines.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Trigonalisation : cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

Le lemme suivant est utile en soi et dit que toute matrice complexe est trigonalisable. Une démonstration alternative passant par le polynôme caractéristique sera présentée dans la remarque \ref{RemXFZTooXkGzQg} utilisant la proposition \ref{PropKNVFooQflQsJ}.
\begin{lemma}[Lemme de Schur complexe, trigonisation\cite{NormHKNPKRqV}]  \label{LemSchurComplHAftTq}
    Si \( A\in\eM(n,\eC)\), il existe une matrice unitaire \( U\) telle que \( UAU^{-1}\) soit triangulaire supérieure.
\end{lemma}
\index{lemme!Schur complexe}
%TODO : Le lemme de Schur est souvent énoncé en disant que si p est une représentation irréductible, alors les seuls endomorphismes de V commutant avec tous les p(g) sont les multiples de l'idenditié. Quel est le lien avec ceci ?

\begin{proof}
    Étant donné que \( \eC\) est algébriquement clos, nous pouvons toujours considérer un vecteur propre \( v_1\) de \( A\), de valeur propre \( \lambda_1\). Nous pouvons utiliser un procédé de Gram-Schmidt pour construire une base orthonormée \( \{ v,u_2,\ldots, u_n \}\) de \( \eR^n\), et la matrice (unitaire)
    \begin{equation}
        Q=\begin{pmatrix}
             \uparrow   &   \uparrow    &       &   \uparrow    \\
             v   &   u_2    &   \cdots    &   u_n    \\ 
             \downarrow   &   \downarrow    &       &   \downarrow
         \end{pmatrix}.
    \end{equation}
    Nous avons \( Q^{-1}AQe_1=Q^{-1} Av=\lambda Q^{-1} v=\lambda e_1\), par conséquent la matrice \( Q^{-1} AQ\) est de la forme
    \begin{equation}
        Q^{-1}AQ=\begin{pmatrix}
            \lambda_1    &   *    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( *\) représente une ligne quelconque et \( A_1\) est une matrice de \( \eM(n-1,\eC)\). Nous pouvons donc répéter le processus sur \( A_1\) et obtenir une matrice triangulaire supérieure (nous utilisons le fait qu'un produit de matrices orthogonales est une matrice orthogonale).  
\end{proof}
En particulier les matrices hermitiennes, anti-hermitiennes et unitaires sont trigonalisables par une matrice unitaire, qui peut être choisie de déterminant \( 1\).

\begin{lemma}       \label{LEMooRCFGooPPXiKi}
    Soit \( A\in \eM(n,\eC)\) et une matrice unitaire \( U\) telle que \( A=UTU^{-1}\) où \( T\) est triangulaire. 
    \begin{enumerate}
        \item
            En ce qui concerne les polynômes caractéristiques, \( \chi_A=\chi_T\).
        \item
            Pour les spectres, \( \Spec(A)=\Spec(T)\).
        \item
            Les valeurs propres de \( A\) sont les éléments diagonaux de \( T\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Vu que \( U\) commute évidemment avec \( \mtu\) nous avons
    \begin{equation}
        \chi_A(\lambda)=\det(A-\lambda \mtu)=\det(UTU^{-1}-\lambda\mtu)=\det\big( U(T-\lambda\mtu)U^{-1} \big).
    \end{equation}
    À ce niveau nous utilisons le fait que le déterminant soit multiplicatif \ref{PropYQNMooZjlYlA} pour conclure :
    \begin{equation}
        \chi_A(\lambda)=\det\big( U(T-\lambda\mtu)U^{-1} \big)=\det(U)\det(T-\lambda\mtu)\det(U^{-1})=\det(T-\lambda\mtu)=\chi_T(\lambda).
    \end{equation}

    Pour les spectres, l'égalité des polynômes caractéristique implique l'égalité des spectres parce que les valeurs propres sont les racines du polynôme caractéristique par le théorème \ref{ThoWDGooQUGSTL}.
    
    Les valeurs propres d'une matrice triangulaire sont les valeurs sur la diagonale.
\end{proof}

\begin{remark}
    Le lemme mentionne le fait que les valeurs propres de \( A\) sont les éléments diagonaux de \( T\). Mais attention : ceci ne dit rien au niveau des multiplicités géométriques. Un nombre peut être cinq fois sur la diagonale de \( T\) alors que l'espace propre correspondant pour \( A\) n'est que de dimension \( 1\). Exemple : la matrice
    \begin{equation}
        A=\begin{pmatrix}
            1    &   1    \\ 
            0    &   1    
        \end{pmatrix}
    \end{equation}
    a deux \( 1\) sur la diagonale. Le nombre \( 1\) est bien une valeur propre de \( A\), mais le système
    \begin{equation}
        A\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}=\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}
    \end{equation}
    donne \( y=0\) et donc un espace propre de dimension seulement \( 1\).
\end{remark}

\begin{corollary}   \label{CorUNZooAZULXT}
    Le polynôme caractéristique\footnote{Définition \ref{DefOWQooXbybYD}.} sur \( \eC\) d'une matrice s'écrit sous la forme
    \begin{equation}
        \chi_A(X)=\prod_{i=1}^r(X-\lambda_i)^{m_i}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres distinctes de \( A\) et \( m_i\) sont les multiplicités correspondantes.
\end{corollary}
\index{polynôme!caractéristique}

\begin{proof}
    Le lemme \ref{LemSchurComplHAftTq} nous donne l'existence d'une base de trigonalisation; dans cette base les valeurs propres de \( A\) sont sur la diagonale et nous avons 
    \begin{equation}
        \chi_A(X)=\det(A-X\mtu)=\det\begin{pmatrix}
            X-\lambda_1    &   *    &   *    \\
            0    &   \ddots    &   *    \\
            0    &   0    &   X-\lambda_r
        \end{pmatrix},
    \end{equation}
    qui vaut bien le produit annoncé.
\end{proof}

\begin{corollary}       \label{CORooTPDHooXazTuZ}
    Si \( A\in \eM(n,\eC)\) et \( k\in \eN\) alors
    \begin{equation}
        \Spec(A^k)=\{ \lambda^k\tq \lambda\in \Spec(A) \}.
    \end{equation}
\end{corollary}

\begin{proof}
    Par le lemme \ref{LemSchurComplHAftTq} nous avons une matrice unitaire \( U\) et une triangulaire \( T\) telles que \( A=UTU^{-1}\). En passant à a puissance \( k\) nous avons aussi
    \begin{equation}
        A^k=UT^kU^{-1}.
    \end{equation}
    Donc le spectre de \( A^k\) est celui de \( T^k\) (lemme \ref{LEMooRCFGooPPXiKi} et le fait qu'une puissance d'une matrice triangulaire est encore triangulaire). Or les éléments diagonaux de \( T^k\) sont les puissance \( k\)\ieme des éléments diagonaux de \( T\), qui sont les valeurs propres de \( A\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe, ce qu'on a}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Théorème spectral hermitien]      \label{LEMooVCEOooIXnTpp}
    Pour un opérateur hermitien\footnote{Définition \ref{DEFooKEBHooWwCKRK}.},
    \begin{enumerate}
        \item
            le spectre est réel,
        \item
            deux vecteurs propres à des valeurs propres distinctes sont orthogonales\footnote{Pour la forme \eqref{EqFormSesqQrjyPH}.}.
    \end{enumerate}
\end{lemma}
\index{spectre!matrice hermitienne}

\begin{proof}
    Soit \( v\) un vecteur de valeur propre \( \lambda\). Nous avons d'une part 
    \begin{equation}
        \langle Av, v\rangle =\lambda\langle v, v\rangle =\lambda\| v \|^2,
    \end{equation}
    et d'autre part, en utilisant le fait que \( A\) est hermitien,
    \begin{equation}
        \langle Av, v\rangle =\langle v, A^*v\rangle =\langle v, Av\rangle =\bar\lambda\| v \|^2,
    \end{equation}
    par conséquent \( \lambda=\bar\lambda\) parce que \( v\neq 0\).

    Soient \( \lambda_i\) et \( v_i\) (\( i=1,2\)) deux valeurs propres de \( A\) avec leurs vecteurs propres correspondants. Alors d'une part
    \begin{equation}
        \langle Av_1, v_2\rangle =\lambda_1\langle v_1, v_2\rangle ,
    \end{equation}
    et d'autre part
    \begin{equation}
        \langle Av_1, v_2\rangle =\langle v_1, Av_2\rangle =\lambda_2\langle v_1, v_2\rangle .
    \end{equation}
    Nous avons utilisé le fait que \( \lambda_2\) était réel. Par conséquent, soit \( \lambda_1=\lambda_2\), soit \( \langle v_1, v_2\rangle =0\).
\end{proof}

\begin{remark}      \label{REMooMLBCooTuKFmz}
    Un opérateur de la forme \( A^*A\) est évidemment hermitien. De plus ses valeurs propres sont toutes positives parce que si \( A^*Ax=\lambda v\) alors
    \begin{equation}
        0\leq \langle Av, Av\rangle =\langle A^*Av, v\rangle =\lambda\langle v, v\rangle .
    \end{equation}
    Donc \( \lambda\geq 0\).
\end{remark}

\begin{definition}  \label{DefWQNooKEeJzv}
    Un endomorphisme est \defe{normal}{normal!endomorphisme}\index{matrice!normale} s'il commute avec son adjoint.
\end{definition}

\begin{theorem}[Théorème spectral pour les matrices normales\footnote{Définition \ref{DefWQNooKEeJzv}}\cite{LecLinAlgAllen,OMzxpxE,HOQzXCw}]\index{théorème!spectral!matrices normales}  \index{diagonalisation!cas complexe}  \label{ThogammwA}
    Soit \( A\in\eM(n,\eC)\) une matrice de valeurs propres \( \lambda_1,\ldots, \lambda_n\) (non spécialement distinctes). Alors les conditions suivantes sont équivalentes :
    \begin{enumerate}
        \item   \label{ItemJZhFPSi}
            \( A\) est normale,
        \item   \label{ItemJZhFPSii}
            \( A\) se diagonalise par une matrice unitaire,
        \item
            \( \sum_{i,j=1}^n| A_{ij} |^2=\sum_{j=1}^n| \lambda_j |^2\),
        \item
            il existe une base orthonormale de vecteurs propres de \( A\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous allons nous contenter de prouver \ref{ItemJZhFPSi}\( \Leftrightarrow\)\ref{ItemJZhFPSii}.
    %TODO : le reste.

    Soit \( Q\) la matrice unitaire donnée par la décomposition de Schur (lemme \ref{LemSchurComplHAftTq}) : \( A=QTQ^{-1}\). Étant donné que \( A\) est normale nous avons
    \begin{equation}
        QTT^*Q^{-1}=QT^*TQ^{-1},
    \end{equation}
    ce qui montre que \( T\) est également normale. Or une matrice triangulaire supérieure normale est diagonale. En effet nous avons \( T_{ij}=0\) lorsque \( i>j\) et
    \begin{equation}
        (TT^*)_{ii}=(T^*T)_{ii}=\sum_{k=1}^n| T_{ki} |^2=\sum_{k=1}^n| T_{ik} |^2.
    \end{equation}
    Écrivons cela pour \( i=1\) en tenant compte de \( | T_{k1} |^2=0\) pour \( k=2,\ldots, n\),
    \begin{equation}
        | T_{11} |^2=| T_{11} |^2+| T_{12} |^2+\cdots+| T_{1n} |^2,
    \end{equation}
    ce qui implique que \( T_{11}\) est le seul non nul parmi les \( T_{1k}\). En continuant de la sorte avec \( i=2,\ldots, n\) nous trouvons que \( T\) est diagonale.

    Dans l'autre sens, si \( A\) se diagonalise par une matrice unitaire, \( UAU^*=D\), nous avons
    \begin{equation}
        DD^*=UAA^*U^*
    \end{equation}
    et 
    \begin{equation}
        D^*D=UA^*AU^*,
    \end{equation}
    qui ce prouve que \( A\) est normale.
\end{proof}

Tant que nous en sommes à parler de spectre de matrices hermitiennes\ldots Soit une matrice inversible \( A\in \GL(n,\eC)\). La matrice \( A^*A\) est hermitienne\footnote{Définition \ref{DEFooKEBHooWwCKRK}.} et le théorème \ref{LEMooVCEOooIXnTpp} nous assure que ses valeurs propres sont réelles. Par la remarque \ref{REMooMLBCooTuKFmz}, ses valeurs propres sont même positives.

\begin{lemma}[\cite{ooLMMRooUXhOdx}]   \label{LEMooHUGEooVYhZdZ}
    Si \( A\) est une matrice carré et inversible,
    \begin{equation}
        \Spec(A^*A)=\Spec(AA^*)
    \end{equation}
\end{lemma}

\begin{proof}
    Nous allons montrer l'égalité des polynômes caractéristiques. D'abord une simple multiplication montre que
    \begin{equation}
        (A^*A-\lambda\mtu)A^{-1}=A^{-1}(AA^*-\lambda\mtu).
    \end{equation}
    Nous prenons le déterminant de cette égalité en utilisant les propriétés \ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} et \ref{ITEMooZMVXooLGjvCy} :
    \begin{equation}
        \det(A^*A-\lambda\mtu)\det(A^{-1})=\det(A^{-1})\det(AA^*-\lambda\mtu).
    \end{equation}
    En simplifiant par \( \det(A^{-1})\) (qui est non nul parce que \( A\) est inversible) nous obtenons l'égalité des polynômes caractéristiques et donc l'égalité des spectres.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Lemme de Schur réel]  \label{LemSchureRelnrqfiy}
    Soit \( A\in\eM(n,\eR)\). Il existe une matrice orthogonale \( Q\) telle que \( Q^{-1}AQ\) soit de la forme
    \begin{equation}        \label{EqMtrTSqRTA}
        QAQ^{-1}=\begin{pmatrix}
            \lambda_1    &   *    &   *    &   *    &   *\\  
            0    &   \ddots    &   \ddots    &   \ddots    &   \vdots\\  
            0    &   0    &   \lambda_r    &   *    &   *\\  
            0    &   0    &   0    &   \begin{pmatrix}
                a_1    &   b_1    \\ 
                c_1    &   d_1    
            \end{pmatrix}&   *\\  
            0    &   0    &  0     &   0    &   \begin{pmatrix}
                a_s    &   b_s    \\ 
                c_s    &   d_s    
            \end{pmatrix}
        \end{pmatrix}.
    \end{equation}
    Le déterminant de \( A\) est le produit des déterminants des blocs diagonaux et les valeurs propres de \( A\) sont les \( \lambda_1,\ldots, \lambda_r\) et celles de ces blocs.
\end{lemma}
\index{lemme!Schur réel}

\begin{proof}
    Si la matrice \( A\) a des valeurs propres réelles, nous procédons comme dans le cas complexe. Cela nous fournit le partie véritablement triangulaire avec les valeurs propres \( \lambda_1,\ldots, \lambda_r\) sur la diagonale. Supposons donc que \( A\) n'a pas de valeurs propres réelles. Soit donc \( \alpha+i\beta \) une valeur propre (\( \beta\neq 0\)) et \( u+iv\) un vecteur propre correspondant où \( u\) et \( v\) sont des vecteurs réels. Nous avons
    \begin{equation}
        Au+iAv=A(u+iv)=(\alpha+i\beta)(u+iv)=\alpha u-\beta v+i(\alpha v+\beta v),
    \end{equation}
    et en égalisant les parties réelles et imaginaires,
    \begin{subequations}
        \begin{align}
            Au&=\alpha u-\beta v\\
            Av&=\alpha v+\beta u.
        \end{align}
    \end{subequations}
    Sur ces relations nous voyons que ni \( u\) ni \( v\) ne sont nuls. De plus \( u\) et \( v\) sont linéairement indépendants (sur \( \eR\)), en effet si \( v=\lambda u\) nous aurions \( Au=\alpha u-\beta\lambda u=(\alpha-\beta\lambda)u\), ce qui serait une valeur propre réelle alors que nous avions supposé avoir déjà épuisé toutes les valeurs propres réelles.

    Étant donné que \( u\) et \( v\) sont deux vecteurs réels non nuls et linéairement indépendants, nous pouvons trouver une base orthonormée \( \{ q_1,q_2 \}\) de \( \Span\{ u,v \}\). Nous pouvons étendre ces deux vecteurs en une base orthonormée \( \{ q_1,q_2,q_3,\ldots, q_n \}\) de \( \eR^n\). Nous considérons à présent la matrice orthogonale dont les colonnes sont formées de ces vecteurs : \( Q=[q_1\,q_2\,\ldots q_n]\).

    L'espace \( \Span\{ e_1,e_2 \}\) est stable par \( Q^{-1} AQ\), en effet nous avons
    \begin{equation}
        Q^{-1} AQe_1=Q^{-1} Aq_1=Q^{-1}(aq_1+bq_2)=ae_1+be_2.
    \end{equation}
    La matrice \( Q^{-1}AQ\) est donc de la forme
    \begin{equation}
        Q^{-1} AQ=\begin{pmatrix}
            \begin{pmatrix}
                \cdot    &   \cdot    \\ 
                \cdot    &   \cdot    
            \end{pmatrix}&   C_1    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( C_1\) est une matrice réelle \( 2\times (n-1)\) quelconque et \( A_1\) est une matrice réelle \( (n-2)\times (n-2)\). Nous pouvons appliquer une récurrence sur la dimension pour poursuivre.

    Notons que si \( A\) n'a pas de valeurs propres réelles, elle est automatiquement d'ordre pair parce que les valeurs propres complexes viennent par couple complexes conjuguées.

    En ce qui concerne les valeurs propres, il est facile de voir en regardant \eqref{EqMtrTSqRTA} que les valeurs propres sont celles des blocs diagonaux. Étant donné que \( QAQ^{-1}\) et \( A\) ont même polynôme caractéristique, ce sont les valeurs propres de \( A\).
\end{proof}

\begin{theorem}[Théorème spectral, matrice symétrique\cite{KXjFWKA}] \label{ThoeTMXla}
    Une matrice symétrique réelle,
    \begin{enumerate}
        \item
            a un spectre contenu dans \( \eR\)
        \item
            est diagonalisable par une matrice orthogonale.
    \end{enumerate}
    Si \( M\) est une matrice symétrique réelle alors \( \eR^n\) possède une base orthonormée de vecteurs propres de \( M\).
\end{theorem}
\index{diagonalisation!cas réel}
\index{rang!diagonalisation}
\index{endomorphisme!diagonalisation}
\index{spectre!matrice symétrique réelle}
\index{théorème!spectral!matrice symétrique}

\begin{proof}
    Soit \( A\) une matrice réelle symétrique. Si \( \lambda\) est une valeur propre complexe pour le vecteur propre complexe \( v\), alors d'une part \( \langle Av, v\rangle =\lambda\langle v, v\rangle \) et d'autre part \( \langle Av, v\rangle =\langle v, Av\rangle =\bar\lambda\langle v, v\rangle \). Par conséquent \( \lambda=\bar\lambda\).
    
    Le lemme de Schur réel \ref{LemSchureRelnrqfiy} donne une matrice orthogonale qui trigonalise \( A\). Les valeurs propres étant toutes réelles, la matrice \( QAQ^{-1}\) est même triangulaire (il n'y a pas de blocs dans la forme \eqref{EqMtrTSqRTA}). Prouvons que \( QAQ^{-1}\) est symétrique :
    \begin{equation}
        (QAQ^{-1})^t=(Q^{-1})^tA^tQ^t=QA^tQ^{-1}=QAQ^{-1}
    \end{equation}
    où nous avons utilisé le fait que \( Q\) était orthogonale (\( Q^{-1}=Q^t\)) et que \( A\) était symétrique (\( A^t=A\)). Une matrice triangulaire supérieure symétrique est obligatoirement une matrice diagonale.

    En ce qui concerne la base de vecteurs propres, soit \( \{ e_i \}_{i=1,\ldots, n}\) la base canonique de \( \eR^n\) et \( Q\) une matrice orthogonale e telle que \( A=Q^tDQ\) avec \( D\) diagonale. Nous posons \( f_i=Q^te_i\) et en tenant compte du fait que \( Q^t=Q^{-1}\) nous avons \( Af_i=Q^tDQQ^te_i=Q^t\lambda_i e_i=\lambda_if_i\). Donc les \( f_i\) sont des vecteurs propres de \( A\). De plus ils sont orthonormés parce que
    \begin{equation}
        \langle f_i, f_j\rangle =\langle Q^te_i, Q^te_j\rangle =\langle e_i, Q^tQe_j\rangle =\langle e_i, e_j\rangle =\delta_{ij}.
    \end{equation}
\end{proof}
Le théorème spectral pour les opérateurs auto-adjoints sera traité plus bas parce qu'il a besoin de choses sur les formes bilinéaires, théorème \ref{ThoRSBahHH}.
% et les choses sur la dégénérescences utilisent le théorème spectral, cas réel. Donc l'enchaînement est très loumapotiste.

\begin{remark}  \label{RemGKDZfxu}
    Une matrice symétrique est diagonalisable par une matrice orthogonale. Nous pouvons en réalité nous arranger pour diagonaliser par une matrice de \( \SO(n)\). Plus généralement si \( A\) est une matrice diagonalisable par une matrice \( P\in\GL^+(n,\eR)\) alors elle est diagonalisable par une matrice de \( \GL^-(n,\eR)\) en changeant le signe de la première ligne de \( P\). Et inversement.

    En effet, si nous avons \( P^tDP=A\), alors en notant \( *\) les quantités qui ne dépendent pas de \( a\), \( b\) ou~\( c\),
    \begin{equation}
        \begin{aligned}[]
        \begin{pmatrix}
            a    &   *    &   *    \\
            b    &   *    &   *    \\
            c    &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1    &       &       \\
                &   \lambda_2    &       \\
                &       &   \lambda_3
            \end{pmatrix}
            \begin{pmatrix}
                a    &   b    &   c    \\
                *    &   *    &   *    \\
                *    &   *    &   *
            \end{pmatrix}&=
        \begin{pmatrix}
            a    &   *    &   *    \\
            b    &   *    &   *    \\
            c    &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1a    &   \lambda_1b    &   \lambda_1c    \\
            *    &   *    &   *    \\
            *    &   *    &   *
        \end{pmatrix}\\
        &=\begin{pmatrix}
            \lambda_1 a^2+*   &   \lambda_1ab+*    &   \lambda_1ac  +*  \\
            \ldots    &   \ldots    &   \ldots    \\
            \ldots    &   \ldots    &   \ldots
        \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Nous voyons donc que si nous changeons les signes de \( a\), \( b\) et \( c\) en même temps, le résultat ne change pas.
\end{remark}

\begin{definition}[Matrice définie positive, opérateur définit positif]    \label{DefAWAooCMPuVM}
    Un opérateur sur un espace vectoriel sur \( \eC\) ou \( \eR\) est \defe{définit positif}{opérateur!définit positif} si toutes ses valeurs propres sont réelles et strictement positives.  Il est \defe{semi-définie positive}{semi-définie positive} si ses valeurs propres sont réelles positives ou nulles.
\end{definition}
Afin d'éviter l'une ou l'autre confusion, nous disons souvent \emph{strictement} définie positive pour positive.

Nous notons \( S^+(n,\eR)\)\nomenclature[A]{\( S^+(n,\eR)\)}{matrices symétriques semi-définies positives} l'ensemble des matrices réelles \( n\times n\) semi-définies positives. L'ensemble \( S^{++}(n,\eR)\)\nomenclature[A]{\( S^{++}(n,\eR)\)}{matrices symétriques strictement définies positives} est l'ensemble des matrices symétriques strictement définies positives.

\begin{remark}
    Nous ne définissons pas la notion de matrice définie positive pour une matrice non symétrique.
\end{remark}

Lorsqu'un énoncé parle d'une matrice symétrique, le premier réflexe est de la diagonaliser : considérer une matrice orthogonale \( T\) telle que \( T^tMT=D\) avec \( D\) diagonale. Et les valeurs propres sur la diagonale : \( D_{kl}=\delta_{kl}\lambda_k\). Les matrices symétriques définies positives ont cependant des propriétés même en dehors de leur base de diagonalisation.

\begin{lemma}   \label{LemWZFSooYvksjw}
    Soit une matrice symétrique \( M\).
    \begin{enumerate}
        \item       \label{ITEMooSKRAooOgHbGA}
           Elle est strictement définie positive si et seulement si \( \langle x, Mx\rangle >0\) pour tout \( x\) non nul dans \( \eR^n\).
        \item       \label{ITEMooMOZYooWcrewZ}
           Elle est semi définie positive si et seulement si \( \langle x, Mx\rangle \geq 0\) pour tout \( x\) non nul dans \( \eR^n\).
       \item        \label{ITEMooRRMFooHSOHxZ}
           Si elle est seulement définie positive, alors \( \langle x, Mx\rangle \geq \lambda\| x \|^2\) dès que \( \lambda\geq 0\) minore toutes les valeurs propres.
    \end{enumerate}
\end{lemma}

\begin{proof}
    Démonstration en trois parties.
    \begin{subproof}
    \item[\ref{ITEMooSKRAooOgHbGA}]
    Soit \( \{ e_i \}_{i=1,\ldots, n}\) une base orthonormée de vecteurs propres de \( M\) dont l'existence est assurée par le théorème spectral \ref{ThoeTMXla}. Nous nommons \( x_i\) les coordonnées de \( x\) dans cette base. Alors,
    \begin{equation}
        \langle x,Mx \rangle =\sum_{i,j}x_i\langle e_i, x_jMe_j\rangle =\sum_{i,j}x_ix_j\langle e_i, \lambda_je_j\rangle =\sum_{ij}x_ix_j\lambda_j\delta_{ij}=\sum_i\lambda_ix_i^2
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( M\). Cela est strictement positif pour tout \( x\) si et seulement si tous les \( \lambda_i\) sont strictement positifs.
\item[\ref{ITEMooMOZYooWcrewZ}]

    Nous avons encore 
    \begin{equation}
        \langle x, Mx\rangle =\sum_{i}\lambda_ix_i^2.
    \end{equation}
    Cela est plus grand ou égal à zéro si et seulement si tous les \( \lambda_i\) sont plus grands ou égaux à zéro.
        
\item[\ref{ITEMooRRMFooHSOHxZ}]

        Soit une matrice orthogonale \( T\) diagonalisant \( M\), c'est à dire telle que \( T^tMT=D\) avec \( D\) diagonale. Nous allons vérifier que 
        \begin{equation}
            \langle Tx, Mtx\rangle \geq \lambda\| Tx \|^2
        \end{equation}
        pour tout \( x\). Vu que \( T\) est une bijection\footnote{Une matrice orthogonale a un déterminant \( \pm 1\).}, cela impliquera le résultat pour tout \( x\). Si nous considérons la base de diagonalisation \( \{ e_k \}\) pour les valeurs propres \( \lambda_k\), nous avons le calcul
        \begin{subequations}
            \begin{align}
                \langle Tx, MTx\rangle &=\langle x, T^tMTx\rangle \\
                &=\langle x, Dx\rangle \\
                &=\sum_k\langle x, x_kDe_k\rangle \\
                &=\sum_k\lambda_kx_k \underbrace{\langle x, e_k\rangle }_{=x_k}\\
                &\geq \sum_k\lambda| x_k |^2\\
                &=\lambda\| x \|^2\\
                &=\lambda\| Tx \|^2.
            \end{align}
        \end{subequations}
        Au dernier passage nous avons utilisé le fait que \( T\) est une isométrie (proposition \ref{PropKBCXooOuEZcS}).

    \end{subproof}
\end{proof}

Les personnes qui aiment les vecteurs lignes et colonnes écriront des inégalités comme
\begin{equation}
    x^tMx\geq x^tx.
\end{equation}
Tout à l'autre bout du spectre des personnes névrosées des notations, on trouvera des inégalités comme
\begin{equation}
    M(x\otimes x)\geq x\cdot x.
\end{equation}
Le penchant personnel de l'auteur de ces lignes est la notation avec le produit tensoriel. Si vous aimez ça, vous pouvez lire \ref{SeOOpHsn}.

La notation adoptée ici avec le produit scalaire \( \langle x, Mx\rangle \) est entre les deux. Elle a l'avantage de n'être pas technologique comme le produit tensoriel (si vous y mettez les pieds, vous devez savoir ce que vous faites), tout en évitant de se casser la tête à savoir qui est un vecteur ligne ou un vecteur colonne.

\begin{corollary}
    Une matrice symétrique strictement définie positive est inversible.
\end{corollary}

\begin{proof}
    Si \( Ax=0\) alors \( \langle Ax, x\rangle =0\). Mais dans le cas d'une matrice strictement définie positive, cela implique \( x=0\) par le lemme \ref{LemWZFSooYvksjw}.
\end{proof}

\begin{lemma}
    Pour une base quelconque, les éléments diagonaux d'une matrice symétrique semi-définie positive sont positifs. Si la matrice est strictement définie positive, alors les éléments diagonaux sont strictement positifs.
\end{lemma}

\begin{proof}
    Il s'agit d'une application du lemme \ref{LemWZFSooYvksjw}. Si \( A\) est définie positive et que \( \{ e_i \}\) est une base, alors
    \begin{equation}
        A_{ii}=\langle Ae_i, e_i\rangle \geq \lambda\| e_i \|^2=\lambda\geq 0.
    \end{equation}
    Si \( A\) est strictement définie positive, alors \( \lambda\) peut être choisit strictement positif.
\end{proof}
 
%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Pseudo-réduction simultanée}
%---------------------------------------------------------------------------------------------------------------------------

\begin{corollary}[Pseudo-réduction simultanée\cite{JMYQgLO}]  \label{CorNHKnLVA}
    Soient \( A,B\in \gS(n,\eR)\) avec \( A\) définie positive\footnote{Définition \ref{DefAWAooCMPuVM}.}. Alors il existe \( Q\in \GL(n,\eR)\) tell que \( Q^tBQ\) soit diagonale et \( Q^tAQ=\mtu\).
\end{corollary}

\begin{proof}
    Nous allons noter \( x\cdot y\) le produit scalaire usuel de \( \eR^n\) et \( \{ e_i \}_{i=1,\ldots, n}\) sa base canonique.

    Vu que \( A\) est définie positive, nous avons que l'expression\footnote{On peut aussi l'écrire de façon plus matricielle sous la forme \( \langle x, y\rangle =x^tAy\).} \( \langle x, y\rangle =x\cdot Ay\) est un produit scalaire sur \( \eR^n\). Autrement dit, \( E\) muni de cette forme bilinéaire symétrique est un espace euclidien, ce qui fait dire à la proposition \ref{PropUMtEqkb} qu'il existe une base de \( \eR^n\) orthonormée \( \{ f_i \}_{i=1,\ldots, n}\) pour ce produit scalaire, c'est à dire qu'il existe une matrice \( P\in \GL(n,\eR)\) telle que \( P^tAP=\mtu\). Ici, \( P\) est la matrice de changement de base de la base canonique à notre base orthonormée, c'est à dire la matrice qui fait \( Pe_i=f_i\) pour tout \( i\). Voyons cela avec un peu de détails.

    Pour savoir ce que valent les éléments de la matrice \( P^tAP\), nous nous souvenons que \( P^tAPe_j\) est un vecteur dont les coordonnées sont les éléments de la \( j\)\ieme colonne de \( P^tAP\). Nous avons donc \( (P^tAP)_{ij}=e_i\cdot P^tAPe_i\). Calculons :
    \begin{equation}
            (P^tAP)_{ij}=e_i\cdot P^tAPe_i
            =Pe_i\cdot APe_j
        =f_i\cdot Af_j
        =\langle f_i, f_j\rangle 
        =\delta_{ij}
    \end{equation}
    où nous avons utilisé le fait que \( A\) était auto-adjointe pour la passer de l'autre côté du produit scalaire (usuel). Au final nous avons effectivement \( P^tAP=\mtu\).

    La matrice \( P^tBP\) est une matrice symétrique, donc le théorème spectral \ref{ThoeTMXla} nous donne une matrice \( R\in \gO(n,\eR)\) telle que \( R^tP^tBPR\) soit diagonale. En posant maintenant \( Q=PR\) nous avons la matrice cherchée.
\end{proof}
Note : nous avons prouvé la pseudo-réduction simultanée comme corollaire du théorème de diagonalisation des matrices symétriques \ref{ThoeTMXla}. Il aurait aussi pu être vu comme un corollaire du théorème spectral \ref{ThoRSBahHH} sur les opérateurs auto-adjoints via son corollaire \ref{CorSMHpoVK}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Équivalence des normes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{normes_equiv}

Au premier coup d'œil, les notions dont nous parlons dans ce chapitre ont l'air très générales. Nous prenons en effet n'importe quel espace vectoriel $V$ de dimension finie, et nous le munissons de n'importe quelle norme (rien que dans $\eR^m$ nous en avons défini une infinité par l'équation \eqref{EqDeformeLp}). À partir de ces données, nous définissons les boules, la topologie, l'adhérence, etc.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{En dimension finie}
%---------------------------------------------------------------------------------------------------------------------------

Dans $\eR^n$, les normes $\| . \|_{L^1}$, $\| . \|_{L^2}$ et $\| . \|_{\infty}$ ne sont pas égales. Cependant elles ne sont pas complètement indépendante au sens où l'on sent bien que si un vecteur sera grand pour une norme, il sera également grand pour les autres normes; les normes «vont dans le même sens». Cette notion est précisée par le concept de norme équivalente. 

\begin{definition}		\label{DefEquivNorm}
    Deux normes $N_1$ et $N_2$ sur $\eR^m$ sont \defe{\wikipedia{fr}{Norme_équivalente}{équivalentes}}{equivalence@équivalence!norme}\index{norme!équivalence}\index{équivalence!de norme} s'il existe deux nombres réels strictement positifs $k_1$ et $k_2$ tels que
	\begin{equation}
		k_1N_1(x)\leq N_2(x)\leq k_2 N_1(x),
	\end{equation}
	pour tout $x$ dans $\eR^m$. Dans ce cas nous écrivons que $N_1\sim N_2$.
\end{definition}
Il est possible de démontrer que cette notion est une relation d'équivalence (définition \ref{DefHoJzMp}) sur l'ensemble des normes existantes sur $\eR^m$.

\begin{proposition} \label{PropLJEJooMOWPNi}
    Pour \( \eR^N\), nous avons les équivalences de normes $\| . \|_{L^1}\sim\| . \|_{L^2}$, $\| . \|_{L^1}\sim\| . \|_{\infty}$ et $\| . \|_{L^2}\sim\| . \|_{\infty}$. Plus précisément nous avons les inégalités
    \begin{enumerate}
        \item\label{ItemABSGooQODmLNi}
           $ \| x \|_2\leq \| x \|_1\leq\sqrt{n}\| x \|_2$
        \item\label{ItemABSGooQODmLNii}
            $\| x \|_{\infty}\leq \| x \|_1\leq n \| x \|_{\infty}$
        \item\label{ItemABSGooQODmLNiii}
            $\| x \|_{\infty}\leq \| x \|_2\leq \sqrt{n}\| x \|_{\infty}$
    \end{enumerate}
\end{proposition}


\begin{proof}
    En mettant au carré la première inégalité nous voyons que nous devons vérifier l'inégalité
    \begin{equation}
        | x_1 |^2+\cdots+| x_n |^2\leq\big( | x_1 |+\cdots+| x_n | \big)^2
    \end{equation}
    qui est vraie parce que le membre de droite est égal au carré de chaque terme plus les double produits. La seconde inégalité provient de l'inégalité de Cauchy-Schwarz (théorème \ref{ThoAYfEHG}) sur les vecteurs
    \begin{equation}
        \begin{aligned}[]
            v&=\begin{pmatrix}
                1/n    \\ 
                \vdots    \\ 
                1/n    
            \end{pmatrix},
            &w&=\begin{pmatrix}
                | x_1 |    \\ 
                \vdots    \\ 
                | x_n |    
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Nous trouvons 
    \begin{equation}
        \frac{1}{ n }\sum_i| x_i |\leq\sqrt{b\cdot\frac{1}{ n }}\sqrt{\sum_i| x_i |^2},
    \end{equation}
    et par conséquent
    \begin{equation}
        \sum_i| x_i |\leq\sqrt{n}\| x \|_2.
    \end{equation}
    
    La première inégalité de \ref{ItemABSGooQODmLNiii}<++> se démontre en remarquant que si \( a\) et \( b\) sont positifs, \( a\leq\sqrt{a^2+b}\). En appliquant cela à \( a=\max_i| x_i |\), nous avons
    \begin{equation}
        \max_i| x_i |\leq\sqrt{ | x_1 |^2+\cdots+| x_n |^2  }
    \end{equation}
    parce que \( \max_i| x_i |\) est évidemment un des termes de la somme. Pour la seconde inégalité de \ref{ItemABSGooQODmLNiii}, nous avons
    \begin{equation}
        \sqrt{\sum_k| x_k |^2}\leq\left( \sum_k\max_i| x_i |^2 \right)^{1/2}=\sqrt{n}\| x \|_{\infty}.
    \end{equation}
    Pour obtenir cette inégalité, nous avons remplacé tous les termes \( | x_k |\) par le maximum.
\end{proof}

En réalité, toutes les normes \( \| . \|_{L^p}\) et \( \| . \|_{\infty}\) sont équivalentes et, plus généralement, nous avons le résultat suivant, très étonnant à première vue, et en réalité assez difficile à prouver :
\begin{theorem}[\cite{TrenchRealAnalisys}]		\label{ThoNormesEquiv}
	Sur un espace vectoriel de dimension finie, toutes les normes (pas seulement les normes $L^p$ que nous avons définies sur $\eR^m$) sont équivalentes.
\end{theorem}
% TODO : la preuve est à la page 583 de Trench.

\begin{corollary}
    Soit \( V\) un espace vectoriel de dimension finie et \( \| . \|_1\), \( \| . \|_2\) deux normes sur \( V\). Alors l'identité \( \id\colon V\to V\) est un isomorphisme d'espace topologique \( (V,\| . \|_1)\to (V,\| . \|_2)\).

    De plus les ouverts sont les mêmes : une partie de \( V\) est ouverte dans \( (V,\| . \|_1)\) si et seulement si elle est ouverte dans \( (V,\| . \|_2)\).
\end{corollary}

Le théorème d'équivalence de norme sera utilisé pour montrer que l'ensemble des formes quadratiques non dégénérées de signature \( (p,q)\) est ouvert dans l'ensemble des formes quadratiques, proposition \ref{PropNPbnsMd}. Plus généralement il est utilisé à chaque fois que l'on fait de la topologie sur les espaces de matrices en identifiant \( \eM(n,\eR)\) à \( \eR^{n^2}\), pour se rassurer en se disant que ce qu'on fait ne dépend pas de la norme choisie.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Contre-exemple en dimension infinie}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecPOlynomesCE}

Lorsque nous considérons des espaces vectoriels de dimension infinie, les choses ne sons plus aussi simples. Nous voyons ici sur l'exemple de l'espace des polynômes que le théorème \ref{ThoNormesEquiv} n'est plus valable si on enlève l'hypothèse de dimension finie.

On considère l'ensemble des fonctions polynômiales à coefficients réels sur  l'intervalle $[0,1]$.
\begin{equation}
\mathcal{P}_\eR([0,1])=\{p:[0,1]\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \ldots, \, a_i\in\eR,\,\forall i\in \eN\}.
\end{equation}
Cet ensemble, muni des opérations usuelles de somme entre polynômes et multiplications par les scalaires, est un espace vectoriel.  

Sur $\mathcal{P}(\eR)$ on définit les normes suivantes 
\begin{equation}
\begin{aligned}
&\|p\|_\infty=\sup_{x\in[0,1]}\{p(x)\},\\
&\|p\|_1 =\int_0^1|p(x)|\, dx,\\
&\|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}.\\
\end{aligned}
\end{equation}
Les inégalités suivantes sont  immédiates
\begin{equation}
\begin{aligned}
&\|p\|_1 =\int_0^1|p(x)|\, dx\leq \|p\|_\infty,\\
&\|p\|_2 =\left(\int_0^1|p(x)|^2\, dx\right)^{1/2}\leq \|p\|_\infty,\\
\end{aligned}
\end{equation}
mais la norme $\|\cdot\|_\infty$ n'est  équivalente ni à $\|\cdot\|_1$, ni à $\|\cdot\|_2$. Soit $p_k(x)= x^k$. Alors
\begin{equation}
\begin{aligned}
&\|p_k\|_\infty=1,\\
&\|p_k\|_1 =\int_0^1x^k\, dx=  \frac{1}{k+1},\\
&\|p_k\|_2 =\left(\int_0^1x^{2k}\, dx\right)^{1/2}=\sqrt{\frac{1}{2k+1}}.
\end{aligned}
\end{equation}
Pour $k\to \infty$ les normes $\|p_k\|_1$, $\|p_k\|_2$ tendent vers zéro, alors que la norme $\|p_k\|_\infty$ est constante, donc les normes ne sont pas équivalentes parce que il n'existe pas un nombre positif $m$ tel que 
\begin{equation}
\begin{aligned}
& m \|p_k\|_\infty\leq \|p_k\|_1 ,\\
& m \|p_k\|_\infty\leq \|p_k\|_2 ,\\
\end{aligned}
\end{equation}
uniformément pour tout $k$ dans $\eN$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espace vectoriel normé}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SeckwyQjK}

\begin{definition}[\cite{BrunelleMatricielle}]  \label{DefOYPooZIoWnI}
    Soit \( E\) un espace vectoriel (pas spécialement de dimension finie). Une  \defe{norme}{norme} sur $E$ est une application $\| . \|\colon E\to \eR$ telle que
    \begin{enumerate}
        \item
            $\| v \|=0$ seulement si $A=0$,
        \item
            $\| \lambda v \|=| \lambda |\cdot\| v \|$,
        \item
            $\| v+w \|\leq\| v \|+\| w \|$

    \end{enumerate}
    pour tout $v,w\in E$ et pour tout $\lambda\in\eR$.
\end{definition}

\begin{definition}  \label{DefDQRooVGbzSm}
    Si \( V\) et \( W\) sont des espaces vectoriels nous munissons \( \aL(V,W)\) d'une structure d'espace vectoriel en définissant la somme et le produit par un scalaire de la façon suivante. Si $T$ et $U$ sont des élément de $\aL(V,W)$ et si $\lambda$ est un réel, nous définissons les éléments $T+U$ et $\lambda T$ par
    \begin{enumerate}
        \item
            $(T+U)(x)=T(x)+U(x)$;
        \item
            $(\lambda T)(x)=\lambda T(x)$
    \end{enumerate}
    pour tout \( x\in V\).
\end{definition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Norme opérateur}
%---------------------------------------------------------------------------------------------------------------------------

La proposition suivante donne une norme (au sens de la définition \ref{DefNorme}) sur $\aL(V,W)$ afin d'obtenir un espace vectoriel normé.
\begin{proposition}		\label{PropNormeAppLineaire}
    Soit le nombre
	\begin{equation}
        \|T\|_{\aL}=\sup_{x\in V}\frac{\|T(x)\|_{W}}{\|x\|_{V}}.
	\end{equation}
    \begin{enumerate}
        \item
            Si \( V\) est de dimension finie, alors \( \| T \|_{\aL}<\infty\).
        \item
            L'application \( T\mapsto\| T \|_{\aL}\) est une norme sur l'espace vectoriel des applications linéaires \( V\to W\).
        \item
            Nous avons la formule
            \begin{equation}    \label{EqFZPooIoecGH}
                \| T \|_{\aL}=\sup_{x\in V}\frac{\|T(x)\|_{W}}{\|x\|_{V}} =\sup_{\|x\|_{V}=1}\|T(x)\|_{W}
            \end{equation}
    \end{enumerate}
\end{proposition}
\index{norme!d'une application linéaire}

\begin{proof}
    Si \( V\) est de dimension finie alors l'ensemble $\{ \| x \|= 1 \}$ est compact par le théorème de Borel-Lebesgue \ref{ThoXTEooxFmdI}. Alors la fonction 
    \begin{equation}
        x\mapsto \frac{ \| T(x) \| }{ \| x \| }
    \end{equation}
    est une fonction continue sur un compact. Le corollaire \ref{CorFnContinueCompactBorne} nous dit alors qu'elle est bornée. Le supremum est donc un nombre réel fini.

    Nous vérifions que l'application $\| . \|$ de $\aL(V,W)$ dans $\eR$ ainsi définie est effectivement une norme.
    \begin{enumerate}
        \item
            $\|T\|_{\aL}=0$ signifie que $\|T(x)\|=0$ pour tout $x$ dans $V$. Comme  $\|\cdot\|_W$ est une norme nous concluons que $T(x)=0_{n}$ pour tout $x$ dans $V$, donc $T$ est l'application nulle. 
    \item
        Pour tout $a$ dans $\eR$ et tout  $T$ dans $\aL(V,W)$ nous avons
        \begin{equation}
            \|aT\|_{\mathcal{L}}=\sup_{\|x\|_{V}\leq 1}\|aT(x)\|_{W}=|a|\sup_{\|x\|_{V}\leq 1}\|T(x)\|_{W}=|a|\|T\|_{\mathcal{L}}.
        \end{equation}
    \item 
        Pour tous $T_1$ et $T_2$ dans $\aL(V,W)$ nous avons
      \begin{equation}\nonumber
        \begin{aligned}
           \|T_1+ T_2\|_{\mathcal{L}}&=\sup_{\|x\|\leq 1}\|T_1(x)+T_2(x)\|\leq\\
     &\leq\sup_{\|x\|\leq 1}\|T_1(x)\| +\sup_{\|x\|\leq 1}\|T_2(x)\|\\
     &=\|T_1\|\|T_2\|.
        \end{aligned}
      \end{equation}
    \end{enumerate}


    Enfin nous prouvons la formule alternative \eqref{EqFZPooIoecGH}. Nous allons montrer que les ensembles sur lesquels ont prend le supremum sont en réalité les mêmes :
    \begin{equation}
        \underbrace{\left\{ \frac{ \| Ax \| }{ \| x \| }\right\}_{x\neq 0}}_{A}=\underbrace{\left\{ \| Ax \|\tq \| x \|=1 \right\}}_{B}.
    \end{equation}
    Attention : ce sont des sous-ensembles de réels; pas de sous-ensembles de \( \eM(\eR)\) ou des sous-ensembles de \( \eR^n\).

    Pour la première inclusion, prenons un élément de \( A\), et prouvons qu'il est dans \( B\). C'est à dire que nous prenons \( x\in V\) et nous considérons le nombre \( \| Ax \|/\| x \|\). Le vecteur \( y=x/\| x \|\) est un vecteur de norme $1$, donc la norme de \( Ay\) est un élément de \( B\), mais
    \begin{equation}
        \| Ay \|=\frac{ \| Ax \| }{ \| x \| }.
    \end{equation}
    Nous avons donc \( A\subset B\).

    L'inclusion \( B\subset A\) est immédiate.
\end{proof}

La proposition \ref{PropNormeAppLineaire} nous permet de définir une famille de normes sur \( \aL(E,F)\) dès que \( E\) et \( F\) sont des espaces vectoriels normés.
\begin{definition}[Norme opérateur\cite{ooTZRDooWmjBJi}, thème \ref{THEMEooHSLLooBQpFAr}]          \label{DefNFYUooBZCPTr}
    Soit un opérateur \( A\) entre les espaces vectoriels normés \( E\) et \( F\). Le nombre
    \begin{equation}
        \| A \|_{\aL}=\sup_{x\in E}\frac{\|A(x)\|_{F}}{\|x\|_{E}} =\sup_{\|x\|_{E}=1}\|T(x)\|_{F}
    \end{equation}
    est la \defe{norme opérateur}{norme!d'application linéaire} de $A$. Nous disons que cette norme est \defe{subordonnée}{subordonnée!norme} aux normes choisies sur \( E\) et \( F\). 
\end{definition}
En d'autres termes, il y a autant de normes opérateur sur \( \aL(E,F)\) qu'il y a de paires de choix de normes sur \( E\) et \( F\). En particulier, cela donne lieu à toutes les normes \( \| A \|_p\) qui correspondent aux normes \( \| . \|_p\) sur \( \eR^n\). 

La norme opérateur est liée à la continuité par la proposition \ref{PROPooQZYVooYJVlBd}.

\begin{definition}
    La \defe{topologie forte}{topologie!forte} sur l'espace des opérateurs est la topologie de la norme opérateur.  
\end{definition}
Lorsque nous considérons un espace vectoriel d'applications linéaires, nous considérons toujours\footnote{Sauf lorsque les événements nous forceront à trahir.} dessus la topologie liée à cette norme. 

Il existe aussi la \defe{topologie faible}{topologie!faible} donnée par la notion de convergence\quext{Est-ce qu'on peut décrire cette topologie à partir de ses ouverts ? Facilement ?} \( A_i\to A\) si et seulement si \( A_ix\to Ax\) pour tout \( x\in E\).
    %TODO : il faut mettre au clair quelle est vraiment la topologie faible à partir des ouverts.

\begin{probleme}
    Je crois, mais demande confirmation, que la topologie faible est celle des semi normes \( \{ p_v \}_{v\in E}\) données par \( p_v(A)=\| A \|\). En effet la notion de convergence associée par la proposition \ref{PropQPzGKVk} est \( A_i\to A\) si et seulement si \( p_v(A_i-A)\to 0\). Cette condition signifie \( \| A_i(v)-A(v) \|\to 0\), c'est à dire \( A_i(v)\to A(v)\).

    Si le lecteur veut parler de cela au jury d'un concours, il est évident qu'il devra être capable d'ajouter des petits symboles au-dessus de toutes les flèches «\( \to\)» du paragraphe précédent pour indiquer pour quelles topologies sont les convergences dont on parle.
\end{probleme}

\begin{remark}
    Il faut noter que la topologie faible n'est pas une topologie métrique. Cela même si la condition \( A_ix\to Ax\), elle, est métrique vu qu'elle est écrite dans \( E\).

    Dans le cas où \( E\) est de dimension infinie, la topologie faible est réellement différente de la topologie forte. Nous verrons à la sous-section \ref{subsecaeSywF} que dans le cas des projections sur un espaces de Hilbert, l'égalité
    \begin{equation}
        \sum_{i=1}^{\infty}\pr_{u_i}=\id
    \end{equation}
    est vraie pour la topologie faible, mais pas pour la topologie forte.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Norme d'algèbre}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Norme d'algèbre\cite{ooTZRDooWmjBJi}]  \label{DefJWRWQue}
    Si \( A\) est une algèbre\footnote{Définition \ref{DefAEbnJqI}.}, une \defe{norme d'algèbre}{norme!d'algèbre} sur \( A\) est une norme telle que pour toute \( u,v\in A\),
    \begin{equation}
        \| uv \|\leq \| u \|\| v \|.
    \end{equation}
\end{definition}
L'intérêt d'une norme d'algèbre est entre autres de mieux se comporter pour les séries, voir par exemple \ref{subsecEVnZXgf}.

\begin{definition}[\cite{ooYLHAooCzQvoa}]      \label{DEFooEAUKooSsjqaL}
    Le \defe{rayon spectral}{rayon!spectral} d'une matrice carrée $A$, noté $\rho(A)$, est défini de la manière suivante :
    \begin{equation}    \label{EQooNVNOooNjJhSS}
        \rho(A)=\max_i|\lambda_i|
    \end{equation}
    où les $\lambda_i$ sont les valeurs propres de $A$.
\end{definition}

\begin{normaltext}
    Quelque remarques sur la définition du rayon spectral.
    \begin{itemize}
        \item 
             Même si \( A\) est une matrice réelle, les valeurs propres sont dans \( \eC\). Donc dans \eqref{EQooNVNOooNjJhSS}, \( | \lambda_i |\) est le module dans \( \eC\) de \( \lambda_i\).
        \item
            Vu que les valeurs propres de \( A\) sont les racines de son polynôme caractéristique (théorème \ref{ThoWDGooQUGSTL}), il y en a un nombre fini et le maximum est bien définit.
        \item
            La définition s'applique uniquement pour les espaces de dimension finie.
    \end{itemize}
\end{normaltext}

\begin{lemma}       \label{LEMooIBLEooLJczmu}
    Soient des espaces vectoriels normés \( E\) et \( F\), de dimension finie ou non sur les corps \( \eR\) ou \( \eC\). Pour tout \( A\in \aL(E,F)\), et pour tout \( u\in E\) nous avons la majoration
    \begin{equation}
        \| Au \|\leq \| A \|\| u \|
    \end{equation}
    où la norme sur \( A\) est la norme opérateur subordonnée à la norme sur \( u\).
\end{lemma}

\begin{proof}
    Si \( u\in E\) alors, étant donné que le supremum d'un ensemble est plus grand ou égal à chacun de éléments qui le compose,
    \begin{equation}
        \| A \|=\sup_{x\in E}\frac{ \| Ax \| }{ \| x \| }\geq \frac{ \| Au \| }{ \| u \| },
    \end{equation}
    donc le résultat annoncé : \( \| Au \|\leq \| A \|\| u \|\).
\end{proof}

\begin{proposition}[\cite{ooYLHAooCzQvoa}]      \label{PROPooKLFKooSVnDzr}
    Soit une matrice \( A\in \eM(n,\eC)\) de rayon spectral \( \rho(A)\). Soit une norme \( \| . \|\) sur \( \eC^n\) et la norme opérateur correspondante. Alors
    \begin{equation}
        \rho(A)\leq \| A^k \|^{1/k}
    \end{equation}
    pour tout \( k\in \eN\).
\end{proposition}

\begin{proof}
    Soit \( v\in \eC^n\) et \( \lambda\in \eC\) un coupe vecteur-valeur propre. Nous avons \( \| Av \|=| \lambda |\| v \|\) et aussi
    \begin{equation}
        | \lambda |^k\| v \|=\| \lambda^kv \|=\| A^kv \|\leq \| A^k \|\| v \|.
    \end{equation}
    La dernière inégalité est due au fait que nous avons choisit sur \( \eM(n,\eC)\) la norme subordonnée à celle choisie sur \( \eC^n\), via le lemme \ref{LEMooIBLEooLJczmu}. Nous simplifions par \( \| v \|\) et obtenons \( | \lambda |\leq \| A^k \|^{1/k}\). Étant donné que \( \rho(A)\) est la maximum de tous les \( \lambda\) possibles, la majoration passe au maximum :
    \begin{equation}
        \rho(A)\leq \| A^k \|^{1/k}.
    \end{equation}
\end{proof}

\begin{lemma}[La norme opérateur est une norme d'algèbre\cite{MonCerveau}]   \label{LEMooFITMooBBBWGI}
    Soient des espaces vectoriels normés \( E\), \( F\) et \( G\). Soient des opérateurs linéaires bornés \( B\colon E\to F\), \( A\colon F\to G\). Alors
    \begin{equation}
        \| AB \|\leq \| A \|\| B \|.
    \end{equation}
\end{lemma}

\begin{proof}
    
    Nous avons les (in)égalités suivantes :
    \begin{subequations}
        \begin{align}
            \| AB \|&=\sup_{x\in E}\frac{ \| ABx \|_G }{ \| x \|_E }\\
            &=\sup_{\substack{x\in E\\Bx\neq 0}}\frac{ \| ABx \| }{ \| x \| }\frac{ \| Bx \|_F }{ \| Bx \|_F }\\
            &=\sup_{\substack{x\in E\\Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| }\frac{ \| Bx \| }{ \| x \| }\\
            &\leq\underbrace{\sup_{\substack{x\in E\\Bx\neq 0}}\frac{ \| ABx \| }{ \| Bx \| }}_{\leq\| A \|}\underbrace{\sup_{\substack{y\in E\\By\neq 0}}\frac{ \| Bx \| }{ \| y \| }}_{=\| B \|}\\
            &\leq \| A \|\| B \|.
        \end{align}
    \end{subequations}
    La dernière inégalité provient que dans \( \sup_{\substack{x\in E\\Bx\neq 0}}\| ABx \|/\| x \|\), le supremum est pris sur un ensemble plus petit que celui sur lequel porte la définition de la norme de \( A\) : seulement l'image de \( B\) au lieu de tout l'espace de départ de \( A\).
\end{proof}

La chose impressionnante dans la proposition suivante est que \( \rho(A)\) est définit indépendamment du choix de la norme sur \( \eM(n,\eK)\) ou sur \( \eK\). Lorsque nous écrivons \( \| A \|\), nous disons implicitement qu'une norme a été choisie sur \( \eK\) et que nous avons pris la norme subordonnée sur \( \eM(n,\eK)\).
\begin{proposition}[\cite{ooETMNooSrtWet}]      \label{PROPooWZJBooTPLSZp}
    Soit \( A\) une matrice de \( \eM(n,\eR)\) ou \( \eM(n,\eC)\). Alors 
    \begin{equation}
        \rho(A)\leq \| A \|.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous devons séparer les cas suivant que le corps de base soit \( \eR\) ou \( \eC\).

    \begin{subproof}
        \item[Pour \( A\in \eM(n,\eC)\)]
            Soit \( \lambda\) une valeur propre de \( A\) telle que \( | \lambda |\) soit la plus grande. Nous avons donc \( \rho(A)=| \lambda |\). Soit un vecteur propre \( u\in \eC^n\) pour la valeur propre \( \lambda\). En prenant la norme sur l'égalité \( Au=\lambda u\), et en utilisant le lemme \ref{LEMooIBLEooLJczmu},
            \begin{equation}
                | \lambda |\| u \|=\| Au \|\leq \| A \|\| u \|.
            \end{equation}
            Donc \( | \lambda |\leq \| A \|\) et \( \rho(A)\leq\| A \|\).

        \item[Pour \( A\in \eM(n,\eR)\)]

            L'endroit qui coince dans le raisonnement fait pour \( \eM(n,\eC)\) est que certes \( A\in \eM(n,\eR)\) possède une plus grande valeur propre en module et qu'un vecteur propre lui est associé. Mais ce vecteur propre est a priori dans \( \eC^n\), et non dans \( \eR^n\). Nous pouvons donc écrire \( Au=\lambda u\), mais pas \( \| Au \|=| \lambda |\| u \|\) parce que nous ne savons pas quelle norme prendre sur \( \eC^n\).

            Il n'est pas certain que nous ayons une norme sur \( \eC^n\) qui se réduit sur \( \eR^n\) à celle choisie implicitement dans l'énoncé. Nous allons donc ruser un peu.

            Soit une norme \( N\) sur \( \eC^n\)\footnote{Il y en a plein, par exemple celle du produit scalaire \( \langle x, y\rangle =\sum_kx_k\bar y_k\).}. Nous nommons également \( N\) la norme subordonnée sur \( \eM(n,\eC)\) et la norme restreinte sur \( \eM(n,\eR)\). Vu que \( N\) est une norme sur \( \eM(n,\eR)\) et que ce dernier est de dimension finie, le théorème \ref{ThoNormesEquiv} nous indique que \( N\) est équivalente à \( \| . \|\). Il existe donc \( C>0\) tel que
            \begin{equation}        \label{EQooBNWMooNgnMxC}
                 N(B)\leq C\| B \|
            \end{equation}
            pour tout \( B\in \eM(n,\eR)\). Nous avons maintenant
            \begin{equation}
                \rho(A)^m\leq N(A^m)\leq C\| A^m \|\leq C\| A \|^m.
            \end{equation}
            Justifications :
            \begin{itemize}
                \item Par la proposition \ref{PROPooKLFKooSVnDzr}.
                \item Parce que \( A^m\in \eM(n,\eR)\) et la relation \eqref{EQooBNWMooNgnMxC}.
                \item Par itération du lemme \ref{LEMooFITMooBBBWGI}.
            \end{itemize}
            
            Nous avons donc \( \rho(A)\leq C^{1/m}\| A \|\) pour tout \( m\in\eN\). En prenant \( m\to \infty\) et en tenant compte de \( C^{1/m}\to 1\) nous trouvons \( \rho(A)\leq \| A \|\).
    \end{subproof}
\end{proof}

\begin{proposition}
    Si \( A\in \eM(n,\eR)\) alors \( \rho(A)^m=\rho(A^m)\) pour tout \( m\in \eN\).
\end{proposition}

\begin{proof}
    La matrice \( A\) peut être vue dans \( \eM(n,\eC)\) et nous pouvons lui appliquer le corollaire \ref{CORooTPDHooXazTuZ} :
    \begin{equation}        \label{EQooJJIYooDBacjn}
        \Spec(A^k)=\{ \lambda^k\tq \lambda\in\Spec(A) \}.
    \end{equation}
    À noter qu'il n'y a pas de magie : le spectre de la matrice réelle \( A\) est déjà défini en voyant \( A\) comme matrice complexe. Le spectre dont il est question dans \eqref{EQooJJIYooDBacjn} est bien celui dont on parle dans la définition du rayon spectral.

    Nous avons ensuite :
    \begin{subequations}
        \begin{align}
            \rho(A^k)&=\max\{ | \lambda |\tq \lambda\in\Spec(A^k) \}\\
            &=\max\{ | \lambda^k |\tq \lambda\in\Spec(A) \}\\
            &=\max\{ | \lambda |^k\tq\lambda\in\Spec(A) \}\\
            &=\rho(A)^k.
        \end{align}
    \end{subequations}
\end{proof}

\begin{proposition}[Bornée si et seulement si continue\cite{GKPYTMb}]       \label{PROPooQZYVooYJVlBd}
    Soient \( E\) et \( F\) des espaces vectoriels normés. Une application linéaire \( E\to F\) est bornée si et seulement si elle est continue.
\end{proposition}

\begin{proof}
    Nous commençons par supposer que \( A\) est bornée. Par le lemme \ref{LEMooFITMooBBBWGI}, pour tout \( x,y\in E\), nous avons
    \begin{equation}
        \| A(x)-A(y) \|=\| A(x-y) \|\leq \| A \|\| x-y \|.
    \end{equation}
    En particulier si \( x_n\stackrel{E}{\longrightarrow}x\) alors
    \begin{equation}
        0\leq \| A(x_n)-A(x) \|\leq \| A \|\| x-x_n \|\to 0
    \end{equation}
    et \( A\) est continue en vertu de la caractérisation séquentielle de la continuité, proposition \ref{PropFnContParSuite}.

    Supposons maintenant que \( \| A \|\) ne soit pas borné, c'est à dire que l'ensemble \( \{ \| A(x) \|\tq \| x \|=1 \}\) ne soit pas borné. Alors pour tout \( k\geq 1\) il existe \( x_k\in B(0,1)\) tel que \( \| A(x_k) \|>k\). La suite \( x_k/k\) tend vers zéro parce que \( \| x_k \|=1\), mais \( \| A(x_k) \|\geq 1\) pour tout \( k\). Cela montre que \( A\) n'est pas continue.
\end{proof}

\begin{definition}[\cite{ooAISYooXtUafT}]      \label{DEFooTLQUooJvknvi}
    Soient \( E\) et \( F\) deux espaces vectoriels normés.
    \begin{itemize}
        \item
            L'ensemble des applications linéaires \( E\to F\) est noté \( \aL(E,F)\).
        \item Un \defe{morphisme}{morphisme!espace vectoriel normé} est une application linéaire \( E\to F\) continue pour la topologie de la norme opérateur. Nous avons vu dans la proposition \ref{PROPooQZYVooYJVlBd} que la continuité était équivalente à être bornée. L'ensemble des morphismes est noté \( \cL(E,F)\)\nomenclature[B]{\( \cL(E,F)\)}{applications linéaires bornées (continues)}. 
        \item
            Un \defe{isomorphisme}{isomorphisme!espace vectoriel normé} est un morphisme continu inversible dont l'inverse est continu. Nous notons \( \GL(E,F)\) l'ensemble des isomorphismes entre \( E\) et \( F\).
    \end{itemize}
\end{definition}

Le point important de la définition \ref{DEFooTLQUooJvknvi} est la continuité. En dimension infine, la continuité n'est par exemple pas équivalente à l'inversibilité (penser à \( e_k\mapsto ke_k\)).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Application linéaire continue et bornée}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons vu dans la proposition \ref{PROPooQZYVooYJVlBd} que pour une application linéaire, être bornée est équivalent à être continue. Nous allons maintenant voir un certain nombre d'exemples illustrant ce fait.

\begin{example}[Une application linéaire non continue]  \label{ExHKsIelG}
    Soit \( V\) l'espace vectoriel normé des suites \emph{finies} de réels muni de la norme usuelle $\| c \|=\sqrt{\sum_{i=0}^{\infty}| c_i |^2}$ où la somme est finie. Nous nommons \( \{ e_k \}_{k\in \eN}\) la base usuelle de cet espace, et nous considérons l'opérateur \( f\colon V\to V\) donnée par \( f(e_k)=ke_k\). C'est évidemment linéaire, mais ce n'est pas continu en zéro. En effet la suite \( u_k=e_k/k\) converge vers \( 0\) alors que \( f(u_k)=e_k\) ne converge pas.
\end{example}

Cet exemple aurait pu également être donnée dans un espace de Hilbert, mais il aurait fallu parler de domaine.
%TODO : le faire, et regarder si Hilbet n'est pas la complétion de cet espace. Référencer à l'endroit qui définit l'espace vectoriel librement engendré. Ici ce serait par N.

%TODO : dire qu'une application bilinéaire sur RxR n'est pas une application linéaire sur R^2

\begin{example}[Une autre application linéaire non continue\cite{GTkeGni}]      \label{EXooDMVJooAJywMU}
    En dimension infinie, une application linéaire n'est pas toujours continue. Soit \( E\) l'espace des polynômes à coefficients réels sur \( \mathopen[ 0 , 1 \mathclose]\) muni de la norme uniforme. L'application de dérivation \( \varphi\colon E\to E\), \( \varphi(P)=P'\) n'est pas continue.

    Pour la voir nous considérons la suite \( P_n=\frac{1}{ n }X^n\). D'une part nous avons \( P_n\to 0\) dans \( E\) parce que \( P_n(x)=\frac{ x^n }{ n }\) avec \( x\in \mathopen[ 0 , 1 \mathclose]\). Mais en même temps nous avons \( \varphi(P_n)=X^{n-1}\) et donc \( \| \varphi(P_n) \|=1\).

    Nous n'avons donc pas \( \lim_{n\to \infty} \varphi(P_n)=\varphi(\lim_{n\to \infty} P_n)\) et l'application \( \varphi\) n'est pas continue en \( 0\). Elle n'est donc continue nulle part par linéarité.

    Nous avons utilisé le critère séquentiel de la continuité, voir la définition \ref{DefENioICV} et la proposition \ref{PropFnContParSuite}.
\end{example}

\begin{remark}  \label{RemOAXNooSMTDuN}
Cette proposition permet de retrouver l'exemple \ref{ExHKsIelG} plus simplement. Si \( \{ e_k \}_{k\in \eN}\) est une base d'un espace vectoriel normé formée de vecteurs de norme \( 1\), alors l'opérateur linéaire donné par \( u(e_k)=ke_k\) n'est pas borné et donc pas continu.
\end{remark}

C'est également ce résultat qui montre que le produit scalaire est continu sur un espace de Hilbert par exemple.

\begin{lemma}   \label{LemWWXVSae}
Soit \( F\) un espace de Banach et deux suites \( A_k\to A\) et \( B_k\to B\) dans \( \aL(F,F)\). Alors \( A_k\circ B_k\to A\circ B\) dans \( \aL(F,F)\), c'est à dire
\begin{equation}
    \lim_{n\to \infty} (A_kB_k)=\left( \lim_{n\to \infty} A_k \right)\left( \lim_{n\to \infty} B_k \right).
\end{equation}
\end{lemma}

\begin{proof}
    Il suffit d'écrire
    \begin{equation}
        \| A_kB_k-AB \|\leq \| A_kB_k-A_kB \|+\| A_kB-AB \|.
    \end{equation}
    Le premier terme tend vers zéro pour \( k\to\infty\) parce que 
    \begin{subequations}
        \begin{align}
            \| A_kB_k-A_kB \|&=\| A_k(B_k-B) \|\\
            &\leq \| A_k \|\| B_k-B \|\to \| A \|\cdot 0\\
            &=0
        \end{align}
    \end{subequations}
    où nous avons utilisé la propriété fondamentale de la norme opérateur : la proposition \ref{PROPooQZYVooYJVlBd}. Le second terme tend également vers zéro pour la même raison.
\end{proof}

\begin{proposition}[Distributivité de la somme infinie] \label{PropQXqEPuG}
    Soient \( E\) un espace normé, une suite \( (u_k)\) dans \( \GL(E)\) ainsi que \( a\in\GL(E)\). Pourvu que la série \( \sum_{n=0}^{\infty}u_k\) converge nous avons
    \begin{equation}
        \left( \sum_{k=0}^{\infty}u_k \right)a=\sum_{k=0}^{\infty}(u_ka).
    \end{equation}
\end{proposition}

\begin{proof}
    Par définition de la somme infinie,
    \begin{equation}
        \spadesuit=\left( \sum_{k=0}^{\infty}u_k \right)a=\left( \lim_{n\to \infty} \sum_{k=0}^nu_k \right)a.
    \end{equation}
    Le lemme \ref{LemWWXVSae} appliqué à \( n\mapsto\sum_{k=0}^nu_k\) et à la suite constante \( a\) nous donne
    \begin{equation}    \label{EqOAoopjz}
        \spadesuit=\lim_{n\to \infty} \left( \sum_{k=0}u_ka \right),
    \end{equation}
    ce que nous voulions par distributivité de la somme finie : dans \eqref{EqOAoopjz}, le \( a\) est dans ou hors de la somme, au choix. L'important est qu'il soit dans la limite.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Normes de matrices et d'applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecNomrApplLin}

\begin{theorem}[Norme matricielle et rayon spectral\cite{ooBCKVooVunKyT}]       \label{THOooNDQSooOUWQrK}
    La norme $2$ d'une matrice est liée au rayon spectral de la façon suivante :
    \begin{equation}
        \|A\|_2=\sqrt{\rho(A{^t}A)}
    \end{equation}
    ou plus généralement par \( \| A \|_2=\sqrt{\rho(A^*A)}\).
\end{theorem}

\begin{proposition} \label{PropMAQoKAg}
    La fonction
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eR)\times \eM(n,\eR)&\to \eR \\
            (X,Y)&\mapsto \tr(X^tY) 
        \end{aligned}
    \end{equation}
    est un produit scalaire sur \( \eM(n,\eR)\).
\end{proposition}
\index{trace!produit scalaire sur \( \eM(n,\eR)\)}
\index{produit!scalaire!sur \( \eM(n,\eR)\)}

\begin{proof}
    Il faut vérifier la définition \ref{DefVJIeTFj}.
    \begin{itemize}
        \item La bilinéairité est la linéarité de la trace.
        \item La symétrie de \( f\) est le fait que \( \tr(A^t)=\tr(A)\).
        \item L'application \( f\) est définie positive parce que si \( X\in \eM\), alors \( X^tX\) est symétrique définie positive, donc diagonalisable avec des nombres positifs sur la diagonale. La trace étant un invariant de similitude, nous avons \( f(X,X)=\tr(X^tX)\geq 0\). De plus si \( \tr(X^tX)=0\), alors \( X^tX=0\) (pour la même raison de diagonalisation). Mais alors \( \| Xu \|=0\) pour tout \( u\in E\), ce qui signifie que \( X=0\).
    \end{itemize}
\end{proof}

\begin{example}
	Soit $m=n$, un point $\lambda$ dans $\eR$ et $T_{\lambda}$ l'application linéaire définie par $T_{\lambda}(x)=\lambda x$. La norme de $T_{\lambda}$ est alors
\[
\|T_{\lambda}\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|\lambda x\|_{\eR^n}= |\lambda|.
\]
Notez que $T_{\lambda}$ n'est rien d'autre que l'homothétie de rapport $\lambda$ dans $\eR^m$.
\end{example}

\begin{example}
	Considérons la rotation $T_{\alpha}$ d'angle $\alpha$ dans $\eR^2$. Elle est donnée par l'équation matricielle
	\begin{equation}
		T_{\alpha}\begin{pmatrix}
			x	\\ 
			y	
		\end{pmatrix}=\begin{pmatrix}
			\cos\alpha	&	\sin\alpha	\\ 
			-\sin\alpha	&	\cos\alpha	
		\end{pmatrix}\begin{pmatrix}
			x	\\ 
			y	
		\end{pmatrix}=\begin{pmatrix}
			\cos(\alpha)x+\sin(\alpha)y	\\ 
			-\sin(\alpha)x+\cos(\alpha)y	
		\end{pmatrix}
	\end{equation}
	Étant donné que cela est une rotation, c'est une isométrie : $\| T_{\alpha}x \|=\| x \|$. En ce qui concerne la norme de $T_{\alpha}$ nous avons
	\begin{equation}
		\| T_{\alpha} \|=\sup_{x\in\eR^2}\frac{ \| T_{\alpha}(x) \| }{ \| x \| }=\sup_{x\in\eR^2}\frac{ \| x \| }{ \| x \| }=1.
	\end{equation}
	Toutes les rotations dans le plan ont donc une norme $1$. La même preuve tient pour toutes les rotations en dimension quelconque. 
\end{example}

%TODO : le théorème de fuite des compacts qui dit qu'une solution de y'=f(y,t) cesse d'exister seulement si elle tend vers +- infini.

\begin{example}
  Soit $m=n$, un point $b$ dans $\eR^m$ et $T_b$ l'application linéaire définie par $T_b(x)=b\cdot x$ (petit exercice : vérifiez qu'il s'agit vraiment d'une application linéaire).  La norme de $T_b$ satisfait les inégalités suivantes 
 \[
\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\leq \sup_{\|x\|_{\eR^m}\leq 1}\|b \|_{\eR^n}\|x\cdot x\|_{\eR^n}\leq\|b \|_{\eR^n},
\]
\[
\|T_b\|_{\mathcal{L}}=\sup_{\|x\|_{\eR^m}\leq 1}\|b\cdot x\|_{\eR^n}\geq \left\|b\cdot \frac{b}{\|b \|_{\eR^n}}\right\|_{\eR^n}=\|b \|_{\eR^n},
\]
donc $\|T_b\|_{\mathcal{L}}=\|b \|_{\eR^n}$.
\end{example}

\begin{proposition}
    Une application linéaire de \( \eR^m\) dans \( \eR^n\) est continue.
\end{proposition}

\begin{proof}
      Soit $x$ un point dans $\eR^m$. Nous devons vérifier l'égalité
      \begin{equation}
       \lim_{h\to 0_m}T(x+h)=T(x).
      \end{equation}
      Cela revient à prouver que $\lim_{h\to 0_m}T(h)=0$, parce que $T(x+h)=T(x)+T(h)$. Nous pouvons toujours majorer $\|T(h)\|_n$ par $\|T\|_{\mathcal{L}(\eR^m,\eR^n)}\| h \|_{\eR^m}$ (lemme \ref{LEMooIBLEooLJczmu}). Quand $h$ s'approche de $ 0_m $ sa norme $\|h\|_m$ tend vers $0$, ce que nous permet de conclure parce que nous savons que de toutes façons, $\| T \|_{\aL}$ est fini.
\end{proof}

Note : dans un espace de dimension infinie, la linéarité ne suffit pas pour avoir la continuité : il faut de plus être borné (ce que sont toutes les applications linéaires \( \eR^m\to\eR^n\)). Voir la proposition \ref{PROPooQZYVooYJVlBd}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Suites}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooLLUGooOwZRyI}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Limites, convergence}
%---------------------------------------------------------------------------------------------------------------------------

Nous disons qu'une suite réelle $(x_n)$ converge\footnote{Voir la définition \ref{PropLimiteSuiteNum} pour plus de détail.} vers $\ell$ lorsque pour tout $\varepsilon$, il existe un $M$ tel que
\begin{equation}
	n>N\Rightarrow | x_n-\ell |\leq\varepsilon.
\end{equation}
Le concept fondamental de cette définition est la notion de valeur absolue qui permet de donner la «distance» entre deux réels. Dans un espace vectoriel normé quelconque, cette notion est généralisée par la distance associée à la norme (définition \ref{DefEVNetDistance}). Nous pouvons donc facilement définir le concept de convergence d'une suite dans un espace vectoriel normé.

\begin{definition}		\label{DefCvSuiteEGVN}
	Soit une suite $(x_n)$ dans un espace vectoriel normé $V$. Nous disons qu'elle est
    \defe{convergente}{convergence!dans un espace vectoriel normé} s'il existe un élément $\ell\in V$ tel que
	\begin{equation}
		\forall \varepsilon>0,\,\exists N\in\eN\tq n\geq N\Rightarrow \| x_n-l \|<\varepsilon.
	\end{equation}
	Dans ce cas, $\ell$ est appelé la \defe{limite}{limite!suite} de la suite $(x_n)$.
\end{definition}


\begin{lemma}		\label{LemLimAbarA}
	Soit $(x_n)$ une suite convergente contenue dans un ensemble $A\subset V$. Alors la limite $x_n$ appartient à $\bar A$.
\end{lemma}

\begin{proof}
	Supposons que nous ayons une partie $A$ de $V$, et une suite $(x_n)$ dont la limite $\ell$ se trouve hors de $\bar A$. Dans ce cas, il existe un $r>0$ tel que\footnote{Une autre manière de dire la même chose : si $\ell\notin\bar A$, alors $d(\ell,A)>0$.} $B(\ell,r)\cap A=\emptyset$. Si tous les éléments $x_n$ de la suite sont dans $A$, il n'y en a donc aucun tel que $d(x_n,\ell)=\| x_n-\ell \|<r$. Cela contredit la notion de convergence $x_n\to \ell$.
\end{proof}

Nous avons déjà mentionné dans l'exemple \ref{ParlerEncoredeF} que zéro était un point adhérent à l'ensemble $F=\{ (-1)^n/n\tq n\in\eN_0 \}$. Nous savons maintenant que $0$ étant la limite de la suite, il est automatiquement adhérent à l'ensemble des éléments de la suite.

\begin{corollary}		\label{CorAdhEstLim}
	Soit $a$ un point de l'adhérence d'une partie $A$ de $V$. Alors il existe une suite d'éléments dans $A$ qui converge vers $a$.
\end{corollary}

\begin{proof}
	Si $a\in A$, alors nous pouvons prendre la suite constante $x_n=a$. Si $a$ n'est pas dans $A$, alors $a$ est dans $\partial A$, et pour tout $n$, il existe un point de $A$ dans la boule $B(a,\frac{1}{ n })$. Si nous nommons $x_n$ ce point, la suite ainsi construite est une suite contenue dans $A$ et qui converge vers $a$ (ce dernier point est laissé à la sagacité du lecteur ou de la lectrice).
\end{proof}

En termes savants, ce corollaire signifie que la fermeture $\bar A$ est composé de $A$ plus de toutes les limites de toutes les suites contenues dans $A$.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Critère de Cauchy}
%---------------------------------------------------------------------------------------------------------------------------


\begin{lemma}
    Une suite de Cauchy dans un espace vectoriel normé admettant une sous-suite convergente est elle-même convergente vers la même limite.
\end{lemma}

\begin{proof}
    Soit \( (a_n)\) une suite de Cauchy dans un espace vectoriel normé \( E\) et \( \ell\) la limite d'une sous-suite de \( (a_n)\). Soit \( \epsilon>0\) et \( N\in \eN\) tel que \( \| a_m-a_p \|<\epsilon\) dès que \( m,p\geq N\). Nous allons montrer que si \( k>N\) alors \( \| a_k-\ell \|<2\epsilon\). Pour cela nous considérons un \( n>N\) tel que \( \| a_n-\ell \|\leq \epsilon\) et nous calculons
    \begin{equation}
        \| a_k-\ell \|\leq \| a_k-a_n \|+\| a_n-\ell \|\leq 2\epsilon.
    \end{equation}
\end{proof}

\begin{theorem}[Critère de Cauchy]  \label{ThoHGyzAva}
    Une suite réelle est convergence si et seulement si elle est de Cauchy. En particulier \( \eR\) est un espace complet.
\end{theorem}
\index{critère!de Cauchy}
\index{complétude!de \( \eR\)}

\begin{proof}
    Soit \( (a_n)\) une suite de Cauchy dans \( \eR\).   
\end{proof}
%TODO : à faire, complétude de R.

\begin{definition}
    Nous disons que deux suites \( (u_n)\) et \( (v_n)\) sont \defe{équivalentes}{equivalence@équivalence!de suites} s'il existe une fonction \( \alpha\colon \eN\to \eR\) telle que
    \begin{enumerate}
        \item
            pour tout \( n\) à partir d'un certain rang, \( u_n=v_n\alpha(n)\)
        \item
            \( \alpha(n)\to 1\).
    \end{enumerate}
\end{definition}

\begin{lemma}
    Si les suites \( (u_n)\) et \( (v_n)\) sont équivalentes et si \( (v_n)\) admet une limite \( l\) différente de \( 1\), alors les suites \( (\ln u_n)\) et \( (\ln v_n)\) sont équivalentes.
\end{lemma}

\begin{proof}
    En effet si \( u_n=v_n\alpha(n)\) alors
    \begin{equation}
        \ln(u_n)=\ln(v_n)+\ln\big( \alpha(n) \big)=\ln(v_n)\left( 1+\frac{ \ln\big( \alpha(n) \big) }{ \ln(v_n) } \right),
    \end{equation}
    et comme \( \alpha(n)\to 1\), la parenthèse tend vers \( 1\).
\end{proof}

\begin{lemma}[Formule de Stirling\cite{MEHuVnb}]        \label{LemCEoBqrP}
    Nous avons l'équivalence de suites
    \begin{equation}
        n!\sim \left( \frac{ n }{ e } \right)^n\sqrt{2\pi n}.
    \end{equation}
\end{lemma}
\index{formule!Stirling}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Approximation}
%---------------------------------------------------------------------------------------------------------------------------

Le lemme suivant est surtout intéressant en dimension infinie.
\begin{lemma}
    Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\); il existe une suite \( (v_n)\) dans \( A\) telle que \( v_n\stackrel{V}{\longrightarrow}v\) et \( \| v_n \|\leq \| v \|\) pour tout \( n\).
\end{lemma}

\begin{proof}
    Vu que \( A\) est dense, il existe une suite \( a_n\) dans \( A\) telle que \( a_n\to v\). Ensuite il suffit de poser
    \begin{equation}
        v_n=\frac{ n }{ n+1 }\frac{ \| v \| }{ \| a_n \| }a_n.
    \end{equation}
    Par construction nous avons toujours
    \begin{equation}
        \| v_n \|=\frac{ n }{ n+1 }\| v \|\leq \| v \|.
    \end{equation}
    Et de plus, la norme étant continue\footnote{Où dans le calcul suivant nous utilisons la continuité de la norme ? Posez-vous la question.},
    \begin{equation}
        \lim_{n\to \infty} v_n=\lim_{n\to \infty} \frac{ n }{ n+1 }\lim_{n\to \infty} \frac{ \| v \| }{ \| v_n \| }\lim_{n\to \infty} v_n=v.
    \end{equation}

    Le fait que \( v_n\) soit dans \( A\) est dû au fait que \( A\) soit vectoriel.
\end{proof}

\begin{proposition}     \label{PROPooVEMGooYKhMFy}
    Soit un espace vectoriel normé \( V\) et un sous-espace vectoriel dense \( A\). Soit \( v\in V\); pour tout \( a\in \eR\) nous avons
    \begin{equation}
        \sup\{ | v\cdot a |\tq a\in A\text{ et }\| a \|\leq \lambda \}=\lambda\| v \|.
    \end{equation}
\end{proposition}

\begin{proof}
    D'abord pour tout \( a\in A\) vérifiant \( \| a \|\leq \lambda\) l'inégalité de Cauchy-Schwarz \ref{ThoAYfEHG} donne
    \begin{equation}
        | v\cdot a |\leq \| v \|\| a \|\leq \lambda\| v \|.
    \end{equation}
    Donc le supremum dont on parle est majoré par \( \lambda\| v \|\).

    Il nous faut l'inégalité dans l'autre sens. Par densité nous pouvons choisir une suite \( v_n\in A\) tel que \( v_n\to v\). Ensuite nous posons
    \begin{equation}
        a_n=\frac{ \lambda }{ \| v_n \| }v_n.
    \end{equation}
    Nous avons \( \| a_n \|=\lambda\) pour tout \( n\) et
    \begin{equation}
        | v\cdot a_n |=\frac{ \lambda }{ \| v_n \| }| v\cdot v_n |,
    \end{equation}
    et en passant à la limite,
    \begin{equation}
        \lim_{n\to \infty} | v\cdot a_n |=\frac{ \lambda }{ \| v \| }\| v\cdot v \|=\lambda\| v \|.
    \end{equation}
    Donc l'ensemble sur lequel nous prenons le supremum contient une suite convergente vers \( \lambda\| v \|\). Le supremum est donc au moins aussi grand que cela.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Séries}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooYCQBooSZNXhd}

\begin{definition}\label{DefGFHAaOL}
Soit \( (a_k)\) une suite dans un espace vectoriel normé \( (V,\| . \| )\). La \defe{série}{série!dans un espace vectoriel normé} associée, notée \( \sum_{k=0}^{\infty}a_k\) est la limite des \defe{sommes partielles}{somme!partielle}, c'est à dire la limite de la suite
    \begin{equation}
        s_k=\sum_{i=0}^ka_i
    \end{equation}
    si elle existe dans \( V\).

    Si une telle limite existe nous disons que \( \sum_{k=0}^{\infty}a_k\) \defe{converge}{convergence!série} dans \( V\). Si la limite de la suite des sommes partielles n'existe pas nous disons que la série \defe{diverge}{série!divergence}.
\end{definition}
\begin{remark}
    Si la limite de la suite des sommes partielles n'existe pas dans \( V\), alors elle peut parfois exister dans des extensions de \( V\). Par exemple une série de rationnels convergeant vers \( \sqrt{2}\) dans \( \eR\) ne converge pas dans \( \eQ\). Autre exemple : avec une bonne topologie sur \( \bar \eR\), une série peut ne pas converger dans \( \eR\) mais converger vers \( \pm\infty\) dans \( \bar \eR\).
\end{remark}

Dans le cas des espaces de fonctions, nous avons une norme importante : la norme uniforme définie par \( \| f \|_{\infty}=\sup\{ f(x) \}\) où le supremum est pris sur l'ensemble de définition de \( f\).
\begin{definition}[Convergence absolue] \label{DefVFUIXwU}
    Nous disons que la série \( \sum_{n=0}^{\infty}a_n\) dans l'espace vectoriel normé \( V\) \defe{converge absolument}{convergence!absolue} si la série \( \sum_{n=0}^{\infty}\| a_n \|\) converge dans \( \eR\).
\end{definition}

Dans le cas de l'espace des fonctions muni de la norme uniforme, nous parlons de convergence normale.

\begin{definition}[Convergence normale] \label{DefVBrJUxo}
    Une série de fonctions \( \sum_{n\in \eN}u_n \) converge \defe{normalement}{convergence!normale} si la série de nombres \( \sum_n\| u_n \|_{\infty}\) converge. C'est à dire si la série converge absolument pour la norme \( \| f \|_{\infty}\).
\end{definition}
Cela n'est rien d'autre que la convergence absolue dans l'espace des fonctions muni de la norme uniforme.

La convergence normale est à ne pas confondre avec la convergence uniforme. La somme \( \sum_nf_n\) \defe{converge uniformément}{convergence!uniforme!série de fonctions} vers la fonction \( F\) si la suite des sommes partielles converge uniformément, c'est à dire si 
\begin{equation}        \label{EqLNCJooVCTiIw}
    \lim_{N\to \infty} \| \sum_{n=1}^Nf_n-F \|_{\infty}=0.
\end{equation}

\begin{proposition} \label{PropAKCusNM}
    Une série convergeant absolument dans un espace de Banach\footnote{Un espace vectoriel normé complet. Typiquement \( \eR\).} y converge au sens usuel.
\end{proposition}

\begin{proof}
    Soit \( (a_k)\) une suite dans un espace vectoriel normé complet dont la série converge absolument. Nous allons montrer que la suite des sommes partielles est de Cauchy. Cela suffira à montrer sa convergence par hypothèse de complétude.

    Nous avons
    \begin{equation}
        \| s_p-s_l \|=\| \sum_{k=l+1}^{p}a_k\|  \leq\sum_{k=l+1}^p\| a_k \|=\bar s_p-\bar s_l
    \end{equation}
    où \( \bar s_n=\sum_{k=0}^n \| a_k \|\) est la suite des sommes partielles de la série des normes (qui converge). Vu que la suite \( (\bar s_n)\) converge dans \( \eR\), elle y es de Cauchy par le théorème \ref{ThoHGyzAva}. Donc il existe un \( N\) tel que \( p,l>N\) implique
    \begin{equation}
        \| s_p-s_l \|=\bar s_p-\bar s_l\leq \epsilon.
    \end{equation}
    Cela signifie que \( (s_n)\) est une suite de Cauchy et donc convergente.
\end{proof}

\begin{remark}
    Nous savons que sur les espaces vectoriels de dimension finie toutes les normes sont équivalentes (théorème \ref{DefEquivNorm}). La notion de convergence de série ne dépend alors pas du choix de la norme. Il n'en est pas de même sur les espaces de dimension infinie. Une série peut converger pour une norme mais pas pour une autre.
\end{remark}
Lorsque nous verrons la convergence de séries, nous verrons que la convergence normale est la convergence absolue pour la norme uniforme.

\begin{lemma}       \label{LemCAIPooPMNbXg}
    Si \( E\) et \( F\) sont des espaces de Banach\quext{Je crois qu'il ne faut pas que \( E\) soit complet.}, l'espace \( \aL(E,F)\) est également de Banach.
\end{lemma}

\begin{proof}
    Soit \( (u_n)\) une suite de Cauchy dans \( \aL(E,F)\); si \( x\in E\) il existe \( N\) tel que si \( l,m>N\) alors \( \| a_l-a_m \|<\epsilon\), c'est à dire que pour tout \( \| x \|=1\) on a \( \| u_l(x)-u_n(x) \|<\epsilon\). Cela signifie que \( u_n(x)\) est une suite de Cauchy dans l'espace complet \( F\). Cette suite converge et nous pouvons définir l'application \( u\colon E\to F\) par
    \begin{equation}
        u(x)=\lim_{n\to \infty} u_n(x).
    \end{equation}
    Il suffit maintenant de prouver que \( u\) est linéaire, ce qui est une conséquence directe de la linéarité de la limite :
    \begin{equation}
        u(\alpha x+\beta y)=\lim_{n\to \infty} \big( \alpha u_n(x)+\beta u_n(y) \big).
    \end{equation}
\end{proof}

Le lemme \ref{LemPQFDooGUPBvF} donne une version plus simple de la proposition suivante.
\begin{proposition}     \label{PropQAjqUNp}
    Soit \( E\) un espace de Banach (espace vectoriel normé complet). Si \( A\in\aL(E,E)\) avec \( \| A \|<1\) pour la norme opérateur, alors \( (\mtu-A)\) est inversible et son inverse est donné par
    \begin{equation}
        (\mtu-A)^{-1}=\sum_{k=0}^{\infty}A^k.
    \end{equation}
    Le résultat tient aussi si \( A\) est nilpotente, même si sa norme n'est pas majorée par \( 1\).
\end{proposition}
\index{série!donnant \( (1-A)^{-1}\)}

\begin{proof}
    Étant donné que la norme opérateur est une norme algébrique (lemme \ref{LEMooFITMooBBBWGI}), nous avons \( \| A^k \|\leq \| A \|^k\). Par conséquent la série \( \| A^k \|\) est majorée par la série géométrique qui converge. Par conséquent \( \sum_{k}A^k\) est une série absolument convergence et donc convergente par la proposition \ref{PropAKCusNM} et le fait que \( \aL(E)\) est complet (proposition \ref{LemCAIPooPMNbXg}).
    
    Montrons à présent que la somme est l'inverse de \( \mtu-A\) en utilisant le produit terme à terme autorisé par la proposition \ref{PropQXqEPuG} :
    \begin{equation}
        \sum_{k=0}^nA^k(\mtu-A)=\sum_{k=0}^n(A^k-A^{k+1})=\mtu-A^{n+1}.
    \end{equation}
    Par conséquent 
    \begin{equation}
        \| \mtu-\sum_{k=0}^nA^k(\mtu-A) \|=\| A^{n+1} \|\leq \| A \|^{n+1}\to 0.
    \end{equation}

    Si \( A\) est nilpotente, la convergence de \( \sum_{k=0}^{\infty}A^k\) ne pose pas de problèmes parce que la somme est finie. Le fait que cette somme soit \( (\mtu-A)^{-1}\) s'obtient de la même façon, mais il ne faut pas faire la dernière majoration : si \( A\) est nilpotente, il tombe sous le sens que \( \| A^{n+1} \|\to 0\), mais il n'est cependant pas vrai que \( \| A \|^{n+1}\) tende vers zéro.
\end{proof}

\begin{proposition}\label{propnseries_propdebase}
Les principales propriétés de la somme définie par la limite \eqref{DefGFHAaOL} sont
  \begin{enumerate}
  \item Si une série converge absolument, alors elle converge simplement.
  \item Si la série est à termes positifs --c'est-à-dire pour tout indice $k$, $a_k \in \eR$ et $a_k \geq 0$-- il n'y a aucune différence entre convergence absolue et convergence simple.
  \item\label{point3-seriepropdebase} Si une série converge, son terme général doit tendre vers $0$.
\item 
Si la série converge alors la somme est associative
\item
Si la série converge absolument, alors la somme est commutative.
  \end{enumerate}
\end{proposition}

%TODO : la preuve des autres points
\begin{proof}
    
    \begin{enumerate}
        \item
            
  \item
  \item 
  \item
\item 
Associativité. Supposons que \( \sum_ka_k\) et \( \sum_kb_k\) convergent tous deux. Alors nous avons pour tout \( N\) :
\begin{equation}
    \sum_{k=0}^N(a_k+b_k)=\sum_{k=0}^Na_k+\sum_{k=0}^Nb_k.
\end{equation}
Mais si deux limites existent alors la somme commute avec la limite. C'est le cas pour la limite \( N\to \infty\), donc
\begin{equation}
    \lim_{N\to \infty} \sum_{k=1}^{\infty}(a_k+b_k)=\lim_{N\to \infty} \sum_{k=0}^{\infty}a_k+\lim_{N\to \infty} \sum_{k=0}^{\infty}b_k.
\end{equation}
\item
    \end{enumerate}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Série réelle}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{secseries}

La notion de série formalise le concept de somme infinie. L'absence de certaines propriétés de ces objets (problèmes de commutativité et même d'associativité) incitent à la prudence et montrent à quel point une définition précise est importante. 


\subsection{Critères de convergence absolue}

Étant donné le terme général d'une série, il est souvent --dans les cas qui nous intéressent-- difficile de déterminer la somme de la série. L'exemple de la série géométrique est particulier, puisqu'on connait une formule pour chaque somme partielle, mais pour l'exemple des séries de Riemann il n'y a aucune formule simple pour un $\alpha$ général. D'où l'intérêt d'avoir des critères de convergence ne nécessitant aucune connaissance de l'éventuelle limite de la série.

\begin{lemma}[Critère de comparaison]   \label{LemgHWyfG}
Soient $\sum_i a_i$ et $\sum_j
b_j$ deux séries à termes positifs vérifiant
\begin{equation*}
  0 \leq a_i \leq b_i
\end{equation*}
alors
\begin{enumerate}
\item si $\sum_i a_i$ diverge, alors $\sum_j b_j$ diverge,
\item si $\sum_j b_j$ converge, alors $\sum_i a_i$ converge
  (absolument).
  \end{enumerate}
\end{lemma}


\begin{proposition}[Critère d'équivalence\cite{TrenchRealAnalisys}]
 Soient $\sum_i a_i$ et $\sum_j b_j$ deux séries à termes positifs. Supposons l'existence de la limite (éventuellement infinie) suivante
\begin{equation}
  \limite i \infty \frac{a_i}{b_i} = \alpha 
\end{equation}
avec \( \alpha\in \eR\cup\{ +\infty \}\). Alors
\begin{enumerate}
\item si $\alpha \neq 0$ et $\alpha\neq \infty$, alors
  \begin{equation}
    \sum_i a_i \text{~converge} \ssi \sum_j b_j\text{~converge,}
  \end{equation}
\item si $\alpha = 0$ et $\sum_j b_j$ converge, alors $\sum_i a_i$
  converge (absolument),
\item si $\alpha = +\infty$ et $\sum_j b_j$ diverge, alors $\sum_i
  a_i$ diverge.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}
    \item
        Le fait que la suite $a_n/b_n$ converge vers $\alpha$ signifie que tant sa limite supérieure que sa limite inférieure convergent vers $\alpha$. En particulier la suite $\frac{ a_n }{ b_n }$ est bornée vers le haut et vers le bas. À partir d'un certain rang $N$, il existe $M$ tel que 
        \begin{equation}
            \frac{ a_n }{ b_n }<M
        \end{equation}
        et il existe $m$ tel que
        \begin{equation}
            \frac{ a_n }{ b_n }>m.
        \end{equation}
        Nous avons donc $a_n<Mb_n$ et $a_n>mb_n$. La série de $(a_n)$ converge donc si et seulement si la série de $(b_n)$ converge.
    \item
        Si $\alpha=0$, cela signifie que pour tout $\epsilon$, il existe un rang tel que $\frac{ a_n }{ b_n }<\epsilon$, et donc tel que $a_n<\epsilon b_k$. La suite de $(a_i)$ converge donc dès que la suite de $(b_i)$ converge.
    \item
        Pour tout $M$, il existe un rang dans la suite à partir duquel on a $\frac{ a_i }{ b_i }>M$, et donc $a_k>Mb_k$. Si la série de $(b_k)$ diverge, la série de $(a_k)$ doit également diverger.
\end{enumerate}
\end{proof}


\begin{proposition}[Critère du quotient\cite{KeislerElemCalculus}]     \label{PropOXKUooQmAaJX}
    Soit $\sum_i a_i$ une série. Supposons l'existence de la limite (éventuellement infinie) suivante
    \begin{equation}
      \limite i \infty \abs{\frac{a_{i+1}}{a_i}} = L
    \end{equation}
    avec \( L\in \eR\cup\{ +\infty \}\).  Alors
    \begin{enumerate}
    \item si $L < 1$, la série converge absolument,
    \item si $L > 1$, la série diverge,
    \item si $L = 1$ le critère échoue : il existe des exemple de convergence et des exemples de divergence.
    \end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}
    \item
        Soit $b$ tel que $L<b<1$. À partir d'un certain rang $K$, on a $\left| \frac{ a_{i+1} }{ a_i } \right| <b$. En particulier,
        \begin{equation}
            | a_{K+1} |<b| a_K |,
        \end{equation}
        et pour $a_{K+2}$ nous avons
        \begin{equation}
            | a_{K+2} |<b| a_{K+1} |<b^2| a_K |.
        \end{equation}
        Au final,
        \begin{equation}
            | a_{K+n} |<b^n| a_K |.
        \end{equation}
        Étant donné que la série $\sum_{n\geq K}b^n$ converge (parce que $b<1$), la queue de suite $\sum_{i\geq K}a_i$ converge, et par conséquent la suite au complet converge.
    \item
        Si $L>1$, on a
        \begin{equation}
            | a_K |<| a_{K+1} |<| a_{K+2} |<\ldots
        \end{equation}
        Il est donc impossible que la suite $(a_i)$ converge vers zéro. La série ne peut donc pas converger.
    \item
        Par exemple la suite harmonique $a_n=\frac{1}{ n }$ vérifie $L=1$, mais la série ne converge pas. Par contre, la suite $a_n=\frac{ 1 }{ n^2 }$ vérifie aussi le critère avec $L=1$ tandis que la série $\sum_n\frac{1}{ n^2 }$ converge.
\end{enumerate}
\end{proof}


\begin{proposition}[Critère de la racine\cite{TrenchRealAnalisys}]
    Soit $\sum_i a_i$ une série, et considérons
    \begin{equation*}
      \limsup_{i \rightarrow \infty} \sqrt[i]{\abs{a_i}} = L 
    \end{equation*}
    avec \( L\in \eR\cup\{ +\infty \}\). Alors
    \begin{enumerate}
    \item si $L < 1$, la série converge absolument,
    \item si $L> 1$, la série diverge,
    \item si $L = 1$ le critère échoue.
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Si $L<1$, il existe un $r\in \mathopen] 0 , 1 \mathclose[$ tel que $| a_n |^{1/n}<r$ pour les grands $n$. Dans ce cas, $| a_n |<r^{n}$, et la série converge absolument parce que la série $\sum_nr^n$ converge du fait que $r<1$.
        \item
            Si $L>1$, il existe un $r>1$ tel que $| a_n |^{1/n}>r>1$. Cela fait que $| a_n |$ prend des valeurs plus grandes que $n$ pour une infinité de termes. Le terme général $a_n$ ne peut donc pas être une suite convergente. Par conséquent la suite diverge au sens où elle ne converge pas.

    \end{enumerate}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Critères de convergence simple}
%---------------------------------------------------------------------------------------------------------------------------

Les critères de comparaison, d'équivalence, du quotient et de la racine sont des critères de convergence absolue. Pour conclure à une convergence simple qui n'est pas une convergence absolue, le critère d'Abel sera notre outil principal.  

\subsubsection{Critère d'Abel}

\begin{proposition}[Critère d'Abel]
    Soit la série $\sum_i c_iz_i$ avec
    \begin{enumerate}
        \item $(c_i)$ est une suite réelle décroissante qui tend vers zéro,
        \item $(z_i)$ est une suite dans $\eC$ dont la suite des sommes partielles est bornée dans $\eC$, c'est à dire qu'il existe un $M>0$ tel que pour tout $n$,
        \begin{equation}
            \left| \sum_{i=1}^nz_i \right| \leq M.
        \end{equation}
        Alors la série $\sum_ic_iz_i$ est convergente.
    \end{enumerate}
\end{proposition}
Remarquons que ce critère ne donne pas de convergence absolue.

\begin{corollary}[Critère des séries alternées]\index{critère!série alternée}       \label{CoreMjIfw}
    Si \( (a_n)\) est une suite décroissante à limite nulle, alors la série
  \begin{equation}
    \sum_{n=0}^\infty {(-1)}^n a_n
  \end{equation}
  converge simplement.
\end{corollary}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Quelques séries usuelles}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooDTYHooZjXXJf}

\begin{example}[Série harmonique]
    La \defe{série harmonique}{série!harmonique} est
    \begin{equation}
        \sum_{i=1}^\infty \frac1i
    \end{equation}
    et diverge (possède une limite $+\infty$).
\end{example}

\begin{example}[Série géométrique] \label{ExZMhWtJS}
    La \defe{série géométrique}{série!géométrique} de raison $q \in \eC$ est
    \begin{equation}    \label{EqZQTGooIWEFxL}
        \sum_{i=0}^\infty q^i.
    \end{equation}
    Étudions la somme partielle \( S_N=1+q+q^2+\cdots +q^{n}\). Nous avons évidemment $S_N-qS_N=1-q^{N+1}$ et donc
    \begin{equation}    \label{EqASYTiCK}
        S_N=\sum_{n=0}^Nq^n=\frac{ 1-q^{N+1} }{ 1-q }.
    \end{equation}
    La limite \( \lim_{N\to \infty} S_N\) existe si et seulement si \( | q |\leq 1\) et dans ce cas nous avons
    \begin{equation}    \label{EqRGkBhrX}
        \sum_{n=0}^{\infty}q^n=\frac{ 1 }{ 1-q }.
    \end{equation}
    La convergence est absolue.

    Si la somme commence en \( n=1\) au lieu de \( n=0\) alors
    \begin{equation}        \label{EqPZOWooMdSRvY}
        \sum_{n=1}^{\infty}q^n=\frac{1}{ 1-q }-1=\frac{ q }{ 1-q }.
    \end{equation}
\end{example}

Un cas particulier de la formule \eqref{EqASYTiCK} est le calcul de \( \sum_{j=1}^{N}q^{-j}\) bien utile lorsque l'on joue avec des nombres binaires (voir l'exemple \ref{EXEMooRHENooGwumoA}). Nous avons
\begin{equation}        \label{EQooFMBAooEJkHWT}
    \sum_{j=1}^Nq^{-j}=\sum_{j=0}^Nq^{-j}-1=\frac{ 1-q^{-N} }{ q-1 }.
\end{equation}

\begin{example}[Série de Riemann]       \label{EXooCTYNooCjYQvJ}
    Pour $\alpha \in \eR$, la \defe{série de Riemann}{série!Riemann}
    \begin{equation}        \label{EqSerRiem}
        \sum_{i=1}^\infty \frac1{i^\alpha}
    \end{equation}
    converge (absolument, puisque réelle et positive) si et seulement si $\alpha > 1$, et diverge sinon.
\end{example}

\begin{example}[Série exponentielle] \label{ExIJMHooOEUKfj}
    La série exponentielle est la série (pour \( t\in \eR\))
    \begin{equation}
        \exp(t)=\sum_{k=0}^{\infty}\frac{ t^k }{ k! }.
    \end{equation}
    Nous montrons qu'elle converge pour tout \( t\in \eR\). Si \( a_k=t^k/k!\) alors \( \frac{ a_{k+1} }{ a_k }=\frac{ t }{ k }\) dont la limite \( k\to \infty\) est zéro (quel que soit \( t\)). En vertu du critère du quotient \ref{PropOXKUooQmAaJX} la série exponentielle converge (absolument) pour tout \( t\in \eR\).

    Toutes les propriétés de la fonction de \( t\) ainsi définie sont dans le théorème \ref{ThoRWOZooYJOGgR}.
\end{example}
\index{exponentielle!convergence}

\begin{example}[Série arithméticogémétrique\cite{QXuqdoo}]
    La \defe{suite arithméticogémétrique}{suite!arithméticogéométrique} est une suite de la forme \( u_{n+1}=au_n+b\) avec \( a\) et \( b\) non nuls. Si elle possède une limite, cette dernière doit résoudre \( l=al+n\), et donc être égale à 
    \begin{equation}
        r=\frac{ b }{ 1-a }.
    \end{equation}
    Il n'est pas très compliqué de trouver le terme général de la suite en fonction de \( a\) et de \( b\). Il suffit de considérer la suite \( v_n=u_n-r\), et de remarquer que cette suite est géométrique :
    \begin{equation}
        v_{n+1}=av_n.
    \end{equation}
    Par conséquent \( v_n=a^nv_0\), ce qui donne pour la suite \( (u_n)\) la formule
    \begin{equation}
        u_n=a^n(u_0-r)+r.
    \end{equation}
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Moyenne de Cesaro}
%---------------------------------------------------------------------------------------------------------------------------

Si \( (a_n)_{n\in \eN} \) est une suite dans \( \eR\) ou \( \eC\), alors sa \defe{moyenne de Cesaro}{moyenne!de Cesaro}\index{Cesaro!moyenn} est la limite (si elle existe) de la suite
\begin{equation}
    c_n=\frac{1}{ n }\sum_{k=1}^na_k.
\end{equation}
En un mot, c'est la limite des moyennes partielles.

\begin{lemma}       \label{LemyGjMqM}
    Si la suite \( (a_n)\) converge vers la limite \( \ell\) alors la suite admet une moyenne de Cesaro qui vaudra \( \ell\).
\end{lemma}

\begin{proof}
    Soit \( \epsilon>0\) et \( N\in \eN\) tel que \( | a_n-\ell |<\epsilon\) pour tout \( n>N\). En remarquant que
    \begin{equation}
        \frac{1}{ n }\sum_{k=1}^nk-\ell=\frac{1}{ n }\sum_{k=1}^n(a_k-\ell),
    \end{equation}
    nous avons
    \begin{subequations}
        \begin{align}
            | \frac{1}{ n }\sum_{k=1}^na_k-\ell |&\leq| \frac{1}{ n }\sum_{k=1}^N| a_k-\ell | |+\big| \frac{1}{ n }\sum_{k=N+1}^n\underbrace{| a_k-\ell |}_{\leq \epsilon} \big|\\
            &\leq \epsilon+\frac{ n-N-1 }{ n }\epsilon\\
            &\leq 2\epsilon.
        \end{align}
    \end{subequations}
    Dans ce calcul nous avons redéfinit \( N\) de telle sorte que le premier terme soit inférieur à \( \epsilon\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Écriture décimale d'un nombre}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( b\geq 2\) un entier qui sera la base dans laquelle nous allons écrire les nombres. Nous considérons l'ensemble \( \eD_b\)\nomenclature[Y]{\( \eD_b\)}{l'ensemble de écritures décimales en base \( b\)} des suites dans \( \{ 0,1,\ldots, b-1 \}\) qui n'ont pas une queue de suite uniquement formée de \( b-1\). Autrement dit une suite \( (c_n)\) est dans \( \eD_b\) lorsque pour tout \( N\), il existe \( k>N\) tel que \( c_k\neq b-1\). Associé à cet ensemble nous considérons la fonction
\begin{equation}    \label{EqXXXooOTsCK}
    \begin{aligned}
        \varphi_b\colon \eD_b&\to \mathopen[ 0 , 1 [ \\
            c&\mapsto \sum_{n=1}^{\infty}\frac{ c_n }{ b^n }. 
    \end{aligned}
\end{equation}

\begin{lemma}
    La fonction \( \varphi_b\) est bien définie au sens où elle converge et prend ses valeurs dans \( \mathopen[ 0 , 1 [\).
\end{lemma}
    
\begin{proof}
    Tout se base sur la somme de la série géométrique \eqref{EqRGkBhrX} sous la forme
    \begin{equation}    \label{EqWZGooXJgwl}
        \sum_{k=0}^{\infty}\frac{1}{ b^k }=\frac{ b }{ b-1 }.
    \end{equation}
    La somme \eqref{EqXXXooOTsCK} est donc majorée par \( \sum_n\frac{ b-1 }{ b^n }\) qui converge.

    Pour prouver que l'image de \( \varphi_b\) est bien \( \mathopen[ 0 , 1 [\), nous savons qu'au moins un des \( c_n\) (en fait une infinité) est plus petit que \( b-1\), donc nous avons la majoration stricte\footnote{Notez que la somme \eqref{EqXXXooOTsCK} commence à un tandis que la série géométrique \eqref{EqWZGooXJgwl} commence à zéro.}
        \begin{equation}
            \varphi_b(c)<\sum_{n=1}^{\infty}\frac{ b-1 }{ b^n }=(b-1)\left( \sum_{n=1}^{\infty}\frac{1}{ b^n }-1 \right)=1
        \end{equation}
\end{proof}

Le fait d'introduire l'ensemble \( \eD\) au lieu de l'ensemble de toutes les suites est justifié par la proposition suivante. Elle explique pourquoi un nombre possède au maximum deux écritures décimales distinctes et que ces deux sont obligatoirement de la forme, par exemple en base \( 10\) :
\begin{equation}
    0.34599999999\ldots=0.34600000\ldots
\end{equation}
mais qu'un nombre commençant par \( 0.347\) ne peut pas être égal. C'est pour cela que dans la définition de \( \eD_b\) nous avons exclu les suites qui terminent par tout des \( b-1\).
\begin{proposition} \label{PropSAOoofRlQR}
    Soit la fonction
    \begin{equation}
        \begin{aligned}
            \varphi\colon \{ 0,\ldots, b-1 \}^{\eN}&\to \mathopen[ 0 , 1 [ \\
                x&\mapsto \sum_{n=1}^{\infty}\frac{ x_n }{ b^n }. 
        \end{aligned}
    \end{equation}
    Si \( \varphi(x)=\varphi(y)\) et si \( n_0\) est le plus petit entier tel que \( x_{n_0}\neq y_{n_0}\) alors soit
    \begin{equation}
        x_{n_0}-y_{n_0}=1
    \end{equation}
    et \( x_n=0\), \( y_n=b-1\) pour tout \( n>n_0\), soit le contraire : \( y_{n_0}-x_{n_0}=1\) avec \( y_n=0\) et \( x_n=b-1\) pour tout \( n>n_0\).
\end{proposition}

\begin{proof}
    Nous nous basons sur la formule (facilement dérivable depuis \eqref{EqWZGooXJgwl}) suivante :
    \begin{equation}
        \sum_{k=n_0+1}^{\infty}\frac{1}{ b^k }=\frac{1}{ b^{n_0+1} }\frac{ b }{ b-1 }.
    \end{equation}
    Nous avons 
    \begin{equation}
        0=\varphi(x)-\varphi(y)=\frac{ x_{n_0}-y_{n_0} }{ b^{n_0} }+\sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n }\geq \frac{ x_{n_0}-y_{n_0} }{ b^{n_0} }-\sum_{n=n_0+1}^{\infty}\frac{ b-1 }{ b^n }=\frac{ x_{n_0}-y_{n_0}-1 }{ b^{n_0} }.
    \end{equation}
    Le dernier terme étant manifestement positif\footnote{C'est ici qu'intervient la subdivision entre le cas \( x_{n_0}-y_{n_0}=1\) ou le contraire. En effet si «ce dernier terme était manifestement \emph{négatif}», il aurait fallu majorer avec de \( 1-b\) au lieu de \( 1-b\).}, il est nul et nous avons \( x_{n_0}-y_{n_0}=1\).

    Nous avons donc maintenant
    \begin{equation}    \label{EqHWQoottPnb}
        0=\varphi(x)-\varphi(y)=\frac{1}{ b^{n_0} }+\sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n }.
    \end{equation}
    Nous majorons la dernière somme de la façon suivante, en supposant que \( | x_n-y_n |\neq b-1\) pour un certain \( n>n_0\) :
    \begin{equation}
        \left| \sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n } \right| \leq\sum_{n=n_0+1}^{\infty}\frac{ | x_n-y_n | }{ b^n }<\sum_{n=n_0+1}^{\infty}\frac{ b-1 }{ b^n }=\frac{1}{ b^{n_0} }.
    \end{equation}
    Étant donné cette inégalité stricte, l'équation \eqref{EqHWQoottPnb} ne peut pas être correcte (valoir zéro). Nous avons donc \( | x_n-b_n |=b-1\) pour tout \( n>n_0\). Donc pour chaque \( n>n_0\) nous avons soit \( x_n=0\) et \( y_n=b-1\), soit \( a_n=b-1\) et \( b_n=0\). Pour conclure il faut encore prouver que le choix doit être le même pour tout \( n\).

    Nous nous mettons dans le cas \( x_{n_0}-y_{n_0}=1\); dans ce cas nous avons bien l'égalité \eqref{EqHWQoottPnb} sans petites nuances de signes. Nous écrivons
    \begin{equation}
        \sum_{n=n_0+1}^{\infty}\frac{ x_n-y_n }{ b^n }=(b-1)\sum_{n=n_0+1}^{\infty}\frac{ (-1)^{s_n} }{ b^n }
    \end{equation}
    où \( s_n\) est pair ou impair suivant que \( x_n=0\), \( y_n=b-1\) ou le contraire. Si un des \( (-1)^{s_n}\) est pas \( -1\) alors nous avons l'inégalité stricte
    \begin{equation}
        (b-1)\sum_{n=n_0+1}^{\infty}\frac{ (-1)^{s_n} }{ b^n }>(b-1)\sum_{n=n_0+1}^{\infty}\frac{-1}{ b^n }=-\frac{1}{ b^{n_0} }.
    \end{equation}
    Dans ce cas il est impossible d'avoir \( \varphi(x)-\varphi(y)=0\). Nous en concluons que \( (-1)^{s_n}\) est toujours \( -1\), c'est à dire \( x_n-y_n=1-b\), ce qui laisse comme seule possibilité \( x_n=0\) et \( y_n=b-1\).
\end{proof}

\begin{theorem} \label{ThoRXBootpUpd}
    L'application \( \varphi_b\colon \eD_b\to \mathopen[ 0 , 1 [\) est bijective.
\end{theorem}

\begin{proof}
    En ce qui concerne l'injection, nous savons de la proposition \ref{PropSAOoofRlQR} que si \( \varphi_b(x)=\varphi_b(y)\) pour \( x,y\in\{ 0,\ldots, b-1 \}^{\eN}\), alors soit \( x\) soit \( y\) a une queue de suite composée uniquement de \( b-1\), ce qui est exclu dans \( \eD_b\). Nous en déduisons que \( \varphi_b\) est bien injective en prenant \( \eD_b\) comme ensemble départ.

    La partie lourde est la surjectivité. Nous prenons \( x\in \mathopen[ 0 , 1 [\) et nous allons construire par récurrence une suite \( a\in \eD_b\) telle que \( \varphi_b(a)=x\). Si il existe \( a_1\in\{ 0,\ldots, b-1 \}\) tel que \( x=a_1/b\) alors nous prenons la suite \( (a_1,0,\ldots, )\) et nous avons évidemment \( \varphi(a)=x\). Sinon il existe \( a_1\in\{ 0,\ldots, b-1 \}\) tel que
        \begin{equation}
            \frac{ a_1 }{ b }<x<\frac{ a_1+1 }{ b }
        \end{equation}
        parce que les autres possibilités pour \( x\) sont dans l'ensemble \( \mathopen[ 0 , 1 \mathclose[\setminus\{ \frac{ k }{ b } \}_{k=0,\ldots, b-1}\) que nous subdivisons en
        \begin{equation}
        \mathopen] 0 , \frac{1}{ b } \mathclose[\cup\mathopen] \frac{1}{ b } , \frac{ 2 }{ b } \mathclose[\cup\ldots\cup\mathopen] \frac{ b-1 }{ b } , 1 \mathclose[.
        \end{equation}
        Pour la récurrence nous supposons avoir trouvé \( a_1,\ldots, a_n\) tels que
        \begin{equation}
            \sum_{k=1}^n\frac{ a_k }{ b^k }< x<\sum_{k=1}^{n-1}\frac{ a_k }{ b^k }+\frac{ a_n+1 }{ b^n }.
        \end{equation}
    Encore une fois s'il existe \( a_{n+1}\in\{ 0,\ldots, b-1 \}\) tel que \( \sum_{k=1}^{n+1}\frac{ a_k }{ b^k }=x\) alors nous prenons ce \( a_{n+1}\) et nous complétons la suite avec des zéros pour avoir \( \varphi(a)=x\). Sinon 
%nous subdivisions l'intervalle \( \mathopen]  \frac{ a_n }{ b^n }, \frac{ a_n }{ b^n }+\frac{ a_n+1 }{ b^n } \mathclose[\) (auquel nous retranchons les \( b\) nombres déjà traités) en
 %       \begin{equation}
 %       \mathopen] \frac{ a_n }{ b^n } , \frac{ a_n }{ b^n }+\frac{1}{ b^{n+1} } \mathclose[ \cup \mathopen] \frac{ a_n }{ b^n }+\frac{1}{ b^{n+1} } , \frac{ a_n }{ b^n }+\frac{2}{ b^{n+1} } \mathclose[\cup\ldots\cup\mathopen] \frac{ a_n }{ b^n }+\frac{ b-1 }{ b^{n+1} } , \frac{ a_n }{ b^n }+\frac{ 1 }{ b^n } \mathclose[.
 %       \end{equation}
        , pour simplifier les notations nous notons \( x'=x-\sum_{k=1}^{n}\frac{ a_k }{ b^k }\) et nous avons
        \begin{equation}
            0<x'<\frac{ a_n+1 }{ b^n }.
        \end{equation}
        Le nombre \( x'\) est forcément dans un des intervalles
        \begin{equation}
                \mathopen] \frac{ s }{ b^{n+1} } , \frac{ s+1 }{ b^{n+1} } \mathclose[
        \end{equation}
        avec \( s\in\{ 0,\ldots, b-1 \}\). Nous prenons le \( s\) correspondant à \( x'\) comme \( a_{n+1}\). Dans ce cas nous avons
        \begin{equation}
            \sum_{k=1}^{n+1}\frac{ a_k }{ b^k }< x<\sum_{k=1}^{n+1}\frac{ a_k }{ b^k }+\frac{1}{ b^{n+1} }.
        \end{equation}
        Note : les deux inégalités sont strictes. La première parce que s'il y avait égalité, nous nous serions déjà arrêté en complétant avec des zéros. La seconde parce que 
        \begin{equation}
            \sum_{k=n+2}^{\infty}\frac{ a_k }{ b^k }\leq \sum_{k=n+2}^{\infty}\frac{ b-1 }{ b^k }=\frac{1}{ b^{n+1} }
        \end{equation}
        où l'égalité n'est possible que si \( a_k=b-1\) pour tout \( k\geq n+2\). Dans ce cas nous aurions eu
        \begin{equation}
            x=\sum_{k=1}^{n}\frac{ a_k }{ b^k }+\frac{ a_{n+1}+1 }{ b^{n+1} }
        \end{equation}
        et nous aurions choisit le nombre \( a_{n+1}\) autrement et complété la suite par des zéros à partir de là. Notons que cela prouve au passage que la suite que nous sommes en train de construire est bien dans \( \eD_b\) parce qu'elle ne contiendra pas de queue de suite composée de \( b-1\).

        Ceci termine la construction par récurrence de la suite \( a\in \eD_b\). Par construction nous avons pour tout \( N\geq 1\),
        \begin{equation}
            \sum_{k=1}^N\frac{ a_k }{ b^k }\leq x\leq \sum_{k=1}^N\frac{ a_k }{ b^k }+\frac{1}{ b^{N+1} }, 
        \end{equation}
        autrement dit : \( \varphi_b(a_1,\ldots, a_N)\in B(x,\frac{1}{ b^{N+1} })\). Nous avons donc bien convergence
        \begin{equation}
            \lim_{N\to \infty} \varphi_b(a_1,\ldots, a_N)=x
        \end{equation}
        et l'application \( \varphi_b\) est surjective.
\end{proof}

L'application \( \varphi_b^{-1}\colon \mathopen[ 0 , 1 [\to \eD_b\) est la \defe{décomposition décimale}{décimale!décomposition} en base \( b\) des nombres de \( \mathopen[ 0 , 1 [\).

Tout cela nous permet de montrer entre autres que \( \eR\) n'est pas dénombrable. Vu qu'il y a une bijection entre \( \mathopen[ 0 , 1 [\) et \( \eD_b\), il suffit de prouver que \( \eD_b\) est non dénombrable. De plus il suffit de démontrer que \( \eD_b\) est non dénombrable pour un entier \( b\geq 2\) donné.

\begin{proposition}[\cite{KZIoofzFLV}]  \label{PropNNHooYTVFw} 
    Il n'existe pas de surjection \( \eN\to \eD_b\). Autrement dit \( \eD_b\) est non dénombrable.
\end{proposition}

\begin{proof}
    Nous prenons \( b\neq 2\) pour des raisons qui seront claires plus tard. Soit \( f\colon \eN\to \eD_b\). Pour \( i\in \eN\) nous notons 
    \begin{equation}
        f(n)=(c_i^{(n)})_{i\geq 1},
    \end{equation}
    et nous définissons la suite
    \begin{equation}
        c_k=\begin{cases}
            0    &   \text{si } c_k^{(k)}\neq 0\\
            1    &    \text{si } c_k^{(k)}=0.
        \end{cases}
    \end{equation}
    Cela est une suite dans \( \eD_b\) parce que \( b\neq 2\) et que la suite ne contient que des \( 0\) et des \( 1\). Mais nous n'avons \( f(n)=c\) pour aucun \( n\in \eN\) parce que nous avons \( c_n\neq f(n)_n\).

    Si \( b=2\) alors nous savons que \( \eD_2\sim\mathopen[ 0 , 1 [\sim \eD_3\). Donc \( \eD_2\sim \eD_3\) et \( \eD_2\) ne peut pas plus être mis en bijection avec \( \eN\) que \( \eD_3\).
\end{proof}
\begin{remark}
    La preuve ne fonctionne pas en base \( b=2\) parce que rien n'empêche d'avoir une queue de \( 1\). Il y a alors toutefois moyen de se débrouiller en construisant la suite \( c\) de façon plus subtile. Si \( b=2\) et \( n\in \eN\) alors \( f(n)\) est une suite de \( 0\) et \( 1\) contenant une infinité de \( 0\) (parce qu'il n'y a pas de queue de suite ne contenant que des \( 1\)). Nous construisons alors \( c\) de la façon suivante : d'abord nous recopions \( f(0)\) jusqu'à son \emph{deuxième} zéro que nous changeons en \( 1\); nommons \( n_0\) le rang de ce deuxième zéro. Ensuite nous recopions les éléments de \( f(1) \) à partir du rang \( n_0+1\) jusqu'au second zéro que nous changeons en \( 1\), etc.

    Le fait de prendre le deuxième zéro nous garanti que la suite \( c\) n'aura pas de queue de suite ne contenant que des \( 1\).

    Notons que cette construction s'adapte à tout \( b\); il suffit de prendre le second terme qui n'est pas \( b-1\) et le remplacer par \( b-1\).
\end{remark}

\begin{corollary}
    L'ensemble \( \mathopen[ 0 , 1 [\) n'est pas dénombrable.
\end{corollary}

\begin{proof}
    L'ensemble \( \mathopen[ 0 , 1 [\) est en bijection avec \( \eD_b\) que nous venons de prouver n'être pas dénombrable.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Exponentielle de matrice}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%\label{subsecAOnIwQM}
\label{secAOnIwQM}

\begin{enumerate}
    \item
        En ce qui concerne la continuité, nous aurons évidemment besoin de théorie à propos de l'inversion de limites et de sommes. Nous en parlerons donc en \ref{subsecXNcaQfZ}.
    \item 
        Les séries entières de matrices seront traitées plus en détail autour de la proposition \ref{PropFIPooSSmJDQ}.
\end{enumerate}

\begin{proposition}     \label{PropPEDSooAvSXmY}
    Soit \( V\) un espace vectoriel de dimension finie et \( A\in\End(V)\). La série
    \begin{equation}
        \exp(A)=\mtu+A+\frac{ A^2 }{ 2 }+\frac{ A^3 }{ 3 }+\ldots =\sum_{k=1}^{\infty}\frac{ A^k }{ k! }.
    \end{equation}
    converge normalement dans \( \big( \End(V),\| . \|_{op} \big)\).  L'\defe{exponentielle}{exponentielle!de matrice} de la matrice \( A\) est cette matrice.
\end{proposition}

\begin{proof}
    Vu que la norme opérateur est une norme d'algèbre par le lemme \ref{LEMooFITMooBBBWGI}, nous avons pour tout \( k\) la majoration \( \| A^k \|\leq \| A \|^k\). Nous avons donc
    \begin{equation}
        \sum_{k=0}^{\infty}\frac{ \| A^k \| }{ k! }\leq \sum_k\frac{ \| A \|^k }{ k! }.
    \end{equation}
    La dernière somme converge en vertu de la convergence de la série exponentielle donnée en exemple \ref{ExIJMHooOEUKfj}.
\end{proof}

Étant donné que c'est une limite, il y a une question de convergence et donc de topologie. C'est pour cela que nous ne pouvions pas introduire l'exponentielle de matrice avant d'avoir introduit la norme des matrices. La convergence de la série pour toute matrice sera prouvée au passage dans la proposition \ref{PropFMqsIE}.


La fonction exponentielle \(  x\mapsto e^{x}\) n'est pas un polynôme en \( x\), mais nous avons les résultat marrant suivant.
\begin{proposition} \label{PropFMqsIE}
    Si \( u\) est un endomorphisme, alors \( \exp(u)\) est un polynôme en \( u\)\footnote{Nan, mais j'te jure : \( \exp\) n'est pas un polynôme, mais $\exp(u)$ est un polynôme de \( u\).}.
\end{proposition}

\begin{proof}
    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            \varphi_u\colon \eK[X]&\to \End(E) \\
            P&\mapsto P(u)
        \end{aligned}
    \end{equation}
    Étant donné que l'image de \( \varphi_u\) est un fermé dans \( \End(E)\), il suffit de montrer que la série
    \begin{equation}
        \sum_{k=0}^{\infty}\frac{ \varphi_u(X)^k }{ k! }
    \end{equation}
    converge dans \( \End(E)\) pour qu'elle converge dans \( \Image(\varphi_u)\). Pour ce faire nous nous rappelons de la norme opérateur\footnote{Définition \ref{DefNFYUooBZCPTr}.} et de la propriété fondamentale \( \| A^k \|\leq \| A \|^k\). En notant \( A=\varphi_u(X)\),
    \begin{equation}
        \left\| \sum_{k=n}^m\frac{ A^k }{ k! } \right\|\leq \sum_{k=n}^m\frac{ \| A^k \| }{ k! }\leq \sum_{k=n}^m\frac{ \| A \|^k }{ k! },
    \end{equation}
    ce qui est une morceau du développement de \(  e^{\| A \|}\). La limite \( n\to\infty\) est donc zéro par la convergence de l'exponentielle réelle. La suite des sommes partielles de  $e^{A}$ est donc de Cauchy. La série converge donc parce que nous sommes dans un espace vectoriel réel de dimension finie (\( \End(E)\)).
\end{proof}
% TODO : et tant qu'on y est, justifier la convergence de la série de l'exponentielle réelle.

\begin{remark}
    Pourquoi \( \exp(u)\) est-il un polynôme d'endomorphisme alors que \( \exp\) n'est pas un polynôme ? Lorsque nous disons que la fonction \( x\mapsto \exp(x)\) n'est pas un polynôme, nous sommes en train de localiser la fonction \( \exp\) à l'intérieur de l'espace de toutes les fonctions \( \eR\to \eR\), c'est à dire à l'intérieur d'un espace de dimension infinie. Au contraire lorsqu'on parle de \( \exp(u)\) et qu'on le compare aux endomorphismes \( P(u)\), nous sommes en train de repérer \( \exp(u)\) à l'intérieur de l'espace des matrices qui est de dimension finie. Il n'est donc pas étonnant que l'on parvienne moins à faire la distinction.

    Si par contre nous considérons \( \exp\) en tant qu'application \( \exp\colon \End(E)\to \End(E)\), ce n'est pas un polynôme.

    Si \( u\) et \( v\) sont des endomorphismes, nous aurons des polynômes \( P\) et \( Q\) tels que \( e^u=P(u)\) et \( e^v=Q(v)\); mais nous n'aurons en général évidemment pas \( P=Q\). En cela, \( \exp\) n'est pas un polynôme.
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Sommes de familles infinies}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooHHDXooUgLhHR}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Convergence commutative}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Soit \( x_k\) une suite dans un espace vectoriel normé \( E\). Nous disons que la suite \defe{converge commutativement}{convergence!commutative} vers \( x\in E\) si \( \lim_{n\to \infty}\| x_n-x \| =0\) et si pour toute bijection \( \tau\colon \eN\to \eN\) nous avons aussi
    \begin{equation}
        \lim_{n\to \infty} \| x_{\tau(k)}-x \|=0.
    \end{equation}
    La notion de convergence commutative est surtout intéressante pour les séries. La somme
    \begin{equation}
        \sum_{k=0}^{\infty}x_k
    \end{equation}
    converge commutativement vers \( x\) si \( \lim_{N\to \infty} \| x-\sum_{k=0}^Nx_k \|=0\) et si pour toute bijection \( \tau\colon \eN\to \eN\) nous avons
    \begin{equation}
        \lim_{N\to \infty} \| x-\sum_{k=0}^Nx_{\tau(k)} \|=0.
    \end{equation}
\end{definition}

Nous démontrons maintenant qu'une série converge commutativement si et seulement si elle converge absolument.

Pour le sens inverse, nous avons la proposition suivante.
\begin{proposition}
    Soit \( \sum_{k=0}^{\infty}a_k\) une série réelle qui converge mais qui ne converge pas absolument. Alors pour tout \( b\in \eR\), il existe une bijection \( \tau\colon \eN\to \eN\) telle que \( \sum_{i=0}^{\infty}a_{\tau(i)}=b\).
\end{proposition}
Pour une preuve, voir \href{http://gilles.dubois10.free.fr/analyse_reelle/seriescomconv.html}{chez Gilles Dubois}.

\begin{proposition} \label{PopriXWvIY}
    Soit \( (a_i)_{i\in \eN}\) une suite dans \( \eC\) convergent absolument. Alors elle converge commutativement.
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\). Nous posons \( \sum_{i=0}^\infty a_i=a\) et nous considérons \( N\) tel que
    \begin{equation}
        | \sum_{i=0}^Na_i-a |<\epsilon.
    \end{equation}
    Étant donné que la série des \( | a_i |\) converge, il existe \( N_1\) tel que pour tout \( p,q>N_1\) nous ayons \( \sum_{i=p}^q| a_i |<\epsilon\). Nous considérons maintenant une bijection \( \tau\colon \eN\to \eN \). Prouvons que la série \( \sum_{i=0}^{\infty}| a_{\tau(i)} |\) converge. Nous choisissons \( M\) de telle sorte que pour tout \( n>M\), \( \tau(n)>N_1\); alors si \( p,q>M\) nous avons
    \begin{equation}
        \sum_{i=p}^q| a_{\tau(i)} |<\epsilon.
    \end{equation}
    Par conséquent la somme de la suite \( (a_{\tau(i)})\) converge. Nous devons montrer à présent qu'elle converge vers la même limite que la somme «usuelle» \( \lim_{N\to \infty} \sum_{i=0}^Na_i\).

    Soit \( n>\max\{ M,N \}\). Alors
    \begin{equation}
        \sum_{k=0}^na_{\tau(k)}-\sum_{k=0}^na_k=\sum_{k=0}^Ma_{\tau(k)}-\sum_{k=0}^Na_k+\underbrace{\sum_{M+1}^na_{\tau(k)}}_{<\epsilon}-\underbrace{\sum_{k=N+1}^na_k}_{<\epsilon}.
    \end{equation}
    Par construction les deux derniers termes sont plus petits que \( \epsilon\) parce que \( M\) et \( N\) sont les constantes de Cauchy pour les séries \( \sum a_{\tau(i)}\) et \( \sum a_i\). Afin de traiter les deux premiers termes, quitte à redéfinir \( M\), nous supposons que \( \{ 1,\ldots, N \}\subset \tau\{ 1,\ldots, M \}\); par conséquent tous les \( a_i\) avec \( i<N\) sont atteints par les \( a_{\tau(i)}\) avec \( i<M\). Dans ce cas, les termes qui restent dans la différence
    \begin{equation}
        \sum_{k=0}a_{\tau(k)}-\sum_{k=0}^Na_k
    \end{equation}
    sont des \( a_k\) avec \( k>N\). Cette différence est donc en valeur absolue plus petite que \( \epsilon\), et nous avons en fin de compte que
    \begin{equation}
        \left| \sum_{k=0}^na_{\tau(k)}-\sum_{k=0}^na_k \right| <\epsilon.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PropyFJXpr}
    Soit \( \sum_{i=0}^{\infty}a_i\) une série qui converge mais qui ne converge pas absolument. Pour tout \( b\in \eR\), il existe une bijection \( \tau\colon \eN\to \eN\) telle que \( \sum_{i=}^{\infty}a_{\tau(i)}=b\).
\end{proposition}

Les propositions \ref{PopriXWvIY} et \ref{PropyFJXpr} disent entre autres qu'une série dans \( \eC\) est commutativement sommable si et seulement si elle est absolument sommable.

Soit \( (a_i)_{i\in I}\) une famille de nombres complexes indexée par un ensemble \( I\) quelconque. Nous allons nous intéresser à la somme \( \sum_{i\in I}a_i\).


Soit \( \{ a_i \}_{i\in I}\) des nombres positifs. Nous définissons la somme
\begin{equation}
    \sum_{i\in I}a_i=\sup_{ J\text{ fini}}\sum_{j\in J}a_j.
\end{equation}
Notons que cela est une définition qui ne fonctionne bien que pour les sommes de nombres positifs. Si \( a_i=(-1)^i\), alors selon la définition nous aurions \( \sum_i(-1)^i=\infty\). Nous ne voulons évidemment pas un tel résultat.

Dans le cas de familles de nombres réels positifs, nous avons une première définition de la somme. 
\begin{definition}  \label{DefHYgkkA}
Soit \( (a_i)_{i\in I}\) une famille de nombres réels positifs indexés par un ensemble quelconque \( I\). Nous définissons
\begin{equation}
    \sum_{i\in I}a_i=\sup_{ J\text{ fini dans } I}\sum_{j\in J}a_j.
\end{equation}
\end{definition}

\begin{definition}  \label{DefIkoheE}
    Si \( \{ v_i \}_{i\in I}\) est une famille de vecteurs dans un espace vectoriel normé indexée par un ensemble quelconque \( I\). Nous disons que cette famille est \defe{sommable}{famille!sommable} de somme \( v\) si pour tout \( \epsilon>0\), il existe un \( J_0\) fini dans \( I\) tel que pour tout ensemble fini \( K\) tel que \( J_0\subset K\) nous avons
    \begin{equation}
        \| \sum_{j\in K}v_j-v \|<\epsilon.
    \end{equation}
\end{definition}
Notons que cette définition implique la convergence commutative.

\begin{example}
    La suite \( a_i=(-1)^i\) n'est pas sommable parce que quel que soit \( J_0\) fini dans \( \eN\), nous pouvons trouver \( J\) fini contenant \( J_0\) tel que \( \sum_{j\in J}(-1)^j>10\). Pour cela il suffit d'ajouter à \( J_0\) suffisamment de termes pairs. De la même façon en ajoutant des termes impairs, on peut obtenir \( \sum_{j\in J'}(-1)^i<-10\).
\end{example}

\begin{example}
    De temps en temps, la somme peut sortir d'un espace. Si nous considérons l'espace des polynômes \( \mathopen[ 0 , 1 \mathclose]\to \eR\) muni de la norme uniforme, la somme de l'ensemble
    \begin{equation}
        \{ 1,-1,\pm\frac{ x^n }{ n! } \}_{n\in \eN}
    \end{equation}
    est zéro.

    Par contre la somme de l'ensemble \( \{ 1,\frac{ x^n }{ n! } \}_{n\in \eN}\) est l'exponentielle qui n'est pas un polynôme.
\end{example}

\begin{example}
    Au sens de la définition \ref{DefIkoheE} la famille
    \begin{equation}
        \frac{ (-1)^n }{ n }
    \end{equation}
    n'est pas sommable. En effet la somme des termes pairs est \( \infty\) alors que la somme des termes impairs est \( -\infty\). Quel que soit \( J_0\in \eN\), nous pouvons concocter, en ajoutant des termes pairs, un \( J\) avec \( J_0\subset J\) tel que \( \sum_{j\in J}(-1)^j/j\) soit arbitrairement grand. En ajoutant des termes négatifs, nous pouvons également rendre \( \sum_{j\in J}(-1)^j/j\) arbitrairement petit.
\end{example}

\begin{proposition} \label{PropVQCooYiWTs}
    Si \( (a_{ij})\) est une famille de nombres positifs indexés par \( \eN\times \eN\) alors
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}=\sum_{i=1}^{\infty}\Big( \sum_{j=1}^{\infty}a_{ij} \Big)
    \end{equation}
    où la somme de gauche est celle de la définition \ref{DefHYgkkA}.
\end{proposition}
%TODO : cette proposition peut être vue comme une application de Fubini pour la mesure de comptage. Le faire et référentier ici.

\begin{proof}
    Nous considérons \( J_{m,n}=\{ 0,\ldots, m \}\times \{ 0,\ldots, n \}\) et nous avons pour tout \( m\) et \( n\) :
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}\geq \sum_{(i,j)\in J_{m,n}}a_{ij}=\sum_{i=1}^m\Big( \sum_{j=1}^na_{ij} \Big).
    \end{equation}
    Si nous fixons \( m\) et que nous prenons la limite \( n\to \infty\) (qui commute avec la somme finie sur \( i\)) nous trouvons
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}\geq =\sum_{i=1}^m\Big( \sum_{j=1}^{\infty}a_{ij} \Big).
    \end{equation}
    Cela étant valable pour tout \( m\), c'est encore valable à la limite \( m\to \infty\) et donc
    \begin{equation}
        \sum_{(i,j)\in \eN^2}a_{ij}\geq \sum_{i=1}^{\infty}\Big( \sum_{j=1}^{\infty}a_{ij} \Big).
    \end{equation}
    
    Pour l'inégalité inverse, il faut remarquer que si \( J\) est fini dans \( \eN^2\), il est forcément contenu dans \( J_{m,n}\) pour \( m\) et \( n\) assez grand. Alors
    \begin{equation}
        \sum_{(i,j)\in J}a_{ij}\leq \sum_{(i,j)\in J_{m,n}}a_{ij}=\sum_{i=1}^m\sum_{j=1}^na_{ij}\leq \sum_{i=1}^{\infty}\Big( \sum_{j=1}^{\infty}a_{ij} \Big).
    \end{equation}
    Cette inégalité étant valable pour tout ensemble fini \( J\subset \eN^2\), elle reste valable pour le supremum.    
\end{proof}

La définition générale de la somme \ref{DefIkoheE} est compatible avec la définition usuelle dans les cas où cette dernière s'applique.
\begin{proposition}[commutative sommabilité]\label{PropoWHdjw}
    Soit \( I\) un ensemble dénombrable et une bijection \( \tau\colon \eN\to I\). Soit \( (a_i)_{i\in I}\) une famille dans un espace vectoriel normé. Alors
    \begin{equation}
        \sum_{k=0}^{\infty}a_{\tau(k)}=\sum_{i\in I}a_i
    \end{equation}
    dès que le membre de droite existe. Le membre de gauche est définit par la limite usuelle.
\end{proposition}

\begin{proof}
    Nous posons \( a=\sum_{i\in I}a_i\). Soit \( \epsilon>0\) et \( J_0\) comme dans la définition. Nous choisissons
    \begin{equation}
        N>\max_{j\in J_0}\{ \tau^{-1}(j) \}.
    \end{equation}
    En tant que sommes sur des ensembles finis, nous avons l'égalité
    \begin{equation}
        \sum_{k=0}^Na_{\tau(k)}=\sum_{j\in J_0}a_j
    \end{equation}
    où \( J\) est un sous-ensemble de \( I\) contenant \( J_0\). Soit \( J\) fini dans \( I\) tel que \( J_0\subset J\). Nous avons alors
    \begin{equation}
        \| \sum_{k=0}^Na_{\tau(k)}-a \|=\| \sum_{j\in J}a_j-a \|<\epsilon.
    \end{equation}
    Nous avons prouvé que pour tout \( \epsilon\), il existe \( N\) tel que \( n>N\) implique \( \| \sum_{k=0}^na_{\tau(k)}-a\| <\epsilon\).
\end{proof}

\begin{corollary}
    Nous pouvons permuter une somme dénombrable et une fonction linéaire continue. C'est à dire que si \( f\) est une fonction linéaire continue sur l'espace vectoriel normé \( E\) et \( (a_i)_{i\in I}\) une famille sommable dans \( E\) alors
    \begin{equation}
        f\left( \sum_{i\in I}a_i \right)=\sum_{i\in I}f(a_i).
    \end{equation}
\end{corollary}

\begin{proof}
    En utilisant une bijection \( \tau\) entre \( I\) et \( \eN\) avec la proposition \ref{PropoWHdjw} ainsi que le résultat connu à propos des sommes sur \( \eN\), nous avons
    \begin{subequations}
        \begin{align}
            f\left( \sum_{i\in I}a_i \right)&=f\left( \sum_{k=0}^{\infty}a_{\tau(k)} \right)\\
            &=\sum_{k=0}^{\infty}f(a_{\tau(k)}) \label{SUBEQooCVUTooPmnHER}\\
            &=\sum_{i\in I}f(a_i).
        \end{align}
    \end{subequations}
    Notons que le passage à \eqref{SUBEQooCVUTooPmnHER} n'est pas du tout une trivialité à deux francs cinquante. Il s'agit d'écrire la somme comme la limite des sommes partielles, et de permuter \( f\) avec la limite en invoquant la continuité, puis de permuter \( f\) avec la somme partielle en invoquant sa linéarité.

    Ah, tiens et tant qu'on y est à dire qu'il y a des chose évidentes qui ne le sont pas, oui, il existe des applications linéaires non continues, voir le thème \ref{THEMEooYCBUooEnFdUg}.
\end{proof}

La proposition suivante nous enseigne que les sommes infinies peuvent être manipulée de façon usuelle.
\begin{proposition} \label{PropMpBStL}
    Soit \( I\) un ensemble dénombrable. Soient \( (a_i)_{i\in I}\) et \( (b_i)_{i\in I}\), deux familles de réels positifs telles que \( a_i<b_i\) et telles que \( (b_i)\) est sommable. Alors \( (a_i)\) est sommable.

    Si \( (a_i)_{i\in I}\) est une famille de complexes telle que \( (| a_i |)\) est sommable, alors \( (a_i)\) est sommable.
\end{proposition}

\begin{proposition}[\cite{MonCerveau}]     \label{PROPooWLEDooJogXpQ}
    Soit un espace vectoriel normé \( E\) et une famille sommable\footnote{Définition \ref{DefIkoheE}.} \( \{ v_i \}_{i\in I}\) d'éléments de \( E\). Soit \( f\colon E\to \eC\) une application sur laquelle nous supposons
    \begin{enumerate}
        \item
            \( f\) est linéaire et continue;
        \item
            la partie \( \{ f(v_i)_{i\in I} \} \) est sommable.
    \end{enumerate}
    Alors nous pouvons permuter la somme et \( f\) :
    \begin{equation}        \label{EQooONHXooKqIEbY}
        f\big( \sum_{i\in I}v_i \big)=\sum_{i\in I}f(v_i).
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\); vu que les familles \( \{ v_i \}_{i\in I}\) et \( \{ f(v_i) \}_{i\in I}\) sont sommables, nous pouvons considérer les parties finies \( J_1\) et \( J_2\) de \( I\) telles que
    \begin{equation}
        \big\| \sum_{j\in J_1}v_j-\sum_{i\in I}v_i \big\|\leq \epsilon
    \end{equation}
    et
    \begin{equation}
        \big\| \sum_{j\in J_2}f(v_j)-\sum_{i\in I}f(v_i) \big\|\leq \epsilon
    \end{equation}
    Ensuite nous posons \( J=J_1\cup J_2\). Avec cela nous calculons un peu avec les majorations usuelles :
    \begin{equation}
        \| f(\sum_{i\in I}v_i) -\sum_{i\in I}f(v_i) \|\leq \| f(\sum_{i\in I}v_i)- f(\sum_{j\in J}v_j) \|+  \| f(\sum_{j\in J}v_j)-\sum_i\in If(v_i) \|.
    \end{equation}
    Le second terme est majoré par \( \epsilon\), tandis que le premier, en utilisant la linéarité de \( f\) possède la majoration
    \begin{equation}
        \| f(\sum_{i\in I}v_i)- f(\sum_{j\in J}v_j) \|=\| f(\sum_{i\in I}v_i-\sum_{j\in J}v_j) \|\leq \| f \| \| \sum_{i\in I}v_i- \sum_{j\in J}v_j\|\leq \epsilon\| f \|.
    \end{equation}
    Donc pour tout \( \epsilon>0\) nous avons
    \begin{equation}
        \| f(\sum_{i\in I}v_i) -\sum_{i\in I}f(v_i) \|\leq \epsilon(1+\| f \|).
    \end{equation}
    D'où l'égalité \eqref{EQooONHXooKqIEbY}.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Mini introduction aux nombres \texorpdfstring{p}{$p$}-adiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\subsection{La flèche d'Achille}\label{s:un}

C'est un grand classique que je donne ici juste comme introduction pour montrer que des série infinies peuvent donner des nombres finis de manière tout à fait intuitive.

Achille tire une flèche vers un arbre situé à $\unit{10}{\meter}$ de lui. Disons que la flèche avance à une vitesse constante de $\unit{1}{\meter\per\second}$. Il est clair que la flèche mettra $\unit{10}{\second}$ pour toucher l'arbre. En $\unit{5}{\second}$, elle aura parcouru la moitié de son chemin. On le note :
\[
\text{temps}=5s+\ldots
\]
Reste \( \unit{5}{\meter}\) à faire. En $\unit{2.5}{\second}$, elle aura fait la moitié de ce chemin chemin, soit $2.5m=\frac{10}{4}m$. On le note :
\[
\text{temps}=\frac{10}{2}s+\frac{10}{4}s+
\]
Reste $2.5m$ à faire. La moitié de ce trajet, soit $\frac{10}{8}m$, est parcouru en $\frac{10}{8}s$; on le note encore, mais c'est la dernière fois !

\[
\text{temps}=\frac{10}{2}s+\frac{10}{4}s+\frac{10}{8}s+
\]
En continuant ainsi à regarder la flèche qui parcours des demi-trajets puis des demi de demi-trajets et encore des demi de demi de demi-trajets, et en sachant que le temps total est $10s$, on trouve :
\[
10\left( \frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\frac{1}{16}+\ldots  \right)=10.
\]
On doit donc croire que la somme jusqu'à l'infini des inverse des puissances de deux vaut $1$ :
\[
   \sum_{n=1}^{\infty}\frac{1}{2^n}=1.
\]
Cela peut être démontré à la loyale.

\subsection{La tortue et Achille}

Maintenant qu'on est convaincu que des sommes infinies peuvent représenter des nombres tout à fait normaux, passons à un truc plus marrant.

Achille, qui marche peinard à $\unit{10}{\meter\per\hour}$, part avec $1m$ d'avance sur une tortue qui avance à $\unit{1}{\meter\per\hour}$. Le temps que la tortue arrive au point de départ d'Achille, Achille aura parcouru $10m$, et le temps que la tortue mettra pour arriver à ce point, eh bien, Achille ne sera déjà plus là : il sera à $100m$. Si la tortue tient bon pendant un temps infini, et si l'on est confiant en le genre de raisonnements faits à la section \ref{s:un}, elle rattrapera Achille dans 
\[
1m+10m+100m+1000m+\ldots
\]
Autant dire que ça ne risque pas d'arriver. Et pourtant, mettons en équations : 
\begin{subequations}
    \begin{numcases}{}
        x_{\text{Achile}}(t)=1+10t\\
        x_{\text{tortue}}(t)=t.
    \end{numcases}
\end{subequations}
La tortue rejoints Achille au temps \( t\) tel que \( x_{\text{Achille}(t)}=x_{\text{tortue}}(t)\). Un mini calcul donne $t=-1/9$. Physiquement, c'est une situation logique. Peut-on en déduire une égalité mathématique du style de 
\[
1+10+100+1000+\ldots=-\frac{1}{9}\; ???
\]
Là où les choses deviennent jolies, c'est quand on cherche à voir ce que peut bien être la valeur d'un hypothétique $x=1+10+100+1000+\ldots$. En effet, logiquement on devrait avoir
\begin{equation*}
\begin{split}
\frac{x}{10}&=\frac{1}{10}+1+10+100+\ldots\\
            &=\frac{1}{10}+x.
\end{split}
\end{equation*}
Reste à résoudre l'équation du premier degré : $\frac{x}{10}=x+\frac{1}{10}$. Ai-je besoin de donner la solution ?

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dans les nombres \texorpdfstring{p}{$ p$}-adiques, c'est vrai}
%---------------------------------------------------------------------------------------------------------------------------

Nous nous proposons d'apprendre sur les nombres \( p\)-adiques juste ce qu'il faut pour montrer que l'égalité
\begin{equation}
    \sum_{k=0}^{\infty}10^k=-\frac{1}{ 9 }
\end{equation}
est vraie dans les nombres \( 5\)-adiques. Tout ce qu'il faut est sur \wikipedia{fr}{Nombre_p-adique}{wikipedia}.

Soit \( a\in \eN\) et \( p\), un nombre premier. La \defe{valuation}{valuation!$p$-adique} \( p\)-adique de \( a\) est l'exposant de \( p\) dans la décomposition de \( a\) en nombres premiers. On la note \( v_p(a)\). Pour un rationnel on définit
\begin{equation}
    v_p\left( \frac{ a }{ b } \right)=v_p(a)-v_p(b)
\end{equation}
La \defe{valeur absolue}{valeur absolue!$p$-adique} \( p\)-adique de \( r\in \eQ\) est 
\begin{equation}
    | r |_p=p^{-v_p(r)}.
\end{equation}
Nous posons \( | 0 |_p=0\). De là nous considérons la distance
\begin{equation}
    d_p(x,y)=| x-y |_p.
\end{equation}

\begin{lemma}
    L'espace \( (\eQ,d_p)\) est un espace métrique\footnote{Définition \ref{DefMVNVFsX}}.
\end{lemma}
\index{topologie!\( p\)-adique}

Nous considérons maintenant \( p=5\). Étant donné que \( a=5\cdot 2\) nous avons \( v_5(10)=1\) et
\begin{equation}
    v_5\left( \frac{1}{ 9 } \right)=v_5(1)-v_5(9)=0.
\end{equation}
Nous avons
\begin{equation}
    \sum_{k=0}^N10^k+\frac{1}{ 9 }=\frac{ 10^{N+1} }{ 9 }
\end{equation}
mais
\begin{equation}
    v_p\left( \frac{ 10^{N+1} }{ 9 } \right)=v_5(10^{N+1})-v_5(9)=N+1.
\end{equation}
Par conséquent
\begin{equation}
    d_5\big( \sum_{k=0}^N10^k,-\frac{1}{ 9 } \big)=| \frac{ 10^{N+1} }{ 9 } |_p=p^{-(N+1)}.
\end{equation}
En passant à la limite,
\begin{equation}
    \lim_{N\to \infty} d_5\big( \sum_{k=0}^N10^k,-\frac{1}{ 9 } \big)=0,
\end{equation}
ce qui signifie que\footnote{Voir la définition \ref{DefGFHAaOL} de la convergence d'une série dans un espace métrique.}
\begin{equation}
    \sum_{k=0}^{\infty}10^k=-\frac{1}{ 9 }.
\end{equation}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Fonctions}		\label{Sect_fonctions}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soient $(V,\| . \|_V)$ et $(W,\| . \|_W)$ deux espaces vectoriels normés, et une fonction $f$ de $V$ dans $W$. Il est maintenant facile de définir les notions de limites et de continuité pour de telles fonctions en copiant les définitions données pour les fonctions de $\eR$ dans $\eR$ en changeant simplement les valeurs absolues par les normes sur $V$ et $W$.

La caractérisation suivante est un recopiage de la définition \ref{DefOLNtrxB} lorsque la topologie est donnée par des boules.
\begin{proposition}\label{PropHOCWooSzrMjl}
	Soit $f\colon V\to W$ une fonction de domaine \( \Domaine(f)\subset V\) et soit $a$ un point d'accumulation de $\Domaine(f)$. 
    La fonction \( f\) en $a$ si et seulement s'il existe un élément $\ell\in W$ tel que pour tout $\varepsilon>0$, il existe un $\delta>0$ tel que pour tout $x\in \Domaine(f)$,
    \begin{equation}        \label{EqDefLimzxmasubV}
		0<\| x-a \|_V<\delta\,\Rightarrow\,\| f(x)-\ell \|_W<\varepsilon.
	\end{equation}
	Dans ce cas, nous écrivons $\lim_{x\to a} f(x)=\ell$ et nous disons que $\ell$ est la \defe{limite}{limite} de $f$ lorsque $x$ tend vers $a$.
\end{proposition}

\begin{remark}
    Le fait que nous limitions la formule \eqref{EqDefLimzxmasubV} aux \( x\) dans le domaine de \( f\) n'est pas anodin. Considérons la fonction \( f(x)=\sqrt{x^2-4}\), de domaine \( | x |\geq 2\). Nous avons
    \begin{equation}
        \lim_{x\to 2} \sqrt{x^2-4}=0.
    \end{equation}
    Nous ne pouvons pas dire que cette limite n'existe pas en justifiant que la limite à gauche n'existe pas. Les points \( x<2\) sont hors du domaine de \( f\) et ne comptent dons pas dans l'appréciation de l'existence de la limite.

    Vous verrez plus tard que ceci provient de la \wikipedia{fr}{Topologie_induite}{topologie induite} de \( \eR\) sur l'ensemble \( \mathopen[ 2 , \infty [\).
\end{remark}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Produit fini d'espaces vectoriels normés}\label{sec_prod}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Dans cette sections nous parlons de produits finis d'espaces. Cela ne signifie pas que chacun des espaces soient séparément de dimension finie.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Norme}
%---------------------------------------------------------------------------------------------------------------------------

La définition de la norme sur un produit d'espaces vectoriels normés découle immédiatement de la définition de la distance \ref{DefZTHxrHA} :
\begin{definition}  \label{DefFAJgTCE}
    Soient $V$ et $W$ deux espaces vectoriels normés. On appelle \defe{espace produit}{produit!d'espaces vectoriels normés} de $V$ et $W$ le produit cartésien $V\times W$ 
    \begin{equation}
    V\times W=\{(v,w)\,|\, v\in V,\, w\in W\},
    \end{equation}
    muni de la norme $\|\cdot \|_{V\times W}$
    \begin{equation}	\label{EqNormeVxWmax}
        \|(v,w) \|_{V\times W}=\max\{\|v\|_{V},\|w\|_W\}.
    \end{equation}
\end{definition}
Il est presque immédiat de vérifier que le produit cartésien $V\times W$ est un espace vectoriel pour les opération de somme et multiplication par les scalaires définies composante par composante. C'est à dire,  si $(v_1,w_1)$, $(v_2,w_2)$ sont dans $V\times W$ et $a$, $b$ sont des scalaires, alors  
\begin{equation}
 a (v_1,w_1)+ b(v_2,w_2)=(av_1,aw_1)+ (bv_2,bw_2)=(av_1+bv_2,aw_1+bw_2).
\end{equation}

\begin{lemma}
	L'opération $\|\cdot \|_{V\times W}\colon V\times W\to \eR$ est une norme.
\end{lemma}

\begin{proof}
	On doit vérifier les trois conditions de la définition \ref{DefNorme}.
	\begin{itemize}
		\item Soit $(v,w)$ dans $V\times W$ tel que $\|(v,w)\|_{V\times W}=\max\{\|v\|_{V},\|w\|_W\}=0$. Alors $\|v\|_V=0$ et $\|w\|_W=0$, donc $v=0_V$ et $w=0_W$. Cela implique $(v,w)=(0_v,0_w)=0_{V\times W}$. 
		\item Pour tout $a$ dans $\eR$ et $(v,w)$ dans $V\times W$,  la norme $\|a (v,w)\|_{V\times W}$ est donnée par  $\max\{\|av\|_{V},\|aw\|_W\}$. On peut factoriser $\|av\|_{V}=|a|\|v\|_{V}$ et $\|aw\|_W=|a|\|w\|_W$ et donc $\|a (v,w)\|_{V\times W}=|a|\max\{\|v\|_{V},\|w\|_W\}=|a|\|(v,w)\|_{V\times W}$.
		\item Soient $(v_1,w_1)$ et $(v_2,w_2)$ dans $V\times W$. 
		\begin{equation}
			\begin{aligned}
				\|(v_1,w_1)+(v_2,w_2)\|_{V\times W}&=\max\{\|v_1+v_2\|_{V},\|w_1+w_2\|_W\}\\
				&\leq \max\{\|v_1\|_V+\|v_2\|_{V},\|w_1\|_W+\|w_2\|_W\}\\
				&\leq\max\{\|v_1\|_V,\|w_1\|_W\}+ \max\{\|v_2\|_{V},\|w_2\|_W\}\\
				&=\|(v_1,w_1)\|_{V\times W}+\|(v_2,w_2)\|_{V\times W}.
			\end{aligned}
		\end{equation}
	\end{itemize} 
\end{proof}

Toutes ces définitions se généralisent à un produit fini d'espaces vectoriels normés. Si les espaces \( V_i\) sont des espaces vectoriels normés, nous pouvons mettre sur le produit une topologie et une norme :
\begin{itemize}
    \item La topologie produit donnée en \ref{DefIINHooAAjTdY}
    \item La norme maximum \( \| v_1,\ldots, v_n \|_{max}=\max\{ \| v_1 \|,\ldots, \| v_n \| \}\). Dans le membre de droites, toutes les normes sont différentes.
\end{itemize}
Une question qui vient est la compatibilité entre ces deux constructions. Est-ce que la topologie associée à la norme maximum est le topologie produit ? Oui.

\begin{lemma}[\cite{ooALKGooMAzKpz}]        \label{LEMooWVVCooIGgAdJ}
    La topologie de la norme maximum est la topologie produit\footnote{Définition \ref{DefIINHooAAjTdY}.}.
\end{lemma}

\begin{proposition}[\cite{ooCUHNooNYIeGt}]      \label{PROPooQFTSooPFfbCc}
    Soient des espaces vectoriels normés \( V\) et \( W\) ainsi qu'une forme sesquilinéaire \( p\colon V\times W\to \eC\). Il y a équivalence des faits suivants.
    \begin{enumerate}
        \item
            \( \phi\) est continue.
        \item
            \( \phi\) est continue en \( (0,0)\)
        \item
            \( \phi\) est bornée
        \item
            Il existe \( C\geq 0\) telle que \( | \phi(x,y) |\leq C\| x \|\| y \|  \) pour tout \( (x,y)\in V\times W\).
    \end{enumerate}
    De plus la norme de \( \phi\) est alors donnée par
    \begin{equation}
        \| \phi \|=\min\{  C\geq 0\tq | \phi(x,y) |\leq C\| x \|\| y \|\forall (x,y)\in V\times W  \}.
    \end{equation}
\end{proposition}

On remarque tout de suite que la norme $\|.\|_\infty$ sur $\eR^2$ est la norme de l'espace produit $\eR\times\eR$. En outre cette définition nous permet de trouver plusieurs nouvelles normes dans les espaces $\eR^p$. Par exemple, si nous écrivons $\eR^4$ comme $\eR^2\times \eR^2$ on peut munir $\eR^4$ de la norme produit
\[
\|(x_1,x_2,x_3,x_4)\|_{\infty, 2}=\max\{\|(x_1,x_2)\|_\infty, \|(x_3,x_4)\|_2\}. 
\]    
Les applications de projection de l'espace produit $V\times W$ vers les espaces <<facteurs>>, $V$ $W$ sont notées $\pr_V$ et $\pr_W$ et sont définies par
\begin{equation}
	\begin{aligned}
		\pr_V\colon V\times W&\to V \\
		(v,w)&\mapsto v 
	\end{aligned}
\end{equation}
et
\begin{equation}
	\begin{aligned}
		\pr_W\colon V\times W &\to W \\
		(v,w)&\mapsto w. 
	\end{aligned}
\end{equation}
Les inégalités suivantes sont évidentes
\begin{equation}
	\begin{aligned}[]
		\|\pr_V(v,w)\|_V&\leq \|(v,w)\|_{V\times W} \\
		\|\pr_W(v,w)\|_W&\leq \|(v,w)\|_{V\times W}.
	\end{aligned}
\end{equation}
La topologie de l'espace produit est induite par les topologies des espaces <<facteurs>>. La construction est faite en deux passages : d'abord nous disons que une partie $A\times B$ de $V\times W$ est ouverte si $A$ et $B$ sont des parties ouvertes de $V$ et de $W$ respectivement.  Ensuite nous définissons que une partie quelconque de $V\times W$ est ouverte si elle est une intersection finie ou une réunion de parties ouvertes de $V\times W$ de la forme $A\times B$. 

Ce choix de topologie donne deux propriétés utiles de l'espace produit 
\begin{enumerate}
	\item
		Les projections sont des \defe{applications ouvertes}{application!ouverte}. Cela veut dire que l'image par $\pr_V$ (respectivement $\pr_W$) de toute partie ouverte de $V\times W$ est une partie ouverte de $V$ (respectivement $W$). 
	\item 
		Pour toute partir $A$ de $V$ et $B$ de $W$, nous avons $\Int (A\times B)=\Int A\times \Int B$.\label{PgovlABeqbAbB}
\end{enumerate}
Une propriété moins facile a prouver est que pour toute partie $A$ de $V$ et $B$ de $W$ nous avons  $\overline{A\times B}=\bar{A}\times \bar{B}$. Voir le lemme \ref{LemCvVxWcvVW}.
% position 26329
%et l'exercice \ref{exoGeomAnal-0009}.
  
Ce que nous avons dit jusqu'ici est valable pour tout produit d'un nombre fini d'espaces vectoriels normés. En particulier, pour tout $m>0$  l'espace  $\eR^m$ peut être considéré comme le produit de $m$ copies de $\eR$. 

\begin{example}
	Si $V$ et $W$ sont deux espaces vectoriels, nous pouvons considérer le produit $E=V\times W$. Les projections $\pr_V$ et $\pr_W$\nomenclature{$\pr_V$}{projection de $V\times W$ sur $V$}, définies dans la section \ref{sec_prod}, sont des applications linéaires. 

	En effet, la projection $\pr_V\colon V\times W\to V$ est donnée par $\pr_V(v,w)=v$. Alors,
	\begin{equation}
		\begin{aligned}[]
			\pr_V\big( (v,w)+(v',w') \big)&=\pr_V\big( (v+v'),(w+w') \big)\\
			&=v+v'\\
			&=\pr_V(v,w)+\pr_V(v',w'),
		\end{aligned}
	\end{equation}
	et
	\begin{equation}
		\pr_V\big( \lambda(v,w) \big)=\pr_V\big( (\lambda v,\lambda w) \big)=\lambda v=\lambda\pr_V(v,w).
	\end{equation}
	Nous laissons en exercice le soin d'adapter ces calculs pour montrer que $\pr_W$ est également une projection.
\end{example}

\begin{proposition} \label{PropDXR_KbaLC}
    Si \( \mO\) est un voisinage de \( (a,b)\) dans \( V\times W\) alors \( \mO\) contient un ouvert de la forme \( B(a,r)\times B(b,r)\).
\end{proposition}

\begin{proof}
    Vu que \( \mO\) est un voisinage, il contient un ouvert et donc une boule
    \begin{equation}
        B\big( (a,b),r \big)=\{ (v,w)\in V\times W\tq \max\{ \| v-a \|,\| w-b \| \}< r \}.
    \end{equation}
    Évidemment l'ensemble \( B(a,r)\times B(b,r)\) est dedans.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Suites}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons maintenant parler de suites dans $V\times W$. Nous noterons $(v_n,w_n)$ la suite dans $V\times W$ dont l'élément numéro $n$ est le couple $(v_n,w_n)$ avec $v_n\in V$ et $w_n\in W$. La notions de convergence de suite découle de la définition de la norme via la définition usuelle \ref{DefCvSuiteEGVN}. Il se fait que dans le cas des produits d'espaces, la convergence d'une suite est équivalente à la convergence des composantes. Plus précisément, nous avons le lemme suivant.
\begin{lemma}		\label{LemCvVxWcvVW}
	La suite $(v_n,w_n)$ converge vers $(v,w)$ dans $V\times W$ si et seulement les suites $(v_n)$ et $(w_n)$ convergent séparément vers $v$ et $w$ respectivement dans $V$ et $W$. 
\end{lemma}

\begin{proof}
	Pour le sens direct, nous devons étudier le comportement de la norme de $(v_n,w_n)-(v,w)$ lorsque $n$ devient grand. En vertu de la définition de la norme dans $V\times W$ nous avons
	\begin{equation}
		\Big\| (v_n,w_n)-(v,w) \Big\|_{V\times W}=\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}.
	\end{equation}
	Soit $\varepsilon>0$. Par définition de la convergence de la suite $(v_n,w_n)$, il existe un $N\in\eN$ tel que $n>N$ implique
	\begin{equation}
		\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}<\varepsilon,
	\end{equation}
	et donc en particulier les deux inéquations
	\begin{subequations}
		\begin{align}
			\| v_n-v \|&<\varepsilon\\
			\| w_n-w \|&<\varepsilon.
		\end{align}
	\end{subequations}
	De la première, il ressort que $(v_n)\to v$, et de la seconde que $(w_n)\to w$.

	Pour le sens inverse, nous avons pour tout $\varepsilon$ un $N_1$ tel que $\| v_n-v \|_V\leq\varepsilon$ pour tout $n>N_1$ et un $N_2$ tel que $\| w_n-w \|_W\leq\varepsilon$ pour tout $n>N_2$. Si nous posons $N=\max\{ N_1,N_2 \}$ nous avons les deux inégalités simultanément, et donc
	\begin{equation}
		\max\big\{ \| v_n-v \|_V,\| w_n-w \|_W \big\}<\varepsilon,
	\end{equation}
	ce qui signifie que la suite $(v_n,w_n)$ converge vers $(v,w)$ dans $V\times W$.
\end{proof}

\begin{remark}		\label{RemTopoProdPasRm}
	Il faut remarquer que la norme \eqref{EqNormeVxWmax} est une norme \emph{par défaut}. C'est la norme qu'on met quand on ne sait pas quoi mettre. Or il y a au moins un cas d'espace produit dans lequel on sait très bien quelle norme prendre : les espaces $\eR^m$. La norme qu'on met sur $\eR^2$ est
	\begin{equation}
		\| (x,y) \|=\sqrt{x^2+y^2},
	\end{equation}
	et non la norme «par défaut» de $\eR^2=\eR\times\eR$ qui serait
	\begin{equation}
		\| (x,y) \|=\max\{ | x |,| y | \}.
	\end{equation}
	Les théorèmes que nous avons donc démontré à propos de $V\times W$ ne sont donc pas immédiatement applicables au cas de $\eR^2$.

	Cette remarque est valables pour tous les espaces $\eR^m$. À moins de mention contraire explicite, nous ne considérons jamais la norme par défaut \eqref{EqNormeVxWmax} sur un espace $\eR^m$.
\end{remark}

Étant donné la remarque \ref{RemTopoProdPasRm}, nous ne savons pas comment calculer par exemple la fermeture du produit d'intervalle $\mathopen] 0,1 ,  \mathclose[\times\mathopen[ 4 , 5 [$. Il se fait que, dans $\eR^m$, les fermetures de produits sont quand même les produits de fermetures.

\begin{proposition}		\label{PropovlAxBbarAbraB}
	Soit $A\subset\eR^m$ et $B\subset\eR^m$. Alors dans $\eR^{m+n}$ nous avons $\overline{ A\times B }=\bar A\times \bar B$.
\end{proposition}

La démonstration risque d'être longue; nous ne la faisons pas ici.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Applications multilinéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Application multilinéaire]       \label{DefFRHooKnPCT}
    Une application $T: \eR^{m_1}\times \ldots \times\eR^{m_k}\to\eR^p $ est dite \defe{\( k\)-linéaire}{application!multilinéaire} si pour tout $X=(x_1, \ldots,x_k)$ dans $ \eR^{m_1}\times \ldots \times\eR^{m_k}$ les applications $x_i\mapsto T(x_1, \ldots, x_i,\ldots,x_k)$ sont linéaires pour tout $i$ dans $\{1,\ldots,k\}$, c'est à dire
	\begin{equation}
		\begin{aligned}[]
			T(\cdot,x_2, \ldots, x_i,\ldots,x_k)&\in \mathcal{L}(\eR^{m_1}, \eR^p),\\
			T(x_1,\cdot, \ldots, x_i,\ldots,x_k)&\in \mathcal{L}(\eR^{m_2}, \eR^p),\\
						& \vdots\\
			T(x_1, \ldots, x_i,\ldots,x_{k-1},\cdot)&\in \mathcal{L}(\eR^{m_k}, \eR^p).\\
		\end{aligned}
	\end{equation}
	En particulier lorsque $k=2$, nous parlons d'applications \defe{bilinéaires}{bilinéaire}. Vous pouvez deviner ce que sont les applications \emph{tri}linéaire ou \emph{quadri}linéaire.
\end{definition}

L'ensemble des applications $k$-linéaires de $ \eR^{m_1}\times \ldots \times\eR^{m_k}$ dans $\eR^p$ est noté $\mathcal{L}(\eR^{m_1}\times \ldots \times\eR^{m_k}, \eR^p)$ ou $\mathcal{L}(\eR^{m_1}, \ldots,\eR^{m_k}; \eR^p)$.

\begin{example}
  Soit $A$ une matrice avec $m$ lignes et $n$ colonnes. L'application bilinéaire de $\eR^m\times \eR^n$ dans $\eR$ associée à $A$ est définie par
\[
T_A(x,y)= x^TAy=\sum_{i,j}a_{i,j}x_i y_j, \qquad \forall x\in \eR^m, \, y \in \eR^n.
\]
\end{example}

Nous énonçons la proposition suivante dans le cas d'espaces vectoriels normés\footnote{Sans hypothèses sur la dimension.} parce que nous allons l'utiliser dans ce cas, mais le cas particulier \( E_i=\eR^{m_i}\) et \( F=\eR^p\) est important.
\begin{proposition} \label{PropUADlSMg}
    Soient des espaces vectoriels normés \( E_i\) et \( F\). Une application \( n\)-linéaire
    \begin{equation}
        T\colon E_1\times\ldots\times E_n\to F
    \end{equation}
    est est continue si et seulement s'il existe un réel $L\geq 0$ tel que
  \begin{equation}\label{limitatezza}
     \|T(x_1, \ldots,x_n)\|_F\leq L \|x_1\|_{F_1}\cdots\|x_n\|_{F_n}, \qquad \forall x_i\in E_i.
  \end{equation}
\end{proposition}

\begin{proof}
    Pour simplifier l'exposition nous nous limitons au cas $n=2$ et nous notons $T(x,y)=x*y$

    Supposons que l'inégalité \eqref{limitatezza} soit satisfaite. 
    \begin{equation}\label{LimImplCont}
      \begin{aligned}
        \|x*y-x_0*y_0\|&=\|(x-x_0)*y-x_0*(y-y_0)\|\\
    &\leq \|(x-x_0)*y\|+\|x_0*(y-y_0)\|\\
    &\leq L\|x-x_0\|\|y\| + L\|x_0\|\|y-y_0\|.
      \end{aligned}
    \end{equation}
    Si $x\to x_0$ et $y\to y_0$  on voit que $T$ est continue en passant à la limite aux deux côtes de l'inégalité \eqref{LimImplCont}.

    Soit $T$ continue en $(0,0)$. Évidemment\footnote{Dans la formule suivante, les trois zéros sont les zéros de trois espaces différents.} $0*0=0$, donc il existe $\delta>0$ tel que si $x\in B_{E_1}(0,\delta)$ et $y\in B_{E_2}(0,\delta)$ alors $\|x*y\|\leq 1$. En particulier si \( (x,y)\in B_{E_1\times E_2}(0,\delta)\) nous sommes dans ce cas. Soient maintenant  $x\in E_1\setminus\{ 0 \}$  et $y\in E_2\setminus\{ 0\}$
    \begin{equation}
        x*y=\left(\frac{\|x\|}{\delta}\frac{\delta x}{\|x\|}\right)*\left(\frac{\|y\|}{\delta}\frac{\delta y}{\|y\|}\right)
    =\frac{\|x\|\|y\|}{\delta^2} \left(\frac{\delta x}{\|x\|}\right)*\left(\frac{\delta y}{\|y\|}\right).
     \end{equation}
    On remarque que $\delta x/\|x\|_m$ est dans la boule de rayon $\delta$ centrée en $0_m$ et que $\delta y/\|y\|_n$ est dans la boule de rayon $\delta$ centrée en $0_n$. On conclut 
    \[
     x*y\leq \frac{\|x\|_m\|y\|_n}{\delta^2}.
    \]
    Il faut prendre $L=1/\delta^2$.
\end{proof}

La norme de \( T\) est alors définie comme la plus petite constante \( L\) qui fait fonctionner la proposition \ref{PropUADlSMg}.
\begin{definition}  \label{DefKPBYeyG}
	La norme sur l'espace $\aL(E_1\times \cdots\times E_n, F)$ des applications $k$-linéaires et continues est 
	\begin{equation}
        \|T\|_{E_1\times \ldots\times E_n}=\sup\{ \|T(u_1, \ldots,u_k)\|_{F}\,\vert\,\|u_i\|_{E_i}\leq 1, i=1,\ldots, k \}.
	\end{equation}
\end{definition}
Nous avons donc automatiquement
\begin{equation}    \label{EqYLnbRbC}
    \| T(u,v) \|\leq \| T \|\| u \|\| v \|.
\end{equation}
Et nous notons que cette norme est uniquement définie pour les applications linéaires continues. Ce n'est pas très grave parce qu'alors nous définissons \( \| T \|=\infty\) si \( T\) n'est pas continue. Cela pour retrouver le principe selon lequel on est continue si et seulement si on est borné.

\begin{proposition}\label{isom_isom}
  On définit les fonctions
  \begin{equation}
    \begin{array}{rccc}
      \omega_g: & \mathcal{L}(\eR^{m}\times\eR^{n}, \eR^p)&\to &\mathcal{L}(\eR^{m}, \mathcal{L}(\eR^{n}, \eR^p)),\\
      \omega_d: & \mathcal{L}(\eR^{m}\times\eR^{n}, \eR^p)&\to &\mathcal{L}(\eR^{n}, \mathcal{L}(\eR^{m}, \eR^p)),
    \end{array}
  \end{equation}
par 
\[
\omega_g(T)(x)=T(x,\cdot), \qquad \forall x\in\eR^m,
\]
et
\[
\omega_d(T)(y)=T(\cdot, y), \qquad \forall y\in\eR^n.
\]
Les fonctions $\omega_g$ et $\omega_d$ sont des isomorphismes qui préservent les normes.    
\end{proposition}
