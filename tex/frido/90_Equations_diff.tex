% This is part of Mes notes de mathématique
% Copyright (c) 2008-2009,2011-2017
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Autour de Cauchy-Lipschitz}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooNKICooDnOFTD}

Dans cette section nous étudions les équations différentielles du type
\begin{subequations}
    \begin{numcases}{}
        y'(t)=f\big( t,y(t) \big)\\
        y(t_0)=y_0
    \end{numcases}
\end{subequations}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Fuite des compacts et explosion en temps fini}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Fuite des compacts\cite{GPRooZkclFA,ZPNooLNyWjX}]
Nous considérons l'équation différentielle 
\begin{subequations}
    \begin{numcases}{}
        y'(t)=f\big( t,y(t) \big)\\
        y(t_0)=y_0
    \end{numcases}
\end{subequations}
où \( f\colon I\times \Omega\to \eR^n\) est continue et \( \Omega\) ouvert dans \( \eR^n\). Soit la solution maximale \( y_M\colon J_M=\mathopen] t_{min} , t_{max} \mathclose[\to \Omega\). Si \( t_{max}<\sup(I)\) alors \( y_M(t)\) sort de tout compact de \( \Omega\) lorsque \( t\to t_{max}\).
\end{theorem}
\index{théorème!fuite des compacts}

\begin{proof}
Soit \( K\) un compact de \( \Omega\) et nous considérons une suite \( (t_m)\) dans \( \mathopen] t_{min} , t_{max} \mathclose[\) telle que \( t_m\to t_{max}\). Si nous supposons que \( y_M(t)\) ne sort pas de \( K\) alors nous avons \( y_M(t_m)\in K\), c'est à dire une suite dans un compact. Quitte à passer à une sous-suite, nous supposons qu'elle est convergente. Soit \( x_1\in K\) la limite \( \lim_{m\to \infty}y_M(t_m)=x_1\).

    Vu que \( t_{max}\in I\), la condition initiale \( y(t_{max})=x_1\) est valide et le théorème de Cauchy-Lipschitz \ref{ThokUUlgU} nous donne une unique solution maximale \( y_P\) définie sur un ouvert \( J_P\) autour de \( t_{max}\).

    Nous allons maintenant construire une solution au problème initial qui contredit la maximalité de \( y_M\). Attention : il n'est pas évident a priori que \( y_P(t)=y_M(t)\) sur l'intersection des domaines. Si c'était évident, la proposition serait démontrée.

    Soit \( \tilde J=J_M\cup J_P\cap\mathopen] t_{min} , +\infty \mathclose[\) et la fonction
        \begin{equation}
            \tilde y(t)=\begin{cases}
                y_M(t)    &   \text{si } t<t_{max}\\
                y_P(t)    &    \text{si } t\geq t_{max}.
            \end{cases}
        \end{equation}
        La fonction \( \tilde y\) est continue par construction parce que
        \begin{equation}
            \lim_{t\to t_{max}} y_M(t)=x_1=y_P(t_{max}).
        \end{equation}
        Nous vérifions à présent que \( \tilde y\) est une solution : \( \tilde y'(t_{max})=f\big( t_{max},y(t_{max}) \big)\) :
        \begin{subequations}
            \begin{align}
                \lim_{\epsilon\to 0}\frac{ \tilde y(t_{max}-\epsilon)-\tilde y(t_{max}) }{ \epsilon }&=\lim_{\epsilon\to 0}\frac{ y_M(t_{max}-\epsilon)-y_P(t_{max}) }{ \epsilon }\\
                &=\lim_{\epsilon\to 0}\frac{ y_M(t_{max}-\epsilon)-y_P(t_{max}-\epsilon)+y_P(t_{max}-\epsilon)-y_P(t_{max}) }{ \epsilon }\\
                &=\lim_{\epsilon\to 0}\frac{ y_P(t_{max}-\epsilon)-y_P(t_{max}) }{ \epsilon }\\
                &=y'_P(t_{max}).
            \end{align}
        \end{subequations}
        Donc \( \tilde y\) est solution pour la condition initiale \( \tilde y(t_{max})=x_1\) et coïncide avec \( y_P\) en \( t_{max}\) et avec \( y_M\) avant \( t_{max}\). Donc en réalité \( y_P\), \( y_M\) et \( \tilde y\) sont identiques et cela contredit la maximalité de \( y_M\).
\end{proof}

\begin{corollary}[Explosion en temps fini]      \label{CorGDJQooNEIvpp}
    Soit \( (y_m,J)\) la solution maximale du problème de Cauchy \eqref{XtiXON} :
    \begin{subequations}      
        \begin{numcases}{}
            y'=f(t,y)\\
            y(t_0)=y_0
        \end{numcases}
    \end{subequations}
    avec \( f\colon U=I\times \Omega\to \eR^n\) où \( I\) est ouvert dans \( \eR\) et \( \Omega\) ouvert dans \( \eR^n\). Nous supposons que \( f\) est continue sur \( U\) et localement Lipschitz par rapport à \( y\).
    
    Si la solution maximale est définie sur \( J=\mathopen] t_{min} , t_{max} \mathclose[\) alors nous avons l'alternative suivante :
    \begin{enumerate}
        \item   \label{ItemOLYYooJVkRfj}
            Soit \( t_{max}=\sup(I)\),
        \item       \label{ITEMooUKFAooXwRNSB}
            soit \( t_{max}<\sup(I)\) et \( \lim_{t\to t_{max}}  \| y(t) \|= \infty\).
    \end{enumerate}

    Le résultat tient aussi \emph{mutatis mutandis} pour \( t_{\min}\).
\end{corollary}

\begin{remark}
    Attention : ceci n'est pas une simple paraphrase de la fuite des compacts. L'information supplémentaire que ce corollaire donne est que la solution sort de tout compact \emph{pour ne plus y retourner}.
\end{remark}

\begin{proof}
    L'hypothèse \( t_{max}<\sup(I)\) signifie que la solution finit d'exister avant que les hypothèses sur \( f\) cessent d'être vraies. C'est à dire que la solution maximale est moindre que ce que nous aurions pu espérer.

Soit \( K\) compact et supposons que que pour tout \( t_0<t_{max}\) il existe \( t\in\mathopen] t , t_{max} \mathclose[\) tel que \( y_M(t)\in K\). Alors cela crée une suite \( t_k\) dans \( J\) telle que \( y_M(t_k)\) est dans \( K\). Comme dans le théorème de la fuite des compacts nous concluons l'impossibilité de la chose.

    Donc pour tout compact \( K\) de \( \Omega\), il existe \( T<t_{max}\) tel que \( y_M(t)\in \Omega\setminus K\) pour tout \( t\in\mathopen[ T , t_{max} [\). En prenant des boules fermées de plus en plus grandes en guise de compacts nous concluons que
        \begin{equation}
            \lim_{t\to t_{max}} \| y_M(t) \|=\infty.
        \end{equation}
\end{proof}

\begin{normaltext}      \label{NORMooZROGooZfsdnZ}
    Notons que si \( t_{max}<\infty\), si nous sommes dans l'alternative \ref{CorGDJQooNEIvpp}\ref{ITEMooUKFAooXwRNSB} et si la solution maximale \( y\) est de classe \( C^1\) (ce qui est le cas lorsqu'on utilise Cauchy-Lipschitz \ref{ThokUUlgU}) alors la dérivée de \( y\) est également non bornée dans un voisinage de \( t_{max}\).

    Mais si \( f\) est globalement bornée, alors dans l'équation \( y'=f(t,y)\), la dérivée \( y'\) sera globalement bornée. Dans ce cas, la solution ne peut pas exploser en temps fini et existe donc globalement.
\end{normaltext}

\begin{probleme}
    Êtes-vous d'accord avec \ref{NORMooZROGooZfsdnZ} ?
\end{probleme}

\begin{example}
    Soit l'équation différentielle
    \begin{subequations}
        \begin{numcases}{}
            y'=y(y-1)\sin(yt)\\
            y(0)=\frac{ 1 }{2}.
        \end{numcases}
    \end{subequations}
    La fonction \( f(t,y)=y(y-1)\sin(yt)\) ayant une dérivée bornée partout, est localement Lipschitz et le théorème de Cauchy-Lipschitz \ref{ThokUUlgU} s'applique. Pour toute condition initiale, une solution maximale unique existe.

    Si nous oublions la condition initiale, il est facile de trouver des solutions constantes : \( y'=0\) avec \( y(t)=k\) donne l'équation
    \begin{equation}
        0=k(k-1)\sin(kt).       
    \end{equation}
    Les solutions \( y_1(t)=0\) et \( y_2(t)=1\) sont des solutions existant pour tout \( t\).

    Le graphe de la solution correspondante à la condition initiale \( y(0)=\frac{ 1 }{2}\) ne pouvant pas croiser les graphes de \( y_1\) et \( y_2\), elle est obligée d'exister pour tout \( t\) parce qu'elle ne peut pas exploser en temps fini.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Écart entre deux conditions initiales}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{ooBSUCooIKClhZ,ooUQOJooSPNjlt}]      \label{PROPooOPRRooQgYFDk}
    Soit une fonction \( f\colon I\times \Omega\to \eR^n\) continue et globalement Lipschitz en sa seconde variable (\( \Omega\) est un ouvert de \( \eR^n\)). Soient deux solutions \( y_1\colon I_1\to \eR^n\) et \( y_2\colon I_2\to \eR^n\) aux problèmes
    \begin{subequations}
        \begin{numcases}{}
            y_i'(t)=f\big( t,y_i(t) \big)\\
            y_i(t_0)=a_i
        \end{numcases}
    \end{subequations}
    Alors pour tout \( t\in I_1\cap I_2\) nous pouvons estimer l'écart entre \( y_1\) et \( y_2\) par la formule
    \begin{equation}
        \| y_1(t)-y_2(t) \|\leq  e^{L| t-t_0 |}\| a_1-a_2 \|
    \end{equation}
    où \( L\) est la constante de Lipschitz de \( f\).
\end{proposition}

\begin{proof}
    Nous avons d'abord les majorations suivantes, qui semblent juste jouer avec les notations, mais qui utilisent le fait (contenu dans le théorème de Cauchy-Lipschitz) que \( y_i\) soit de classe \( C^1\) :
    \begin{subequations}
        \begin{align}
            \| y_1(t)-y_2(t) \|&=\| \int_{t_0}^t\big( y'_1(s)-y'_1(s) \big) \|\\
            &\leq L\int_{t_0}^t\| f\big( s,y_1(s) \big)-f\big( s,y_2(s) \big) \|ds\\
            &=L\int_{t_0}^t\| y_1(s)-y_2(s) \|ds
        \end{align}
    \end{subequations}
    C'est à ce moment que nous utilisons le lemme de Grönwall. Vu que
    \begin{equation}
            \| y_1(t)-y_2(t) \|\leq L\int_{t_0}^t\| y_1(s)-y_2(s) \|ds,
    \end{equation}
    nous sommes dans les hypothèses de Grönwall \ref{LEMooUGZGooCczAmKa} en posant
    \begin{subequations}
        \begin{align}
            u(t)&=\| y_1(t)-y_2(t) \|\\
            b(t)&=\| y_1(0)-y_2(0) \|\\
            a(t)&=L.
        \end{align}
    \end{subequations}
    Nous avons la majoration
    \begin{equation}
        \| y_1(t)-y_2(t) \|\leq \| y_1(0)-y_2(0) \|+L\int_0^t\| y_1(0)-y_2(0) \| e^{L(t-s)}ds.
    \end{equation}
    Le calcul de l'intégrale intérieure donne
    \begin{equation}
        \int_0^t e^{L(t-s)}ds=-\frac{1}{ L }( e^{-Lt}-1). 
    \end{equation}
    Avec ça, nous avons
    \begin{equation}
        \| y_1(t)-y_2(t) \|\leq  e^{Lt}\| y_1(0)-y_2(0) \|.
    \end{equation}
\end{proof}

\begin{normaltext}
    Notons que la proposition \ref{PROPooOPRRooQgYFDk} est plutôt une mauvaise nouvelle parce que les solutions restent seulement linéairement proches l'une de l'autre lorsqu'on rapproche les conditions initiales, mais elle divergent exponentiellement vite avec le temps. Donc deux trajectoires arbitrairement proches au départ finissent assez vite par être bien séparées.

   Cette proposition est cependant cruciale parce qu'elle explique que pour des petits \( t\), les solutions ne s'écartent pas beaucoup, c'est à dire que pour \( t\) fixé, l'application qui à une donnée initiale fait correspondre la solution en \( t\) est continue. C'est le premier pas pour parler de régularité du flot.

\end{normaltext}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Flot d'un champ de vecteurs}
%---------------------------------------------------------------------------------------------------------------------------

Nous reprenons l'équation différentielle du théorème de Cauchy-Lipschitz \ref{ThokUUlgU}. En ce qui concerne les notations, \( I\) est un intervalle ouvert de \( \eR\) contenant \( 0\) et l'application \( f\colon I\times \eR^n\to \eR^n \) est continue et localement Lipschitz en sa seconde variable. Pour \( a\in \eR^n\), nous notons \( (J_a,y_a)\) la solution maximale (donc \( y_a\colon J_a\to \eR^n\)) du problème
\begin{subequations}        \label{EQooADJMooRAZWfm}
    \begin{numcases}{}
        y_a(t)=f\big( t,y_a(t) \big)\\
        y_a(0)=a.
    \end{numcases}
\end{subequations}
Nous noterons aussi de temps en temps \( \varphi(t,a)=y_a(t)\).

Nous savons que \( t\mapsto y_a(t)\) est de classe \( C^1\), et cela est directement dans le théorème de Cauchy-Lipschitz. Une question d'une toute autre difficulté est la régularité de \( a\mapsto y_a(t)\) pour \( t\) fixé, et encore pire : celle de \( (t,a)\mapsto y_a(t)\).

Il se fait que l'application \( (t,a)\mapsto y_a(t)\) a la même régularité que celle de \( f\), mais cela va être un peu long à prouver. En ce qui concerne la régularité \( C^1\), ce sera le théorème \ref{THOooSTHXooXqLBoT} dont la démonstration, comme vous pouvez le voir sera copieuse et demandera des propositions intermédiaires pas simples.

\begin{definition}
    Si \( t\) est fixé, l'application
    \begin{equation}
        \begin{aligned}
            \varphi_t\colon \eR^n&\to \eR^n \\
            x&\mapsto \varphi(t,x) = y_x(t)
        \end{aligned}
    \end{equation}
    est le \defe{flot}{flot} du problème de Cauchy \eqref{EQooADJMooRAZWfm}.

    L'application \( t\mapsto \varphi_t\) est ce qui est appelé le groupe à un paramètre de flot, pour des raisons qui arriverons plus tard\quext{ou pas\ldots}.
\end{definition}

Le but est d'étudier les propriétés du flot : est-il continu, une difféomorphisme, existe, pour quels \( t\) ? Où se cache le champ de vecteurs du titre dans l'équation différentielle ?

Nous posons
\begin{equation}
    \mD=\bigcup_{x\in\Omega}\big( J_x\times \{ x \} \big).
\end{equation}

Comme tout produit d'espaces métrique, l'ensemble \( \mD\) est muni d'une métrique via la définition \ref{DefZTHxrHA}.

\begin{proposition}[\cite{ooBSUCooIKClhZ}]      \label{PROPooUDQWooNFrNOQ}
    Soit un intervalle \( I\) ouvert de \( \eR\) contenant \( 0\) et \( \Omega\) un ouvert connexe de \( \eR^n\). Soit une application \( f\colon I\times \eR^n\to \eR^n \) continue et localement Lipschitz en sa seconde variable. Pour \( a\in \eR^n\), nous notons \( (J_a,y_a)\) la solution maximale (donc \( y_a\colon J_a\to \eR^n\)) du problème
    \begin{subequations}     
        \begin{numcases}{}
            y_a(t)=f\big( t,y_a(t) \big)\\
            y_a(0)=a.
        \end{numcases}
    \end{subequations}

    Nous posons
    \begin{equation}
        \mD=\bigcup_{x\in\Omega}\big( J_x\times \{ x \} \big)
    \end{equation}
    Nous définissons la fonction \( \varphi\) par \( \varphi(t,x)=y_x(t)\) là où ça existe.

    L'ensemble \( \mD\) est ouvert. L'application \( \varphi\colon \mD\to \Omega\) est localement Lipschitz.
\end{proposition}

\begin{proof}
    Soit \( (s,a)\in\mD\) et \( (J_a,y_a)\) la solution maximale passant par \( a\) en \( t=0\). Par définition de \( \mD\) nous avons \( s\in J_a\). Nous considérons \( J\), un compact inclus à \( J_a\) et contenant \( 0\) et \( s\) en son intérieur. Nous posons
    \begin{equation}
        K=J\times y_a(J).
    \end{equation}
    Vu que \( y_a\) est continue, cela est un compact. Chaque point de \( K\) possède un voisinage ouvert sur lequel \( f\) est Lipschitz\footnote{Cela est à peu près la définition d'être localement Lipschitz : \ref{DefJSFFooEOCogV}, voir aussi \ref{NORMooYNRAooBgobcK}.}; nous considérons un sous recouvrement fini et le maximum des constantes de Lipschitz. Cela nous crée un voisinage \( V\) de \( K\) dans \( I\times \Omega\) dans lequel \( f\) est Lipschitz.

    Vu que \( V\) est ouvert et \( K\) est compact avec \( K\subset V\), nous pouvons trouver un ouvert \( V'\) et un compact \( K'\) tels que
    \begin{equation}
        K\subset V'\subset K'\subset V.
    \end{equation}
    Sur ce \( V'\), la fonction \( f\) est de plus bornée parce que continue sur le compact \( K'\). Nous renommons \( V'\) en \( V\). Sur \( V\) nous avons :
    \begin{itemize}
        \item \( \| f \|_{\infty,V}\leq M\),
        \item \( f\) est Lipschitz en sa seconde variable, de constante de Lipschitz \( L\).
    \end{itemize}
    

    En tant qu'espace produit, nous avons une distance sur \( I\times \Omega\) donnée en \ref{DefZTHxrHA} :
    \begin{equation}
        d\big( (t,y),(t',y') \big)=\max\big\{  | t-t' |,\| y-y' \|   \big\}.
    \end{equation}
    Nous posons
    \begin{subequations}
        \begin{align}
            V_{\epsilon}(K)=\{ z\in I\times \Omega \tq\,d(z,K)<\epsilon  \}\\
            W_{\epsilon}=\{ (t,y)\in J\times \Omega\tq\,\| y- y_a(t) \|<\epsilon \}.
        \end{align}
    \end{subequations}
    
    \begin{subproof}
    \item[\( \overline{ W_{\epsilon} }\subset \overline{  V_{\epsilon}(K) }\)]

        Soit \( (t,y)\in W_{\epsilon}\). Nous avons :
        \begin{subequations}
            \begin{align}
                d\big( (t,y),K \big)&=\inf_{(t',y')\in K}d\big( (t,y),(t',y') \big)\\
                &=\inf_{(t',y')\in K}\max\{ | t-t' |,\| y-y' \| \}.     \label{SUBEQooUCZXooRpAitk}
            \end{align}
        \end{subequations}
        Mais demander \( (t,y)\in W_{\epsilon}\) signifie que \( t\in J\) et \( \| y-y_a(t) \|\leq \epsilon\). Dans \( K \) nous avons l'élément \( \big( t,y_a(t) \big)\) qui vérifie
        \begin{equation}
            d\big( (t,y),(t,y_a(t)) \big)=\| y-y_a(t) \|\leq \epsilon.
        \end{equation}
        Donc l'infimum de \eqref{SUBEQooUCZXooRpAitk} est majoré par \( \epsilon\). Nous avons prouvé que \( W_{\epsilon}\subset V_{\epsilon}(K)\) et donc même inclusion pour les fermetures.

    \item[Il existe \( \epsilon>0\) tel que \( \overline{ V_{\epsilon}(K) }\subset V\)]

        Supposons que \( \overline{ V_{\epsilon}(K) }\) ne soit inclus à \( V\) pour aucun \( \epsilon\). Alors nous considérons 
        \begin{equation}
            z_n\in \overline{ V_{1/n}(K) }\setminus V.
        \end{equation}
        Nous avons par définition \( d(z_n,K)\leq \frac{1}{ n }\). Vu que \( K\) est compact, il comprend (au moins) un élément réalisant la distance : soit \( z'_n\in K\) tel que
        \begin{equation}
            d(z_n,z'_n)=d(z_n,K).
        \end{equation}
        Nous avons \( d(z_n,z'_n)\to 0\), de telle sorte que les valeurs d'adhérence de \( (z_n)\) et \( (z'_n)\) sont les mêmes. Et comme \( (z'_n)\) est une suite dans un compact, elle a des valeurs d'adhérence. Soit \( z_{\infty}\) l'une d'elles. Vu que c'est une valeur d'adhérence d'une suite contenue dans le compact \( K\), elle est également dans \( K\) : \( z_{\infty}\in K\). Mais en même temps, \( z_n\) est hors de l'ouvert \( V\), et donc dans le fermé \( V^c\). Les valeurs d'adhérences restent dans le fermé, c'est à dire \( z_{\infty}\notin V\). Vu que \( K\subset V\), il y a contradiction.

        Donc il existe \( \epsilon>0\) tel que \( \overline{ V_{\epsilon}(K) }\subset V\).

    \item[Il existe \( \epsilon\) tel que \( \overline{ W_{\epsilon} }\subset V\)]

        Il suffit de prendre le \( \epsilon\) dont nous venons de parler pour avoir
        \begin{equation}
            \overline{ _{\epsilon} }\subset \overline{ V_{\epsilon}(K) }\subset V.
        \end{equation}
        
    \end{subproof}
    Soit le \( \epsilon\) en question, et \( T>0\) tel que \( J\subset \mathopen[ -T , T \mathclose]\). Nous posons \( r=\epsilon e^{-LT}\). Soit \( b\in \overline{ B(a,r) } \) et\quext{Dans \cite{ooBSUCooIKClhZ}, l'ensemble \(X \) est mal défini parce qu'il y a une erreur «de type» évidente. J'espère que ce que je donne ici est une correction qui fonctionne bien. Soyez plus attentif que jamais.}
    \begin{equation}
    X=\{ \tau\in J_{+}\tq\,\mathopen] 0 , \tau \mathclose]\subset J_b\text{ et }    \big( t,y_b(t) \big)\in \overline{ W_{\epsilon} }\,\forall\,t\in\mathopen[ 0 , \tau \mathclose]     \}.
    \end{equation}
Nous allons prouver que \( X=J_{+}\) en prouvant qu'il est ouvert, fermé et non vide dans \( J_+=J\cap\mathopen] 0 , \infty \mathclose[\). Nous parlons bien de la topologie de \( J_+\), celle induite\footnote{Définition \ref{DefVLrgWDB}.} de \( \eR\). Vu que \( 0\in J\), l'ensemble \( J_+\) est ouvert à gauche, mais comme il est compact, il ne va certainement pas jusqu'à \( +\infty\), de telle sorte qu'il est fermé à gauche. Les ouverts de \( J_+\) sont les ensembles de la forme \( \mO\cap J_+\) où \( \mO\) est ouvert de \( \eR\). Il y en a de la forme \( \mathopen] 0 , m \mathclose]\).

    \begin{subproof}
        \item[\( X\) est fermé] 

            C'est parce que \( \overline{ W_{\epsilon} }\) et \( J_b\) sont fermés.


        \item[\( X\) est ouvert] 

            Soit \( \tau\in X\). Si \( \tau=\sup J_+\) alors \( X=J_+\) est un ouvert de \( J_+\). Supposons donc que \( 0<\tau<\sup J_+\). Dans ce cas nous avons
            \begin{equation}
                \big( \tau,y_b(\tau) \big)\in\overline{ W_{\epsilon} }\subset V,
            \end{equation}
            et nous pouvons résoudre localement le problème de Cauchy
            \begin{subequations}
                \begin{numcases}{}
                    y'(t)=f\big( t,y(t) \big)\\
                    y(\tau)=\varphi(\tau,b)=y_b(\tau).
                \end{numcases}
            \end{subequations}
            Ce \( y\) existe jusqu'à \( \tau+\eta\) (pour au moins un petit \( \eta\)), et par l'unicité de la solution, \( y=y_b\) sur \( \mathopen[ \tau , \tau+\eta \mathclose[\). Ceci pour dire que le flot \( \varphi(.,b)\) existe au moins jusqu'à \( \tau+\eta\).

                Grâce à la proposition \ref{PROPooOPRRooQgYFDk} nous pouvons évaluer
                \begin{equation}
                    \| \varphi(\tau,b)-\varphi(\tau,a) \|=\| y_a(\tau)-y_b(\tau) \|\leq  e^{L\tau}\| b-a \|.
                \end{equation}
                Comme nous avions choisi \( r=\epsilon e^{-LT}\) et \( b\in\overline{ B(a,r) }\) nous avons aussi \( \| b-a \|\leq \epsilon e^{-LT}\) et donc
                \begin{equation}
                    \| \varphi(\tau,b)-\varphi(\tau,a) \|\leq\epsilon e^{L(\tau-T)}<\epsilon
                \end{equation}
                parce que nous avions \( \tau<\sup J_+\leq T\), ce qui garantit que \(  e^{L(\tau-T)}<1\).

                Est-ce que ceci nous garantit que \( \tau+\eta\in X\) ? Il faudrait \( \big( \tau+\eta,y_b(\tau+\eta) \big)\in \overline{ W_{\epsilon} }\), c'est à dire \(  \| y_b(\tau+\eta)-y_a(\tau+\eta) \|\leq\epsilon   \). L'ensemble \( J_+\) étant fermé dans l'ouvert \( J_a\), ce dernier déborde certainement. Prenons donc \( \eta\) assez petit pour que \( y_a\) existe jusqu'en \( \tau+\eta\). 

                Vu que \( y_a\) et \( y_b\) sont continues, et qu'en \( \tau\) elles sont distantes de moins de \( \epsilon\), en \( \tau+\eta\), elles restent distantes de moins de \( \epsilon\) (quitte à prendre encore \( \eta\) plus petit).

                Ceci nous permet de conclure que \( X\) est ouvert.

            \item[\( X\) est non vide]

                La solution \( y_b\) au problème
                \begin{subequations}
                    \begin{numcases}{}
                        y_b'(t)=f\big( t,y_b(t) \big)\\
                        y_b(0)=b
                    \end{numcases}
                \end{subequations}
                existe au moins localement et vérifie \( \| y_b(0)-y_a(0) \|=\| b-a \|\leq \epsilon e^{-LT}<\epsilon\). Par continuité nous avons
                \begin{equation}
                    \| y_b(t)-y_a(t) \|<\epsilon
                \end{equation}
                pour tout \( t\) dans un voisinage de \( 0\). Donc \( X\) est non vide.

            \item[Conclusion pour \( X\)]

                La partie \( X\) est ouverte, fermée et non vide dans \( J_+\) qui est connexe. Donc \( X=J_+\) par la proposition \ref{PropHSjJcIr}\ref{ITEMooNIPZooIDPmEf}.

    \end{subproof}

    La conclusion \( X=J_+\) nous enseigne que pour tout \( t\in J_+\) nous avons \( \mathopen] 0 , t \mathclose]\in J_b\) et \( \big( t,y_b(t) \big)\in \overline{ W_{\epsilon} }\). Nous pouvons faire la même chose pour \( J_-\) et au final nous avons que pour tout \( \tau\in J\) nous avons d'abord \( \tau\in J_b\), ce qui prouve \( J\subset J_b\). De plus pour tout \( t\in J\) nous avons aussi
    \begin{equation}
        \big( t,y_b(t) \big)\in\overline{ W_{\epsilon} }\subset V.
    \end{equation}
    Nous en concluons que 
    \begin{equation}
        J\times \overline{ B(a,r) }\subset V.
    \end{equation}

    Nous savons de plus que pour tout \( b\in \overline{ B(a,r) }\), \( J\subset J_b\). Cela signifie que
    \begin{equation}
        J\times \overline{ B(a,r) }\subset \mD.
    \end{equation}

    Mais \( J\times \overline{ B(a,r) }\) est un voisinage de \( (s,a)\) qui était au début de la preuve un point générique choisit dans \( \mD\). Donc \( \mD\) est ouvert parce qu'il contient un voisinage de chacun de ses points.

    Il nous reste à voir que \( \varphi\colon \mD\to \Omega\) est localement Lipschitz. Soit donc le point générique \( (s,a)\) dans \( \mD\) et l'ensemble $V$ qui avait été construit plus haut. Nous allons montrer que \( \varphi\) est Lipschitz sur \( J\times \overline{ B(a,r) }\subset V\). D'abord sur \( V\), \( f\) est Lipschitz, donc
    \begin{subequations}
        \begin{align}
            \| \varphi(t,b_1)-\varphi(t,b_2) \|&\leq  e^{Lt}\| b_1-b_2 \|\\
            &\leq  e^{LT}\| b_1-b_2 \|
        \end{align}
    \end{subequations}
    pour tout \( t\in J\) et \( b_1,b_2\in \overline{ B(a,r) }\).

    Ensuite, \( f\) est bornée, majorée par \( M\) sur $V$, donc
    \begin{subequations}
        \begin{align}
            \| \varphi(t_1,b)-\varphi(t_2,b) \|&=| \int_{\mathopen[ t_1 , t_2 \mathclose]} y'_(s)ds |\\
            &=| \int_{\mathopen[ t_1 , t_2 \mathclose]}f\big( s,y_b(s) \big) |\\
            &\leq \int_{\mathopen[ t_1 , t_2 \mathclose]}| f\big( s,y_b(s) \big) |ds\\
            &\leq M| t_1-t_2 |.
        \end{align}
    \end{subequations}
    
    Et enfin nous prouvons que \( \varphi\) est localement Lipschitz. En posant \( k=\max\{  e^{LT},M \}\) nous avons
    \begin{subequations}
        \begin{align}
            \| \varphi(t_1,b_1)-\varphi(t_2,b_2) \|&\leq \| \varphi(t_1,b_1)-\varphi(t_1,b_2) \|+\| \varphi(t_1,b_2)-\varphi(t_2,b_2) \|\\
            &\leq  e^{LT}\| b_1-b_2 \|+M| t_1-t_2 |\\
            &\leq k\big( \| b_1-b_2 \|+| t_1-t_2 | \big)\\
            &\leq 2k\max\{ \| b_1-b_2 \|,| t_1-t_2 | \}\\
            &=2kd\big(  (b_1,t_2),(b_2,t_2)  \big).
        \end{align}
    \end{subequations}
    Le flot \( \varphi\) est donc Lipschitz de constante \( 2k\).
\end{proof} 

\begin{lemma}[\cite{ooGQTBooJKpoVP}]        \label{LEMooOJSNooXTJoEf}
    Soit un application \( A\colon \overline{ B(t_0,\tau) }\times \overline{ B(a,R) }\to \aL(\eR^n)\) continue par rapport à sa première variable (\( t_0\in \eR\) et \( a\in \eR^n\)). Alors en posant l'équation
    \begin{subequations}
        \begin{numcases}{}
            \frac{ \partial \psi }{ \partial t }(t,b)=A(t,b)\psi(t,b)\\
            \psi(t_0,b)=\psi_0.
        \end{numcases}
    \end{subequations}
    Nous avons l'estimation
    \begin{equation}
        \begin{aligned}[]
        \| \psi(t,v)-\psi(t,w) \|\leq \| \psi_0 \|\tau\max_{s\in\overline{ B(t_0,\tau) }}\| A(s,v)&-A(s,w) \|\times\\
        &\times \exp\left( \tau\max_{s\in\overline{ B(t_0,\tau) }}\max\{ \| A(s,v) \|,\| A(s,w) \| \} \right)
        \end{aligned}
    \end{equation}
    pour tout \( t\in\overline{ B(t_0,\tau) }\) et \( v,w\in V\).
\end{lemma}


\begin{theorem}[\cite{ooGQTBooJKpoVP}]      \label{THOooSTHXooXqLBoT}
    Soit un intervalle ouvert \( I \) de \( \eR\) et un ouvert connexe \( \Omega\) de \( \eR^n\). Soit une fonction \( f\in C^1\big( I\times \Omega,\eR \big)\), \( a\in \Omega\) et \( t_0\in I\).

    Il existe un voisinage \( W\times V = \overline{ B(t_0,\tau) }\times \overline{ B(a,r) }\) de \( (t_0,a)\) dans \( I\times \Omega\) et une unique application \( \varphi\colon W\times V\to \Omega\) telle que
    \begin{subequations}
        \begin{numcases}{}
            \frac{ \partial \varphi }{ \partial t }(t,x)=f\big( t,\varphi(t,x) \big)\\
            \varphi(t_0,x)=x
        \end{numcases}
    \end{subequations}
    pour tout \( x\in V\).

    L'application \( (t,x)\mapsto \varphi(t,x)\) est de classe \( C^1\).
\end{theorem}

\begin{probleme}
    La preuve qui suit doit être lue avec beaucoup d'attention, en particulier sur les incohérences possibles de notations, et sur les oublis possibles de précautions oratoires type «quitte à encore réduire les voisinages \( V\) et $W$».
\end{probleme}

\begin{proof}
    En termes de notations, pour \( x\in \Omega \) fixé nous écrivons \( y_x(t)\) pour \( \varphi(t,x)\) et pour \( t\in I\) fixé nous notons \( \varphi_t(x)\) pour \( \varphi(t,x)\).

    De plus lorsque nous écrirons des choses comme \( g\colon \eR\to \eR\), nous n'entendrons pas que \( g\) est effectivement définie sur tout \( \eR\). La notation \( g\colon \eR\to \eR\) indiquera seulement que la variable de \( g\) est réelle, et que nous comptons préciser le domaine plus tard. Cette remarque s'applique seulement à cette démonstration et non à l'ensemble du livre.

    Nous considérons \( R>0\) tel que \( \overline{ B(a,2R) }\subset\Omega\) et ensuite nous posons \( V=\overline{ B(a,R) }\). La fonction \( y_x\), solution pour la condition initiale \( y_x(t_0)=x\) est définie sur \( W=\mathopen[ t_0-\tau , t_0+\tau  \mathclose]\) et prend ses valeurs dans \( \overline{ B(x,R) }\). Ceci est parce que \( y_x\) est continue, alors en prenant \( \tau\) assez petit, la valeur de \( y_x(t)\) ne va pas s'éloigner de \( x\) lorsque \( t\) ne s'éloigne pas de \( t_0\).

    Nous savons déjà de la proposition \ref{PROPooUDQWooNFrNOQ} que \( \varphi\) est \( C^1\) en \( t\) et localement Lipschitz en sa seconde variable, avec une constante Lipschitz uniforme sur \( W\times V\). Elle est donc continue en tant que fonction
    \begin{equation}
        \varphi\colon V\times W\to \eR^d.
    \end{equation}
    
    \begin{subproof}
        \item[La différentielle partielle \( Df\)]
            Pout \( t\) fixé nous notons \( Df_{(t,x)}\) la différentielle de \( f\) par rapport à \( x\). C'est à dire que
            \begin{equation}
                \begin{aligned}
                    Df_{(t,x)}\colon \eR^n&\to \eR^n \\
                    u&\mapsto \Dsdd{ f(t,x+su) }{s}{0}. 
                \end{aligned}
            \end{equation}
            C'est un élément de \( \aL(\eR^n)\), l'ensemble des application linéaires de \( \eR^n\) vers \( \eR^n\). Nous allons montrer que
            \begin{equation}
                (t,x)\mapsto Df_{(t,x)} 
            \end{equation}
            est continue en tant qu'application \( \eR\times \eR^n\to\aL(\eR^n)\). Pour cela nous introduisons l'application d'inclusion \( i\colon \eR^n\to \eR\times \eR^n\), \( i(u)=(0,u)\). Elle donne
            \begin{equation}
                Df_{(t,x)}(u)=\Dsdd{ f\big( (t,x)+s(0,u) \big) }{s}{0}=df_{(t,x)}\circ i (u).
            \end{equation}
            Autrement dit
            \begin{equation}
                Df_{(t,x)}=df_{(t,x)}\circ i.
            \end{equation}
            Or l'application \( (t,x)\mapsto df_{(t,x)} \) est continue par hypothèse (\( f\) est de classe \( C^1\)) et l'application
            \begin{equation}        \label{EQooZTAPooCduWcl}
                \begin{aligned}
                     \aL(\eR\times \eR^n,\eR^n)&\to \aL(\eR^n,\eR^n) \\
                    A&\mapsto A\circ i 
                \end{aligned}
            \end{equation}
            est également continue. Donc \( (t,x)\mapsto Df_{(t,x)}\) est continue\quext{Si quelqu'un peut prouver ça de façon moins verbeuse, je suis preneur. Il me semble que quel que soit la façon dont on s'y prend, sous le capot, on passe par la continuité de l'application \eqref{EQooZTAPooCduWcl}.}.
            
        \item[L'équation aux variations]

            Soit \( x\in \Omega\). Nous introduisons l'opérateur
            \begin{equation}
                \begin{aligned}
                    S_x\colon \eR\times \aL(\eR^n)&\to \aL(\eR^n) \\
                    S_x(t,\psi)&=Df_{(t,y_x(t))}\circ \psi. 
                \end{aligned}
            \end{equation}
            Par ce que nous avons raconté, cela est une fonction continue en sa première variable et Lipschitz en sa seconde variable. Nous identifions \( \aL(\eR^n)\) à \( \eR^{2^n}\).
            
            Toujours pour chaque \( x\) considéré nous posons l'équation différentielle ordinaire
            \begin{subequations}        \label{EQooQONGooBrxuSA}
                \begin{numcases}{}
                    \frac{ \partial\psi }{ \partial t }(t,x)=S_x\big( t,\psi(t,x) \big)\\
                    \psi(t_0,x)=\id.
                \end{numcases}
            \end{subequations}
            qui est une équation différentielle ordinaire pour \( \psi\colon \eR\times \eR^n\to \aL(\eR^n)\) rentrant dans le cadre de Cauchy-Lipschitz.
            
            Quel est le domaine de définition de \( \psi\) pour sa première variable ? C'est un ouvert autour de \( t_0\). Nous réduisons \( W\) de telle sorte que la solution \( \psi\) soit définie sur \( W\). Idem pour la variable \( x\) qui est dans un voisinage de \( a\).

            L'équation \eqref{EQooQONGooBrxuSA} s'appelle l'\defe{équation aux variations}{équation!aux variations}.

        \item[\( \psi\) est continue en \( (t,x)\) (début)]


            Il s'agit de majorer les deux termes de
            \begin{equation}        \label{EQooVUNUooExeQba}
                \| \psi(t_1,a_1)-\psi(t_2,a_2) \|\leq \| \psi(t_1,a_1)-\psi(t_2,a_1) \|+\| \psi(t_2,a_1)-\psi(t_2,a_2) \|.
            \end{equation}

            \begin{subproof}
            
        \item[Premier terme]
        
            Nous avons
            \begin{subequations}
                \begin{align}
                    \| \psi(t_1,b)-\psi(t_2,b) \|&=\| \int_{\mathopen[ t_1 , t_2 \mathclose]}\frac{ \partial \psi }{ \partial t }(s,b)ds \|\\
                    &\leq\int_{\mathopen[ t_1 , t_2 \mathclose]}\| Df_{(s,y_b(s))}\circ\psi(s,b) \|ds\\
                    &\leq\int_{\mathopen[ t_1 , t_2 \mathclose]}\| Df_{(s,t_b(s))} \|\| \psi(s,b) \|ds\\
                    &\leq | t_1-t_2 |\max_{s\in\mathopen[ t_1 , t_2 \mathclose]}\| Df_{(s,y_b(s))} \|\max_{s\in\mathopen[ t_1 , t_2 \mathclose]}\| \psi(s,b) \|.\label{SUBEQooLYMAooRMaMhn}
                \end{align}
            \end{subequations}

            Nous allons majorer le second maximum. Prenons \( t\in\mathopen[ 0 , \tau \mathclose]\); et posons \( A(u,b)=Df_{(u,y_b(u))}\) pour alléger les notations. Par l'équation de définition de \( \psi\) nous avons
            \begin{equation}
                \psi(t,b)=\psi(0,b)+\int_{\mathopen[ 0 , t \mathclose]}A(u,b)\psi(u,b)du,
            \end{equation}
            et donc
            \begin{equation}
                \| \psi(t,b) \|\leq \| \psi_0 \|+\int_{\mathopen[ 0 , t \mathclose]} \| A(u,b) \|  \| \psi(u,b) \|du.
            \end{equation}
            En y appliquant le lemme de Grönwall dans sa version \ref{LemuBVozy} nous trouvons
            \begin{subequations}
                \begin{align}
                \| \psi(s,b) \|&\leq \| \psi_0 \|\exp\left( \int_{\mathopen[ 0 , s \mathclose]}\| A(u,b) \|du \right)\\
                &\leq \| \psi_0 \|\exp\left( s\max_{u\in\mathopen[ 0 , s \mathclose]}\| A(u,b) \| \right).
                \end{align}
            \end{subequations}
            En retournant à \eqref{SUBEQooLYMAooRMaMhn} nous avons \( \psi_0=\id\) et donc \( \| \psi_0 \|=1\) et
            \begin{equation}
                \max_{s\in\mathopen[ t_1 , t_2 \mathclose]}\| \psi(s,b) \|\leq \max_{s\in \mathopen[ t_1 , t_2 \mathclose]}\exp\left( s\max_{u\in \mathopen[ 0 , t \mathclose]}\| Df_{(u,y_b(u))} \| \right)
            \end{equation}
            Là dedans nous pouvons remplacer \( t\) par \( \max\{ | t_1 |,| t_2 | \}\). Posons enfin, pour alléger les expressions
            \begin{equation}
                a(t_1,t_2,b)=\max_{s\in\mathopen[ t_1 , t_2 \mathclose]}\| Df_{(s,y_b(s))} \|.
            \end{equation}
            La majoration que nous retenons est :
            \begin{equation}
                \| \psi(t_1,b)-\psi(t_2,b) \|\leq | t_1-t_2 |a(t_1,t_2,b)\exp\big( \max\{ | t_1 |,| t_2 | \}a(0,t,b) \big).
            \end{equation}
            Cela tend vers zéro lorsque \( t_1\to t_2\).
            
        \item[Deuxième terme]

            En ce qui concerne le second terme,
            \begin{equation}
                \| \psi(t,b_1)-\psi(t,b_2) \|
            \end{equation}
            nous utilisons le lemme \ref{LEMooOJSNooXTJoEf} qui donne, pour \( t\in\mathopen[ t_0 -\tau, t_0+\tau \mathclose]\),
            \begin{equation}
                \begin{aligned}[]
                    \| \psi(t,b_1)-\psi(t,b_2) \|\leq\tau\max_{s\in \overline{ B(0,\tau) }}&\| Df_{s,y_{b_1}(s)}-Df_{s,y_{b_2}(s)} \|\times\\
                    &\times \exp\left( \tau\max\{ \| Df_{s,y_{b_1}(s)},\| Df_{s,y_{b_2}(s)} \| \| \} \right).
                \end{aligned}
            \end{equation}
            Dans notre cas, \( t_0=0\), donc \( t\in\mathopen[ -\tau , \tau \mathclose]\). Vu la continuité de \( Df\), nous avons
            \begin{equation}
                    \max_{s\in \overline{ B(0,\tau) }}\| Df_{s,y_{b_1}(s)}-Df_{s,y_{b_2}(s)} \|\to 0
            \end{equation}
            lorsque \( b_1\to b_2\).

        \item[\( \psi\) est continue en \( (t,x)\) (fin)]

            Les deux bons calculs faits, nous avons, en repartant de \eqref{EQooVUNUooExeQba},
            \begin{equation}
                \lim_{(t_1,b_1)\to(t_2,b_2)}\| \psi(t_1,b_1)-\psi(t_2,b_2) \|=0,
            \end{equation}
            ce qui signifie que \( \psi\) est une fonction continue de ses deux variables en même temps.
            \end{subproof}

        \item[Différentiabilité de \( \varphi\) (début)]

            Nous montrons maintenant que \( D\varphi(t,x)\) existe. Pour rappel, \( D\) est la différentielle par rapport à la seconde variable. Nous sommes à étudier l'existence de \( D\varphi_{(t,b)}=d(\varphi_t)_b\). Nous posons
            \begin{equation}
                \theta(t,h)=\varphi(t,b+h)-\varphi(t,b)=y_{b+h}(t)-y_b(t)
            \end{equation}
            où \( b\) est le point où nous étudions la différentiabilité. Il est dans un voisinage du point \( a\) fixé depuis le début et autour duquel il existe un voisinage qui donne un sens à tout ce que nous avons fait jusqu'à présent. La dépendance de \( \theta\) en \( b\) est implicite. Vu que \( \varphi\) est Lipschitz en sa seconde variable, nous avons la majoration
            \begin{equation}        \label{EQooKYELooZlfeed}
                \| \theta(t,h) \|\leq C\| h \|
            \end{equation}
            dès que \( t\in V\) et \( b,b+h\in W\).

            De plus, parce que \( t_0\) est le temps de la condition initiale nous avons
            \begin{equation}
                \theta(t_0,h)=y_{b+h}(t_0)-y_{b}(t_0)=a+h-a=h.
            \end{equation}
            Et aussi, par définition de \( \psi\) :
            \begin{subequations}
                \begin{align}
                    \psi(t,b)&=\psi_0+\int_{t_0}^t\frac{ \partial \psi }{ \partial t }(s,b)ds\\
                    &=\psi_0+\int_{t_0}^tDf_{(s,y_b(s))}\circ\psi(s,b)
                \end{align}
            \end{subequations}
            En appliquant à \( h\) et en se souvenant que \( \psi_0=\id\),
            \begin{equation}
                    \psi(t,b)h=h+\int_{t_0}^t\Big( Df_{s,y_b(s)}\circ\psi(s,b)\Big)h\,ds.
            \end{equation}
            Puis on peut faire un calcul assez classique en se souvenant que \( \theta(t_0,h)=h\) :
            \begin{subequations}
                \begin{align}
                    \theta(t,h)&=\theta(t_0,h)+\int_{t_0}^t\big[ \frac{ \partial \varphi }{ \partial t }(s,b+h)-\frac{ \partial \varphi }{ \partial t }(s,b) \big]ds\\
                    &=h+\int_{t_0}^t\big[   f\big( s,y_{b+h}(s) \big)-f\big( s,y_b(s) \big)   \big]ds.
                \end{align}
            \end{subequations}
            On fait la différence entre les deux :
            \begin{equation}
                \theta(t,h)-\psi(t,b)h=-\int_{t_0}^t\big[ Df_{s,y_b(s)}\circ\psi(s,b)h-f\big( s,y_{b+h}(s)+f\big( s,y_b(s) \big) \big) \big]ds.
            \end{equation}
            Nous y ajoutons et soustrayons \( Df_{s,y_{b}(s)}\theta(s,h)\) et nous retenons la majoration suivante :
            \begin{equation}        \label{EQooODHPooDYyBoH}
                \begin{aligned}[]
                    \| \theta(t,h)-\psi(t,b)h \|&\leq \int_{t_0}^t\| Df_{(s,y_b(s))}\psi(s,b)-Df_{(s,y_b(s))} \theta(s,h)\| ds\\
                    &+\int_{t_0}^t\| f(s,y_{b+h}(s))-f(s,y_b(s))+Df_{(s,y_b(s))}\theta(s,h)  \|ds.
                \end{aligned}
            \end{equation}
            Nous allons encore majorer ces deux termes séparément. Soit \( \epsilon>0\).

            \begin{subproof}
    
        \item[Premier terme]

             Ce qui est dans la norme à majorer est
             \begin{equation}
                 Df_{(s,y_b(s))}\big( \psi(s,b)h-\theta(s,h) \big).
             \end{equation}
             Vu que \( Df\) est continue et que \( y_b\) est continue\footnote{Il faut encore réduire les voisinages \( V\) et \( W\) pour que ceci ait un sens.}, l'application \( s\mapsto Df_{s,y_b(s)}\) est continue et donc de norme majorée sur le compact \( \mathopen[ t_0 , t \mathclose]  \). Nous rapellons la notation
             \begin{equation}
                 a(t_0,t,b)=\max_{s\in\mathopen[ t_0 , t \mathclose]}\| Df_{s,y_b(s)} \|,
             \end{equation}
             et nous majorons encore et toujours. D'abord
             \begin{equation}
                 \int_{t_0}^t\| Df_{(s,y_b(s))}\big( \psi(s,b)h-\theta(s,h) \big) \|ds\leq a(t_0,t,b)\int_{t_0}^t\| \psi(s,b)-\theta(s,h) \|ds.
             \end{equation}

        \item[Deuxième terme]

            Pour traiter le deuxième terme, nous allons provisoirement noter \( x=y_b(s)\) et \( y=y_{b+h}(s)\); entre autres, \( y-x=\theta(s,h)\). Ce qui est écrit dans le second terme de \eqref{EQooODHPooDYyBoH} est
            \begin{equation}
                f(s,y)-f(s,x)+Df_{(s,x)}\theta(s,h)=f(s,y)-f(s,x)+Df_{(s,x)}(y-x)
            \end{equation}
            Comme \( D\) ne s'applique pas à la variable \( s\), nous pouvons alléger la notation et déduire de la différentiabilité de \( f\) qu'il existe un \( \eta>0\) tel que \( x,y\in W\) avec \( \| y-w \|\leq \eta\) implique
            \begin{equation}
                \| f(y)-f(x)-Df_x(y-x) \|\leq \epsilon\| y-x \|.
            \end{equation}
            Prenons \( \| h \|\leq \eta/C\) (le \( C\) de \eqref{EQooKYELooZlfeed}); en déballant les notations,
            \begin{equation}
                \| f\big( s,y_{b+h}(s) \big)-f\big( s,y_b(s) \big) -Df_{(s,y_b(s))}\theta(s,h)\|\leq \epsilon\| \theta(s,h) \|\leq \epsilon C\| h \|.
            \end{equation}
            

        \item[Les deux termes ensemble]

            En remettant les deux dans \eqref{EQooODHPooDYyBoH} nous trouvons la majoration
            \begin{equation}
                \| \theta(t,h)-\psi(t,b)h \|\leq | t-t_0 |\epsilon C\| h \|+a(t_0,t,v)\int_{t_0}^t\| \psi(s,b)h-\theta(s,h) \|ds
            \end{equation}
            qui est encore de la graine à Grönwall avec
            \begin{subequations}
                \begin{numcases}{}
                    u(t)=\| \theta(t,h)-\psi(t,b) \|\\
                    b(t)=| t-t_0 |\epsilon C\| h \|\\
                    a(s)=a(t_0,t,v),
                \end{numcases}
            \end{subequations}
            la troisième étant une fonction constante. Cela donne, pour \( t\in\mathopen[ t_0-\tau , t_0+\tau \mathclose]\),
            \begin{equation}
                \| \theta(t,h)-\psi(t,b)h \|\leq| t-t_0 |\epsilon C\| h \|+\int_{t_0}^t(s-t_0)\epsilon C\| h \|a(t_0,t,b)\exp\left( \int_{s}^ta(t_0,t,b)du \right)ds.
            \end{equation}
            En valeur absolue, la différence \( s-t_0\) est majorée par \( \tau\), l'intégrale dans l'exponentielle vaut \( (t-s)a(t_0,t,b)\), et restons avec
            \begin{equation}
                \| \theta(t,h)-\psi(t,b)h \|\leq \tau \epsilon C\| h \|+\tau\int_{t_0}^t\epsilon C\| h \|a(t_0,t,b) e^{a(t_0,t,b)(t-s)}ds.
            \end{equation}
            En supposant \( t>t_0\) nous pouvons calculer l'intégrale. Si vous m'avez suivi jusqu'ici, vous devriez avoir de tels maux de tête que je vous donne la réponse :
            \begin{equation}
                \int_{t_0}^ta(t_0,t,b) e^{(t-s)a(t_0,t,b)}ds= e^{(t_0-t)a(t_0,t,b)}-1.
            \end{equation}
            En remettant dans l'expression,
            \begin{equation}
                \| \theta(t,h)-\psi(t,b)h \|\leq \tau\epsilon C\| h \|+\epsilon C\| h \|a(t_0,t,b)\tau\big(  e^{(t-t_0)}-1 \big)=\tau\epsilon C\| h \| e^{(t-t_0)a(t,t_0,b)}.
            \end{equation}
            Nous pouvons majorer \( t-t_0\) par \( \tau\) et \( a(t,t_0,b) \) par \( a(t_0-\tau,t_0+\tau,b)\) pour avoir la majoration
            \begin{equation}
                \| \theta(t,h)-\psi(t,b)h \|\leq \tau\epsilon C\| h \| e^{\tau a(t_0-\tau,t_0+\tau,b)}.
            \end{equation}
            
        \item[Différentiabilité de \( \varphi(t,b)\) (fin)]

            Nous écrivons la définition \ref{DefDifferentiellePta} de la différentiabilité : nous voulons vérifier que
            \begin{equation}
                \lim_{h\to 0} \frac{ \varphi(t,b+h)-\varphi(t,b)-\psi(t,b)h }{ \| h \| }=0.
            \end{equation}
            Nous remplaçons \( \varphi(t,b+h)-\varphi(t,b)\) par \( \theta(t,h)\) et prenons la norme avec les majorations données :
            \begin{equation}
                \lim_{h\to 0} \frac{ \|  \varphi(t,b+h)-\varphi(t,b)-\psi(t,b)h  \|   }{ \| h \| }\leq \lim_{h\to 0} \tau\epsilon C e^{\tau e(t_0-\tau,t_0+\tau,b)}.
            \end{equation}
            Cela étant valable pour tout \( \epsilon\), nous en déduisons la nullité de la limite.

            Nous avons démontré que \( \varphi\) était différentiable par rapport à sa deuxième variable et que
            \begin{equation}        \label{EQooPJFOooHuOIuw}
                D\varphi_{(t,b)}=\psi(t,b).
            \end{equation}
            \end{subproof}
            
        \item[Conclusion : \( \varphi\) est de classe \( C^1\)]

            Nous avons déjà prouvé que \( (t,b)\mapsto \psi(t,b)\) est continue. Donc de \eqref{EQooPJFOooHuOIuw} nous déduisons que les dérivées partielles \(  (t,b)\mapsto\frac{ \partial \varphi }{ \partial x_i } (t,b)\) sont continues. Mais comme \( \varphi\) est Lipschitz en \( t\), la dérivée partielle \( (t,b)\mapsto \frac{ \partial \varphi }{ \partial t }(t,b)\) est également continue.

            La continuité de toutes les dérivées partielles de \( \varphi\) nous donne la classe \( C^1\) pour \( \varphi\) par la proposition \ref{PropDerContCun}.
    \end{subproof}
\end{proof}

\begin{proposition}[Régularité du flot\cite{ooEGXQooRwPKcC}]
    Soit un intervalle ouvert \( I \) de \( \eR\) et un ouvert connexe \( \Omega\) de \( \eR^n\). Soit une fonction \( f\in C^p\big( I\times \Omega,\eR \big)\), avec \( p\geq 1\) ainsi que \( a\in \Omega\) et \( t_0\in I\).

    Il existe un voisinage \( W\times V = \overline{ B(t_0,\tau) }\times \overline{ B(a,r) }\) de \( (t_0,a)\) dans \( I\times \Omega\) et une unique application \( \varphi\colon W\times V\to \Omega\) telle que
    \begin{subequations}
        \begin{numcases}{}
            \frac{ \partial \varphi }{ \partial t }(t,x)=f\big( t,\varphi(t,x) \big)\\
            \varphi(t_0,x)=x
        \end{numcases}
    \end{subequations}
    pour tout \( x\in V\).

    L'application \( (t,x)\mapsto \varphi(t,x)\) est de classe \( C^p\).
\end{proposition}

\begin{proof}
    Nous savons déjà par le théorème \ref{THOooSTHXooXqLBoT} que \( (t,x)\mapsto \varphi(t,x)\) est de classe \( C^1\). Nous supposons que \( f\) est de classe \( C^p\) avec \( p\geq 2\).

    Vu que \( \varphi\) et \( f\) sont de classe \( C^1\), nous avons aussi que l'application \( (t,x)\mapsto f\big( t,\varphi(t,x) \big)\) est de classe \( C^1\). L'équation donne alors immédiatement le fait que
    \begin{equation}
        (t,x)\mapsto\frac{ \partial \varphi }{ \partial t }(t,x)
    \end{equation}
    est de classe \( C^1\).

    En ce qui concerne la régularité par rapport aux autres variables, il faudra travailler un peu plus. 
    
    \begin{subproof}
    \item[Une équation différentielle pour le flot]
    Nous allons commencer par un habile jeu d'écriture : la formule
    \begin{equation}
        \varphi(t,x)=x+\int_{t_0}^tf\big( s,\varphi(s,x) \big)ds
    \end{equation}
    devient
    \begin{equation}        \label{EQooQGVOooYMEgcM}
        \varphi_t(x)=x+\int_{t_0}^tf\big( s,\varphi_s(x) \big)ds.
    \end{equation}
    Dans le même ordre d'idée nous notons \( f_s(x)=f(s,x)\), et ce qui se trouve dans l'intégrale \eqref{EQooQGVOooYMEgcM} n'est autre que la fonction
    \begin{equation}
        g_s(x)=(f_s\circ\varphi_s)(x).
    \end{equation}
    Tout cela pour différentier l'égalité \eqref{EQooQGVOooYMEgcM} par la proposition \ref{PropAOZkDsh} :
    \begin{subequations}
        \begin{align}
            (d\varphi_t)_x&=\id+\int_{t_0}^t(dg_s)_xds\\
            &=\id+\int_{t_0}^t(df_s)_{\varphi_s(x)}\circ(d\varphi_s)_sds.
        \end{align}
    \end{subequations}
    Nous dérivons ensuite cela par rapport à \( t\) :
    \begin{equation}        \label{EQooBETGooXKWRxX}
        \frac{ \partial  }{ \partial t }\Big( (d\varphi_t)_x \Big)=(df_t)_{\varphi_t(x)}\circ(d\varphi_t)_x.
    \end{equation}
    Cela est une égalité dans \( \aL(\eR^n)\).

    Nous introduisons la fonction 
    \begin{equation}
        \begin{aligned}
            F\colon I\times \aL(\eR^n)\times \Omega&\to \aL(\eR^n)  \\
            (t,A,x)&\mapsto (df_t)_{\varphi_t(x)}\circ A. 
        \end{aligned}
    \end{equation}
    En fait, à la place de \( I\) et \( \Omega\) il faut prendre des petits voisinages dans lesquels les choses ont un sens. Ce que dit l'équation \ref{EQooBETGooXKWRxX} est que l'application
    \begin{equation}
        \begin{aligned}
            A\colon I\times \Omega&\to \aL(\eR^n) \\
            (t,x)&\mapsto (d\varphi_t)_x 
        \end{aligned}
    \end{equation}
    vérifie l'équation différentielle
    \begin{equation}        \label{EQooTOJMooLXLfVv}
        \frac{ \partial A }{ \partial t }(t,x)=F\big( t,A(t,x),x \big).
    \end{equation}

    \item[Une autre équation différentielle]

        Nous n'oublions pas l'équation différentielle pour la dérivée par rapport à \( t\) :
        \begin{equation}        \label{EQooYOJPooKEgiec}
            \frac{ \partial \varphi }{ \partial t }(t,x)=f\big( t,\varphi(t,x) \big).
        \end{equation}

    \item[Régularité \( C^2\)]

        L'équation \eqref{EQooTOJMooLXLfVv} est de type Cauchy-Lipschitz à paramètre (le paramètre est \( x\)) dont les propriétés sont dans le théorème \ref{THOooDTCWooSPKeYu}. Nous savons que la solution est unique et de classe \( C^1\) parce que \( F\) est une composée de fonctions \( C^1\). Ici nous utilisons de façon cruciale le fait que nous avons déjà démontré la régularité \( C^1\) du flot \( (t,x)\mapsto \varphi(t,x)\).

        L'équation \eqref{EQooYOJPooKEgiec} nous dit qu'il en est de même pour la dérivée par rapport à \( t\) : la fonction
        \begin{equation}
            (t,x)\mapsto \frac{ \partial \varphi }{ \partial t }(t,x)
        \end{equation}
        est de classe \( C^1\).

        Toutes les dérivées partielles étant de classe \( C^1\), la fonction \( (t,x)\mapsto \varphi(t,x)\) est de classe \( C^2\).


    \item[Régularité \( C^p\)]

        Il s'agit simplement d'itérer ce processus. Maintenant que nous savons que \( \varphi\) est \( C^2\), l'application \( F\) est de classe \( C^2\), ce qui donne une différentielle de classe \( C^2\) et donc un flot de classe \( C^3\). Et le \emph{show must go on} jusqu'à la régularité \( C^p\).

    \end{subproof}
\end{proof}

\begin{normaltext}      \label{NORMooWEWVooXbGmfE}
    % Attention : ce 'normaltext' est référentié dans l'index thématique sur l'inversion locale. Si on développe ici, il faudra modifier là-bas.
    % position 1051229132
    Le théorème d'inversion locale \ref{ThoXWpzqCn} nous permet de dire que, pour \( t\) fixé, le flot \( x\mapsto \varphi_t(x)\) est un \( C^p\)-difféomorphisme local.
\end{normaltext}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Stabilité de Lyapunov}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefKMCGooOeFKlA}
    Dans le cas de l'équation différentielle \( y'(t)=f\big( y(t),t \big)\) pour \( y\colon \eR\to \eR^n\), un point \( a\in \eR^n\) est un \defe{point d'équilibre}{équilibre!point point une équation différentielle} lorsque la fonction constante \( y(t)=a\) est une solution.

    Le point d'équilibre \( a\in \eR^n\) est \defe{stable}{point!d'équilibre!stable}\index{stabilité!d'un point d'équilibre} si pour tout \( \epsilon>0\), il existe \( \delta>0\) tel que \( \| y(0)-a \|<\delta\) implique \( \| y(t)-a \|<\epsilon\) pour tout \( t\).
\end{definition}

\begin{theorem}[Théorème de stabilité de Lyapunov\cite{MJEooXxBFFY,PAXrsMn,GPRooZkclFA}]    \label{ThoBSEJooIcdHYp}
    Soit l'équation différentielle
    \begin{subequations}    \label{EqZZLBooKBkZkG}
        \begin{numcases}{}
            y'(t)=f(y)\\
            y(0)=y_0
        \end{numcases}
    \end{subequations}
    avec une fonction \( f\colon \eR^n\to \eR^n\) de classe \( C^1\) vérifiant \( f(0)=0\) et \( y_0\in \eR^n\). Nous supposons que l'application linéaire \( df_0\) n'a que des valeurs propres dont la partie réelle est strictement négative. 
    
    Alors
    \begin{enumerate}
        \item   \label{ItemGZEAooAhxuDQi}
            Il existe \( k>0\) tel que si \( \| y_0 \|<k\) alors la solution maximale est définie sur \( \eR\),
        \item   \label{ItemGZEAooAhxuDQii}
            pour le même nombre \( k>0\), si \( \| y_0 \|<k\) alors \( y(t)\stackrel{t\to\infty}{\longrightarrow} 0\) exponentiellement vite,
        \item
            la solution \( y=0\) est un point d'équilibre attractif.
    \end{enumerate}

\end{theorem}
\index{théorème!stabilité de Lyapunov}
\index{stabilité!Lyapunov}

\begin{proof}
    Placer ici une phrase intelligente\footnote{Parce que sinon l'environnement \info{description} qui suit donne un mauvais effet.}.
    \begin{subproof}
        \item[Prolégomène]

    Le théorème de Cauchy-Lipschitz \ref{ThokUUlgU} nous enseigne que l'équation différentielle considérée possède une unique solution maximale (entre autres parce qu'une fonction de classe \( C^1\) est localement Lipschitz) et nous nommons \( J\) l'intervalle sur lequel elle est définie.

\item[Système linéarisé]

    Nous posons \( A=df_0\). La fonction \( y_L(t)= e^{tA}y_0\) est solution du système linéarisé
    \begin{subequations}
        \begin{numcases}{}
            y'(t)=Ay(t)\\
            y(0)=y_0.
        \end{numcases}
    \end{subequations}
    Pour évaluer la norme de \( y_L\) nous utilisons le lemme \ref{LemQEARooLRXEef} : il existe un polynôme \( P\) tel que
    \begin{equation}
        \| y_L(t) \|\leq P\big( | t | \big)\sum_{i=1}^r e^{\real{\lambda_i}t}\| y_0 \|.
    \end{equation}
    Mais par hypothèse, \( \real(\lambda_i)<0\) et si nous posons \( \lambda=\max\{ \real(\lambda_i) \}\) nous avons \( \lambda<0\) et 
    \begin{equation}
        \| y_L(t) \|\leq P\big( | t | \big) e^{\lambda t}\| y_0 \|.
    \end{equation}
    Donc quel que soit \( y_0\) nous avons \( \lim_{t\to \infty} \| y_L(t) \|=0\) c'est à dire \( \lim_{t\to \infty} y_L(t)=0\).

\item[Une forme linéaire]

    Nous définissons la forme bilinéaire suivante sur \( \eR^n\) :
    \begin{equation}
        b(x,y)=\int_0^{\infty}\langle  e^{tA}x,  e^{tA}y\rangle dt.
    \end{equation}
    D'abord cela est bien défini pour tout \( x,y\in \eR^n\) parce que
    \begin{equation}
        \big| \langle  e^{tA}x,  e^{tA}y\rangle  \big|\leq \|  e^{tA}x \|\|  e^{tA}y \|\leq P_1\big( | t | \big)P_2\big( | t | \big) e^{2\lambda t}\| x \|\| y \|,
    \end{equation}
    qui est intégrable entre \( 0\) et \( \infty\) à cause de la décroissance exponentielle\footnote{Proposition \ref{PropBQGBooHxNrrf}.}. Montrons que \( b\) est définie positive. Soit donc \( x\neq 0\) et calculons
    \begin{equation}
        b(x,x)=\int_0^{\infty}\|  e^{tA}x \|^2dt.
    \end{equation}
    Ce qui est dans l'intégrale est forcément (pas strictement) positif pour tout \( t\). Mais si \( x\neq 0\) alors \( \| x \|^2\) est strictement positif et sur un voisinage de \( t=0\) nous avons aussi \( \|  e^{tA}x \|^2\) qui est strictement positif. Ergo \( b(x,x)>0\) dès que \( x\neq 0\), ce qui signifie que \( b\) est strictement définie positive (lemme \ref{LemWZFSooYvksjw}).

    Nous notons \( q\colon V\to \eR\) la forme quadratique associée à \( b\) et aussi la norme qui va avec : \( \| x \|_q=\sqrt{q(x)}\). En ce qui concerne le gradient \( \nabla q\colon V\to V\), nous avons la formule \( \nabla q(x)\cdot y=2b(x,y)\)\cite{MJEooXxBFFY}. En effet, nous utilisons une des nombreuses formules du lemme \ref{LemdfaSurLesPartielles}\footnote{Le fait que \( q\) soit différentiable est simplement le fait que \( b\) soit bilinéaire.} :
    \begin{subequations}
        \begin{align}
            \nabla q(x)\cdot y&=\Dsdd{ q(x+ty) }{t}{0}\\
            &=\Dsdd{ q(x)+t^2q(y)+2tb(x,y) }{t}{0}\\
            &=2b(x,y).
        \end{align}
    \end{subequations}
    Nous avons aussi
    \begin{subequations}
        \begin{align}
            \nabla q(x)\cdot Ax&=2b(x,Ax)\\
            &=2\int_0^{\infty}\langle  e^{tA}x,  e^{tA}Ax\rangle \\
            &=\int_0^{\infty}\frac{ \partial  }{ \partial t }\Big( \langle  e^{tA}x,  e^{tA}x\rangle  \Big)(t)dt\\
            &=\lim_{T\to \infty} \Big[ \langle  e^{tA}x,  e^{tA}x\rangle  \Big]_{t=0}^{t=T}.
        \end{align}
    \end{subequations}
    Mais vu que \( \|  e^{tA}x \|\to 0\), pour \( t\to \infty\) il ne reste que terme \( t=0\) de la différence, c'est à dire
    \begin{equation}    \label{EqUCOGooEFxZSO}
        \nabla q(x)\cdot Ax=2b(x,Ax)=-\| x \|^2.
    \end{equation}
    Étant donné que \( \nabla q(x)\) est le vecteur dirigé vers l'extérieur de l'ellipsoïde de la courbe de niveau de \( q\) au point \( x\), le vecteur \( Ax\) est dirigé vers l'intérieur.

\begin{center}
   \input{auto/pictures_tex/Fig_FNBQooYgkAmS.pstricks}
\end{center}


\item[Majoration de \(  q\big( y(t) \big)'  \)]
    Nous posons 
    \begin{equation}
        \begin{aligned}
            r\colon \eR^n&\to \eR^n \\
            x&\mapsto f(x)-Ax. 
        \end{aligned}
    \end{equation}

    Soit \( y\) la solution maximale au problème \eqref{EqZZLBooKBkZkG} que nous pouvons aussi écrire sous la forme
    \begin{equation}
        y'(t)=r\big( y(t) \big)+Ay(t).
    \end{equation}
    Calculons un peu \ldots
    \begin{subequations}    \label{subeqsZCOLooOzTBLr}
        \begin{align}
            q\big( y(t) \big)'&=b\big( y(t),y(t) \big)'\\
            &=2b(y,y')\\
            &=2b\big( y,Ay \big)+2b\big( y,r(y) \big)\\
            &=-\| y \|^2+2b\big( y,r(y) \big)       &\text{\eqref{EqUCOGooEFxZSO} avec } x=y(t)\\
            &\leq -\| y \|^2+2\| y(t) \|_q\| r\big( y(t) \big) \|_q &\text{Cauchy-Schwarz : } | b(a,b) |\leq \| a \|_q\| b \|_q\text{.}
        \end{align}
    \end{subequations}
    Chacun des deux termes peut encore être majoré. En ce qui concerne le premier, par équivalence des normes\footnote{Définition \ref{DefEquivNorm} et théorème \ref{ThoNormesEquiv}.}, il existe une constante \( C\) telle que \( \| y \|\geq C \| y \|_q\). En renommant immédiatement $C^2$ en \( C\), \( \| y \|^2\geq C\| y \|_q^2=Cq(y)\).

    Pour le second, nous allons utiliser la différentiabilité de \( r\) et le théorème des accroissements finis. Vu que \( df_0=A\) nous avons \( dr_0=df_0-A=0\) et de plus \( r\) est de classe \( C^1\) parce que \( f\) l'est. Toutes les normes étant équivalentes\footnote{Théorème \ref{ThoNormesEquiv}.} sur \( \eR^n\) nous pouvons exprimer la continuité de \( dr\) pour la norme \( \| . \|_q\) : si \( \epsilon>0\) est fixé alors il existe \( \alpha>0\) tel que \( \| x \|<\alpha\) implique \( \| dr_x \|_q<\epsilon\). Nous pouvons écrire les accroissements finis\footnote{Théorème \ref{ThoNAKKght}.} pour la fonction \( r\) :
    \begin{equation}    \label{EqIDTHooCsMSVs}
        \| r(x)-r(0) \|_q\leq \sup_{a\in\mathopen[ 0 , x \mathclose]}\| df_a \|\| x \|_q.
    \end{equation}
    La chose facile à remarquer est que \( r(0)=f(0)=0\). En ce qui concerne les choses difficiles, vu que \( dr\) est continue (parce que \( r\) est \( C^1\)) il existe un \( \delta>0\) tel que \( \| dr_a \|_q<\epsilon\) dès que \( a\in B_q(0,\delta)\). Si nous prenons \( \| x \|_q<\delta\) alors cette majoration est valable pour tous les éléments sur lequel est pris le supremum dans la formule \eqref{EqIDTHooCsMSVs}. Donc
    \begin{equation}
        \| r(x) \|_q\leq \epsilon\| x \|_q
    \end{equation}
    tant que \( \| x \|_q\leq \delta\). Par conséquent, tant que \(  \| y(t) \|_q\leq \delta\) nous avons \( \| r\big( y(t) \big) \|\leq \epsilon\| y(t) \|_q\). Nous continuons le calcul \eqref{subeqsZCOLooOzTBLr} :
    \begin{subequations}
        \begin{align}
            q\big( y(t) \big)'&\leq Cq(y)+2\epsilon\| y(t) \|_q^2\\
            &=-(C-2\epsilon)q(y).
        \end{align}
    \end{subequations}
    Si \( \epsilon\) est petit on a \( C-2\epsilon >0 \) et on pose \( \beta=C-2\epsilon\) pour écrire
    \begin{equation}    \label{EqEYJIooHvSBic}
        q\big( y(t) \big)'\leq -\beta q\big( y(t) \big)
    \end{equation}
    tant que \( \| y(t) \|_q<\delta\).
    
\item[Si \( q(y_0)<\delta  \) alors \( q\big( y(t) \big) < \delta \)]

    Nous posons\footnote{\( t_1\) est bien définit et est bien un minimum. J'en veux pour preuve que si \( q\big( y(t_s) \big)=\delta\), on peut prendre le minimum seulement sur les \( t\in\mathopen[ 0 , t_s \mathclose]\); or par continuité \( q\big( y(t) \big)=\delta\) définit un fermé. Bref \( t_1\) est un infimum sur un compact (fermé borné) et donc bien un minimum atteint.}
    \begin{subequations}    \label{subeqsFNPJooERJkxO}
        \begin{align}
            t_1=\min\{ t>0\tq q\big( y(t) \big)=\delta \}\\
            t_2=\max\{ t<0\tq q\big( y(t) \big)=\delta \}.
        \end{align}
    \end{subequations}
    L'inégalité \eqref{EqEYJIooHvSBic} est valable pour \( t=0\), \( t=t_1\) et \( t=t_2\); nous l'écrivons pour \( t_1\) :
    \begin{equation}
        q\big( y(t) \big)'_{t=t_1}\leq-\beta q\big( y(t_1) \big)\leq-\beta \delta<0
    \end{equation}
    Nous avons donc \( q\big( y(t_1) \big)=\delta\) et \( q\big( y(t) \big)'_{t=t_1}<0\). Par conséquent pour tout \( t\) proche de \( t_1\) avec \( 0<t<t_1 \) il y a \( q\big( y(t) \big)>\delta\).


Pour la même raison, prise en \( t=0\) nous avons pour tout \( t\) proche de \( 0\) avec \( t>0\) que \( q\big( y(t) \big)<\delta\). Par continuité de \( t\mapsto q\big( y(t) \big)\) cette fonction doit passer par la valeur \( \delta\) dans \( \mathopen] t_2 , 0 \mathclose[\) et \( \mathopen] 0 , t_1 \mathclose[\), ce qui contredit la maximalité de \( t_2\) et la minimalité de \( t_1\).

    Ci-dessous, une partie de ce à quoi ressemble le graphe de \( t\mapsto q\big( y(t) \big)\) :
\begin{center}
   \input{auto/pictures_tex/Fig_ASHYooUVHkak.pstricks}
\end{center}

    Deux conclusions :
    \begin{itemize}
        \item
            Vu que \( q\big( y(t) \big)\) est borné pour tout \( t\in \eR\), nous sommes dans le cas \ref{ItemOLYYooJVkRfj} de l'alternative du théorème d'explosion en temps fini \ref{CorGDJQooNEIvpp}. Donc la solution \( y(t)\) existe sur tout \( \eR\) pourvu que \( \| y_0 \|\) soit assez petit. Plus précisément par équivalence des normes, il existe un nombre \( D>0\) tel que \( \| x \|\geq D\| x \|_q\) pour tout \( x\). Si \( \| y_0 \|\leq D\delta\) alors 
            \begin{equation}
                D\| y_0 \|_q\leq \| y_0 \|\leq D\delta,
            \end{equation}
            qui donne immédiatement \( \| y_0 \|_q\leq \delta\), ce qui faut pour faire fonctionner l'existence de \( y(t)\) pour tout \( t\).
        \item 
            Nous pouvons maintenant d'utiliser l'inégalité \eqref{EqEYJIooHvSBic} pour tout \( t\in \eR\) sous la seule hypothèse que \( q(y_0)<\delta\) au lieu de \( q\big( y(t) \big)<\delta\).
    \end{itemize}

    La partie \ref{ItemGZEAooAhxuDQi} de ce théorème est prouvée; nous passons au reste à la partie \ref{ItemGZEAooAhxuDQii}. Pour cela nous supposons que \( q(y_0)<\delta\).

\item[À propos de \(  e^{\beta t}q(y)\)]

    En sous-entendant la dépendance en \( t\) dans \( y\) nous avons
    \begin{equation}
        \Big(  e^{\beta t}q(y) \Big)'=\beta e^{\beta t}q(y)+ e^{\beta t}q(y)'= e^{\beta t}\big( \beta q(y)+q(y)' \big),
    \end{equation}
    mais nous avons déjà prouvé que \( q(y)'\leq -\beta q(y)\) (équation \eqref{EqEYJIooHvSBic}), donc
    \begin{equation}    \label{EqEJMEooFKuxTv}
        \Big(  e^{\beta t}q(y) \Big)'\leq 0
    \end{equation}

\item[Décroissance exponentielle]
    Si \(t\geq 0\), l'inégalité \eqref{EqEJMEooFKuxTv} donne
    \begin{equation}
        e^{\beta t}q\big( y(t) \big)\leq q(y_0),
    \end{equation}
    c'est à dire
    \begin{equation}
        q\big( y(t) \big)\leq  e^{-\beta t}q(y_0)
    \end{equation}
    lorsque \( t\geq 0\). Par équivalence des normes, nous avons des nombres \( D_1\) et \( D_2\) tels que
    \begin{equation}
        D_1\| x \|_q\leq \| x \|\leq D_2\| x \|_q
    \end{equation}
    pour tout \( x\in \eR^n\). Nous avons donc pour tout \( t\geq 0\) que
    \begin{equation}
        \| y(t) \|\leq D_2\| y(t) \|_q\leq D_2\| y_0 \|_q e^{-\beta t}.
    \end{equation}
    Pour rappel, \( \beta>0\), ce qui prouve la partie \ref{ItemGZEAooAhxuDQii} du théorème.

\item[Point d'équilibre]

    Le point \( y=0\) est point d'équilibre (définition \ref{DefKMCGooOeFKlA}) parce que \( f(0)=0\), donc \( y(t)=0\) fonctionne. Dans ce cas, \( y_0=0\).

\item[Stabilité]

La stabilité est le fait que \( \| y(t) \|_q\leq \delta\) dès que \( \| y_0 \|_q\leq \delta\).
    
\end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Système proie et prédateurs : Lokta-Voltera}
%---------------------------------------------------------------------------------------------------------------------------

Le système de \defe{Lokta-Voltera}{Lokta-Voltera} est l'équation différentielle suivante :
\begin{subequations}
    \begin{numcases}{}
        x'=ax-bxy\\
        y'=-cy+dxy
    \end{numcases}
\end{subequations}
où \( a,b,c,d\) sont des constantes positives et avec la condition \( x(t_0)>0\), \( y(t_0)>0\).

En ce qui concerne l'interprétation des équations\cite{QUMHooCSloAC},
\begin{enumerate}
    \item
        \( x(t)\) est le nombres proies
    \item
        \( y(t)\) est le nombres prédateurs
    \item 
        Les proies ont une reproduction rapide qui mène à une croissance exponentielle en absence de prédation (d'où le terme \( ax\)).
    \item
        Au contraire, les prédateurs meurent (ou migrent) rapidement lorsqu'ils n'ont pas de proies et nous supposons une décroissance exponentielle du nombre de prédateurs en l'absence de proies. D'où le terme \( -cy\) avec le signe négatif.
    \item
        Les termes \( -bxy\) et \( dxy\) sont les termes d'interaction entre le proies et les prédateurs. Ils sont proportionnels à la fréquence de leurs rencontres, lesquelles sont avantageuses pour les prédateurs et problématiques pour les proies.
\end{enumerate}

\begin{theorem}[Lokta-Voltera\cite{PAXrsMn}]            \label{ThoJHCLooHjeCvT}
    Soient des constantes positives \( a,b,c,d\) et le système équations différentielles
    \begin{subequations}
        \begin{numcases}{}
            x'=ax-bxy\\
            y'=-cy+dxy\\
            x(t_0)>0, y(t_0)>0.
        \end{numcases}
    \end{subequations}
 
    Alors
    \begin{enumerate}
        \item
            Les solutions sont positives sur leur domaines.
        \item
            Les solutions existent sur \( \eR\).
        \item
            Les solutions sont périodiques.
    \end{enumerate}
\end{theorem}
\index{théorème!Lokta-Voltera}

\begin{proof}
    Nous divisions la preuve.
    \begin{subproof}
    \item[Comment théorème de Cauchy-Lipschitz s'applique]
        Tel quel, le théorème de Cauchy-Lipschitz \ref{ThokUUlgU} ne s'applique pas parce qu'il demande une condition initiale pour avoir unicité. En ce qui concerne les notations, ce qui est noté «\( y\)» dans le théorème est ici le couple \( x,y\) et la fonction \( f\) est alors
        \begin{equation}
            f\big( t,\begin{pmatrix}
                x    \\ 
                y    
            \end{pmatrix}\big)=\begin{pmatrix}
                ax-bxy    \\ 
                -cy+dxy    
            \end{pmatrix}.
        \end{equation}
        C'est une fonction continue localement Lipschitz partout par le lemme \ref{LemCFZUooVqZmpc} et la proposition \ref{PropGIBZooVsIqfY}.
        
        Nous savons cependant que les solutions sont de classe \( C^1\) et que moyennant la donnée d'une condition initiale, la solution est unique.
    \item[Les solutions restent positives]
        Supposons \( x(s)=0\) pour un certain \( s>t_0\). Alors le solution 
        \begin{subequations}
            \begin{numcases}{}
                x(t)=0\\
                y(t)=\exp(-ct)
            \end{numcases}
        \end{subequations}
        est une solution pour \( \mathopen[ t_0 , s+\epsilon \mathclose]\). Par unicité de la solution avec condition initiale \( s(s)=0\), nous avons aussi \( x(t_0)=0\) pour toutes les solutions, ce qui contredit notre condition.

        De la même façon, avoir \( y(s)=0\) donne une solution avec \( y(t)=0\) pour tout \( t\) et donc une contradiction.

    \item[Solutions sur \( \eR\)]

        Nous montrons maintenant que les solutions sont définies sur \( \eR\).

        Nous avons \( x'<ax\), donc pour tout \( t\) où la solution est définie,
        \begin{equation}
            0<x(t)<x(t_0) e^{a(t-t_0)},
        \end{equation}
        c'est à dire que la solution ne peut pas exploser en temps fini\footnote{Voir le corollaire \ref{CorGDJQooNEIvpp}.} : elle est bornée par le haut et le bas. Elle doit donc exister pour tout \( t\in \eR\). Par ailleurs, \( y'<dxy\) donc
        \begin{equation}
            0<y(t)<y(t_0) e^{d\int_{t_0}^{t}x(s)ds}
        \end{equation}
        qui est également contraire à l'explosion en temps fini.

    \item[4 zones : monotonie]

        Nous divisons \( \eR^2\) en quatre zones d'après les signes de \( a-by\) et \( c-dx\). Nous montrons que dans chacune de ces zones, les solutions sont monotones. Prenons par exemple la partie
        \begin{equation}
            \{  (x,y)\in \eR^2\tq   a-by>0 \}\times\{ c-dx<0 \}.
        \end{equation}
        Vu l'équation \( x'=x(a-by)\), tant que \( \big( x(t),y(t) \big)\) est dans cette zone, la fonction \( x'\) a le signe de \( x\) et est donc positive. Donc \( x\) est croissante dans cette zone.

        De la même façon, \( y'=-y(c-dx)\) est \( y'\) a un signe constant dans la zone.

    \item[4 zones : on bouge]

        Nous prouvons à présent qu'une solution ne reste pas dans une zone. 

        \begin{enumerate}
            \item
        Supposons que \( \big( x(t_0),y(t_0) \big) \) soit dans la zone
        \begin{subequations}
            \begin{align}
                \{ a-by>0 \}&&\times&& \{ c-dx>0 \}\\
                x'>0&&&&y'<0
            \end{align}
        \end{subequations}
        et que la solution reste dans cette zone (pour les \( t>t_0\)). Nous avons en particulier \( x'>0\), donc \( x\) est croissante tout en ayant la borne supérieure \(  x<c/d \). Par conséquent \( x\) a une limite que nous appelons \( x_1\in \mathopen[ 0 , \frac{ c }{ d } \mathclose]\).

        De la même façon,; \( y\) est décroissante et bornée vers le bas par zéro. Donc $y$ a une limite que nous notons \( y_1\in\mathopen[ 0 , y(t_0) \mathclose]\). 

        Vu que \( x\) est bornée et de classe \( C^1\) nous avons forcément \( \lim_{t\to \infty} x'(t)=0\). Mais vu que \( x'=ax-bxy\) nous devons avoir
        \begin{equation}
            ax_1-bx_1y_1=0.
        \end{equation}
        Mais ni \( x_1>0\) donc \( a-by_1=0\), ce qui donne \( y_1=\frac{ a }{ b }\) et aussi \( x_1=\frac{ c }{ d }\). Bref, \( y\) est décroissante et tend vers \( a/b\); donc \( y(t_0)>a/b\), ce qui contredit que \( y(t_0)\) soit dans la zone considérée.

        Étant donné que \( x'>0\) et \( y'<0\), la solution sort de la zone pour entrer dans la zone \ldots
    \item
        Supposons que \( \big( x(t_0),y(t_0) \big) \) soit dans la zone
        \begin{subequations}
            \begin{align}
                \{ a-by>0 \}&&\times&& \{ c-dx<0 \}\\
                x'<0&&&&y'>0
            \end{align}
        \end{subequations}
        et que la solution reste dans cette zone (pour les \( t>t_0\)). Les fonctions \( x\) et \( y\) sont convergentes. Par conséquent \( \ln(y)\) converge aussi et vu que \( x\) est croissante,
        \begin{equation}
            \frac{ y' }{ y }=-c+dx\geq -x+dx(t_0)>0
        \end{equation}
        Cela signifie que \( \ln(y)'\) est toujours positive et bornée par le bas. Cela est impossible si \( y\) est borné. 

        Donc on sort de la zone pour entrer dans \ldots
    \item
        Supposons que \( \big( x(t_0),y(t_0) \big) \) soit dans la zone
        \begin{subequations}
            \begin{align}
                \{ a-by<0 \}&&\times&& \{ c-dx<0 \}\\
                x'<0&&&&y'>0
            \end{align}
        \end{subequations}
        et que la solution reste dans cette zone (pour les \( t>t_0\)). 

        Le même type de raisonnement fait passer à la zone\ldots
    \item
        Supposons que \( \big( x(t_0),y(t_0) \big) \) soit dans la zone
        \begin{subequations}
            \begin{align}
                \{ a-by<0 \}&&\times&& \{ c-dx>0 \}\\
                x'<0&&&&y'<0
            \end{align}
        \end{subequations}
        et que la solution reste dans cette zone (pour les \( t>t_0\)). Encore une fois, cela nous fait sortir de la zone et retourne vers la première zone.
        \end{enumerate}

       À ce moment nous voyons déjà que la relation entre proie et prédateurs, c'est un peu le mythe de Sisyphe

   \item[Une intégrale première]

       Posons la fonction
       \begin{equation}
           H(x,y)=by+dx-a\ln(y)-c\ln(x).
       \end{equation}
       Une simple dérivation montre que  \(  x\mapsto H\big( x(t),y(t) \big) \) est constante. Nous considérons la fonction
       \begin{equation}
           \begin{aligned}
               f\colon \eR&\to \eR \\
               s&\mapsto H\big( \frac{ c }{ d },s \big)
           \end{aligned}
       \end{equation}
       dont la dérivée n'est autre que \( f'(s)=b-\frac{ a }{ s }\). La fonction \( f\) est donc décroissante sur l'intervalle \( \mathopen[ \frac{ a }{ b } , \infty [\) et donc injective. Sur les changements de zones, il existe un \( t_0\) tel que
           \begin{subequations}
               \begin{align}
                   x(t_0)&=\frac{ d }{ c }\\
                   y(t_0)&>0.
               \end{align}
           \end{subequations}
            Pour cette valeur \( t_0\) nous avons alors \( H\big( x(t_0),y(t_0) \big)=  f\big( y(t_0) \big)  \). En posant \( s_0=y(t_0)>0\) nous avons
            \begin{equation}
                H(x_0,y_0)=f(s_0)
            \end{equation}
            et \( f\) étant injective, ce \( s_0\) est la seule valeur de \( s\) à vérifier \( H(x_0,y_0)=f(s)\).

        \item[Conclusion]

            La fonction \( x\) passant d'une zone à l'autre, il existe un \( t_1>t_0\) tel que \( x(t_1)=a/b\). Nous avons évidemment
            \begin{equation}
                H\big( x(t_1),y(t_1) \big)=H(x_0,y_0)
            \end{equation}
            parce que \( H\) est constante le long du mouvement. Cela se traduit par
            \begin{equation}
                H\big( \frac{ a }{ b },y(t_1) \big)=f(s_0),
            \end{equation}
            et donc \( y(t_1)=f(s_0)=y(t_0)\). Avec tout cela nous avons
            \begin{subequations}
                \begin{numcases}{}
                    y(t_1)=y(t_0)\\
                    x(t_1)=x(t_2)=\frac{ a }{ b }.
                \end{numcases}
            \end{subequations}
            Cela est donc un point par lequel la solution repasse. Par unicité de la solution, elle est donc périodique.
    \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Équation du second ordre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Wronskien}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons ici une équation différentielle de la forme
\begin{equation}    \label{EqJDAAnWY}
    y''(t)+q(t)y(t)=0
\end{equation}
Dans ce point nous allons considérer la fonction \( q\) sans hypothèse de périodicité. L'équation de Hill (sous-section \ref{SubSecDWwVVPa}) sera la même équation, mais en supposant que \( q\) est périodique.

Nous commençons par argumenter que si \( q\) est continue, alors l'ensemble des solutions de l'équation \eqref{EqJDAAnWY} est un espace vectoriel de dimension deux. Pour cela il suffit d'appliquer la méthode de réduction de l'ordre (section \ref{SecWGdleRM}) puis le théorème de dimension pour les systèmes linéaires (théorème \ref{ThoNYEXqxO}). En effet si la fonction \( y_1\) est solution de \eqref{EqJDAAnWY} si et seulement si le vecteur \(Y= \begin{pmatrix}
    y_1    \\ 
    y_2    
\end{pmatrix}\) est solution du système linéaire
\begin{equation}
    Y'(t)=\begin{pmatrix}
        0    &   1    \\ 
        -q(t)    &   0    
    \end{pmatrix}Y(t).
\end{equation}

Soient deux solutions \( y_1\) et \( y_2\) de l'équation différentielle. Le \defe{Wronskien}{Wronskien} de ces deux solutions est le déterminant
\begin{equation}
    W(t)=\begin{vmatrix}
        y_1    &   y_2    \\ 
        y'_1    &   y'_2    
    \end{vmatrix}.
\end{equation}
Si nous considérons l'équation différentielle
\begin{equation}
    y''+py'+qy=0,
\end{equation}
le Wronskien peut être déterminé sans savoir explicitement \( y_1\) et \( y_2\) parce que \( W=y_1y'_2-y'_1y_2\), et en dérivant,
\begin{subequations}
    \begin{align}
        W'&=y_1y_2''+y'_1y'_2-y''_1y_2-y'_1y'_2\\
        &=y_1(-py'_2-qy_2)-(-py'_1-qy_1)y_2\\
        &=-p\begin{vmatrix}
            y_1    &   y_2    \\ 
            y'_1    &   y'_2    
        \end{vmatrix},
    \end{align}
\end{subequations}
c'est à dire
\begin{equation}    \label{EqHEMRgM}
    W'=-pW.
\end{equation}
Il suffit donc de savoir une condition initiale pour obtenir une équation différentielle pour \( W\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Avec second membre}
%---------------------------------------------------------------------------------------------------------------------------

Une équation différentielle du second ordre avec un second membre se présente sous la forme
\begin{equation}
	ay''(t)+by'(t)+cy(t)=v(t)
\end{equation}
où $v(t)$ est une fonction donnée. Le truc est de commencer par résoudre l'équation différentielle sans second membre, c'est à dire trouver la fonction $y_H(t)$ telle que
\begin{equation}
	ay''_H(t)+by_H'(t)+cy_H(t)=0.
\end{equation}
Cela se fait en utilisant la méthode du polynôme caractéristique.

Ensuite, il faut trouver une solution particulière $y_P(t)$ de l'équation avec le second membre. Une seule. Pour y parvenir, il faut du doigté et un peu de technique. Il faut faire des essais en fonction de ce à quoi ressemble le $v(t)$ :
\begin{enumerate}

	\item
		Si $v(t)$ est un polynôme, alors il faut essayer un polynôme,

	\item
		Si $v(t)=\cos(\omega t)$ ou bien $v(t)=\sin(\omega t)$, alors essayer $y_P(t)=A\cos(t)+B\sin(\omega t)$,

	\item
		Si $v(t)= e^{\omega t}$, alors essayer $y_P(t)=A e^{\omega t}$.

\end{enumerate}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Équation \texorpdfstring{$y''+q(t)y=0$}{y''+q(t)y=0}}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecSyTwyM}


Nous allons donner quelque propriété des solutions de l'équation
\begin{equation}
    y''+qy=0
\end{equation}
en fonction de telle ou telle hypothèse sur \( q\).

\begin{proposition}
    Si \( q\colon \eR^+\to \eR\) est continue et si
    \begin{equation}
        \int_0^{\infty}| q(t) |dt
    \end{equation}
    converge, alors
    \begin{enumerate}
        \item
            toute solution bornée de \( y''+qy=0\) vérifie \( \lim_{t\to \infty} y'(t)=0\),
        \item
            l'équation \( y''+qy=0\) admet des solutions non bornées.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Soit \( y\) une solution bornée, et intégrons l'équation différentielle entre \( 0\) et \( \infty\) :
    \begin{equation}
        \int_0^{\infty}y''(t)dt=-\int_0^{\infty}q(t)y(t)dt.
    \end{equation}
    La fonction \( y\) étant bornée, l'hypothèse sur \( q\) permet de dire que l'intégrale de droite existe. Par ailleurs,
    \begin{equation}
        \int_0^{\infty}y''=\lim_{a\to \infty}\int_0^ay''=\lim_{a\to \infty}y'(a)-y'(0).
    \end{equation}
    Cela justifie que la limite \( \lim_{t\to \infty} y'(t)\) existe. Posons \( \alpha=\lim_{t\to \infty} y'(t)\) et supposons par l'absurde que \( \alpha\neq 0\). Soit \( \epsilon>0\) et \( \lambda\) assez grand pour que
    \begin{equation}
        \| y'-\alpha \|_{\mathopen[ \lambda , \infty [}<\epsilon.
    \end{equation}
    Soit aussi \( x>\lambda\). Nous avons
    \begin{subequations}
        \begin{align}
            y(x)&=y(\lambda)+\int_{\lambda}^xy'(t)dt\\
            &\geq y(\lambda)\int_{\lambda}^x(\alpha-\epsilon)\\
            &=y(\lambda)+\alpha x-\epsilon\lambda.
        \end{align}
    \end{subequations}
    En prenant la limite des deux côtés on voit que \( y(x)\to \infty\) dès que \( \alpha\neq 0\), ce qui est contraire aux hypothèses. Donc \( \alpha=0\).

    Pour la seconde partie de la proposition, nous devons prouver que l'équation \( y''+qy=0\) possède des solutions non bornées. Si l'équation a seulement des solutions bornées et si \( \{ u,v \}\) est une base de solutions, alors nous avons \( u',v'\to 0\). Si nous reprenons l'équation \eqref{EqHEMRgM} avec \( p=0\) nous savons que dans notre cas le Wronskien satisfait à \( W'=0\), c'est à dire qu'il est constant. Mais vu que \( u\) et \( v\) sont bornées et que les dérivées tendent vers zéro, nous avons \( W(t)\to 0\) et donc \( W(t)=0\).

    Or l'annulation identique du Wronskien contredit que \( \{ u,v \}\) serait une base de solutions. Donc il existe des solutions non bornées.
\end{proof}

\begin{proposition} \label{PropMYskGa}
    Soit l'équation différentielle \( y''+qy=0\). Si \( q\) est \( C^1\), strictement positive et croissante, alors toutes les solutions sont bornées.
\end{proposition}
\index{monotonie}

\begin{proof}
    Soit \( y\) une solution et multiplions l'équation par \( 2y'\) (qui est non nulle par hypothèse) :
    \begin{equation}
        2y'y''+2qy'y=0.
    \end{equation}
    Nous allons intégrer cela en nous souvenant que \( 2y'y''\) est la dérivée de \( (y')^2\). Pour tout \( t>0\) nous avons
    \begin{subequations}
        \begin{align}
            0&=y'(t)^2-y'(0)^2+2\underbrace{\int_0^tq(t)y'(t)y(t)dt}_{\text{par partie}}\\
            &=y'(t)^2-y'(0)^2+2\left( [qy^2]_0^t-\int_0^tq'y^2 \right)\\
        \end{align}
    \end{subequations}
    Le terme qui nous intéresse est celui qui contient \( y(t)\) :
    \begin{equation}
        2q(t)y(t)^2=-y'(t)^2+y'(0)^2+2q(0)y(0)^2+2\int_0^t q'y^2
    \end{equation}
    Nous pouvons majorer \( -y'(t)^2\) par zéro et remplacer toutes les constantes par \( K\) :
    \begin{equation}
        q(t)y(t)^2\leq\int_0^tq'y^2+K=\int_0^t\frac{ q' }{ q }qy^2.
    \end{equation}
    C'est le moment d'utiliser le lemme de Grönwall \ref{LemuBVozy} avec \( \phi=qy^2\) et \( \psi=q'/q\). Les hypothèses de croissance et de positivité ont été posées exprès. Bref, on a
    \begin{subequations}
        \begin{align}
            qy^2&\leq K\exp\left( \int_0^t\frac{ q'(s) }{ q(s) }ds \right)\\
            &=K\exp\left( \ln\frac{ q(t) }{ q(0) } \right)\\
            &=K\frac{ q(t) }{ q(0) }.
        \end{align}
    \end{subequations}
    Notons que \( q(0)\) est strictement positif. Nous déduisons que
    \begin{equation}
        y^2\leq \frac{ K }{ q(0) }
    \end{equation}
    et donc \( y\) est bornée.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Équation de Hill}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecDWwVVPa}

L'équation \defe{de Hill}{équation!différentielle!Hill} est une équation différentielle de la forme 
\begin{equation}    \label{EqPQMvzEZ}
    y''+qy=0
\end{equation}
où
\begin{enumerate}
    \item
        \( q\in C^1(\eR,\eR)\),
    \item
        \( q\) est paire et \( \pi\)-périodique
\end{enumerate}
Nous nous intéressons aux solutions complexes de cette équation différentielle.

Nous nommons \( W\subset C^2(\eR,\eC)\) l'espace des solutions complexes de l'équation \eqref{EqPQMvzEZ}. Nous savons par ce qui a été dit en \ref{subsecSyTwyM} que cet espace est de dimension deux. De plus avec le hypothèses faites ici sur \( q\), nous savons que les solutions sont de classe $C^3$ parce que si \( y\) est une solution, alors l'équation \( y''=qy\) nous indique que \( y\) est \( C^1\) parce que \( y''\) existe (\( y'\) est dérivable et donc continue). Mais si \( y\) est de classe \( C^1\), alors le membre de droite \( qy\) est \( C^1\) et donc \( y''\) est \( C^1\), ce qui prouve que \( y\) est de classe \( C^3\). La récurrence ne va pas plus loin parce que \( q\) est seulement de classe \( C^1\).

Nous considérons l'application de translation
\begin{equation}
    \begin{aligned}
        T\colon C^2(\eR,\eC)&\to C^2(\eR,\eC) \\
        (Ty)(x)&=y(x+\pi). 
    \end{aligned}
\end{equation}
En utilisant la règle de dérivation de fonctions composées, \( (Ty)'=Ty'\) et \( (Ty)''=Ty''\), de telle sorte que si \( u\) est solution de l'équation \eqref{EqPQMvzEZ}, alors \( Tu\) est également solution. Donc \( W\) est un espace stable par \( T\).

Le théorème \ref{ThoNYEXqxO} nous permet de choisir une base de \( W\) en imposant des conditions. Nous choisissons une base \( \{ y_1,y_2 \}\) telles que
\begin{equation}
    \begin{aligned}[]
        y_1(0)&=1       &&  y_2(0)=0\\
        y'_1(0)&=0      &&  y_2'(0)=1.
    \end{aligned}
\end{equation}
Le théorème \ref{ThoNYEXqxO} nous assure que deux telles solutions existent et qu'elles forment une base de \( W\) parce que \( W\) est de dimension \( 2\).

\begin{lemma}[\cite{WNxwuWc}]   \label{IVLzNaU}
    Avec ce choix de base \( \{ y_1,y_2 \}\) la matrice de \( T\) est donnée par
    \begin{equation}
        T=\begin{pmatrix}
            y_1(\pi)    &   y_2(\pi)    \\ 
            y'_1(\pi)    &   y'_2(\pi)    
        \end{pmatrix}.
    \end{equation}
    De plus la fonction \( y_1\) est paire et la fonction \( y_2\) est impaire.
\end{lemma}

\begin{proof}

Cherchons la matrice de \( T\) dans cette base en associant \( \begin{pmatrix}
    1    \\ 
    0    
\end{pmatrix}\) à \( y_1\) et \( \begin{pmatrix}
    0    \\ 
    1    
\end{pmatrix}\) à \( y_2\). Si \( T=\begin{pmatrix}
    a    &   b    \\ 
    c    &   d    
\end{pmatrix}\), alors
\begin{equation}    \label{EqSZhBPGy}
    Ty_1=\begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}\begin{pmatrix}
        1    \\ 
        0    
    \end{pmatrix}=\begin{pmatrix}
        a    \\ 
        c    
    \end{pmatrix}=ay_1+cy_2.
\end{equation}
En évaluant cela en \( t=0\),
\begin{equation}
    (Ty_1)(0)=ay_1(0)+cy_2(0)=a,
\end{equation}
donc \(a=(Ty_1)(0)=y_1(\pi)\). En dérivant \eqref{EqSZhBPGy}, en tenant compte du fait que \( (Ty_1)'=Ty_1'\) et en évaluant en \( t=0\), nous trouvons de même \( c=y'_1(\pi)\). Puis le même cinéma avec \( y_2\) donne
\begin{equation}
    T=\begin{pmatrix}
        y_1(\pi)    &   y_2(\pi)    \\ 
        y'_1(\pi)    &   y'_2(\pi)    
    \end{pmatrix}.
\end{equation}
    
Passons maintenant à la parité de \( y_1\) et \( y_2\). Nous posons \( \psi(t)=y_1(-t)\). Alors \( \psi'(t)=-y_1'(-t)\) et \( \psi''(t)=y_1''(t)\), tant et si bien que
\begin{equation}
    \psi''(t)+q(t)\psi(t)=y_1''(-t)+q(t)y_1(-t)=0.
\end{equation}
donc \( \psi\) est une solution de l'équation. Mais
\begin{subequations}
    \begin{numcases}{}
        \psi(0)=y_1(0)\\
       \psi'(0)=-y'_1(0)=0,
    \end{numcases}
\end{subequations}
donc \( \psi\) a les mêmes conditions initiales que \( y_1\). Par conséquent \( \psi=y_1\) (par le l'unicité donnée dans le théorème de Cauchy-Lipschitz \ref{ThokUUlgU}) et \( y_1\) est paire. Nous procédons de même en partant de \( \varphi(t)=-y_2(-t)\) pour trouver que \( \varphi=y_2\) et que donc que \( y_2\) est impaire.


\end{proof}
Remémorons nous toutefois, pour calmer toute enthousiasme excessif, que \( T\) dépend de deux solutions et donc de la fonction \( q\) donnée dans l'équation.

\begin{proposition}[\cite{KXjFWKA}] \label{PropGJCZcjR}
    Nous considérons l'équation \( y''+qy=0\) et sa base de solutions \( \{ y_1,y_2 \}\) en suivant les notations données plus haut.
    \begin{enumerate}
        \item
            Si \( | \tr(T) |<2\), alors toutes les solutions de l'équation sont bornées.
        \item
            Si \( | \tr(T) |=2\) alors nous avons une solution non bornée.
        \item
            Si \( |\tr(T)|>2\) alors toutes les solutions de l'équation sont non bornées.
        \item
            Le cas \( | \tr(T) |=2\) se présente si et seulement si \( y'_1(\pi)y_2(\pi)=0\).
    \end{enumerate}
\end{proposition}
\index{endomorphisme!sous-espace stable}
\index{endomorphisme!diagonalisable}
\index{équation!différentielle!étude qualitative}
\index{équation!différentielle!système}


\begin{proof}
    Remarquons que le déterminant de la matrice \( T\) est égal au Wronskien des solutions \( y_1\) et \( y_2\) calculé en \( t=\pi\). Calculons sa valeur :
    \begin{equation}
        W(y_1,y_2)=\det\begin{pmatrix}
            y_1    &   y_2    \\ 
            y'_1    &   y'_2    
        \end{pmatrix}=y_1y'_2-y'_1y'_2.
    \end{equation}
    En dérivant et en remplaçant \( y''_i\) par \( -qy_i\), nous trouvons tout de suite \( W(y_1,y_2)'=0\). Donc le Wronskien est constant et il est facile de le calculer en \( t=0\) :
    \begin{equation}
        W(y_1,y_2)(0)=1-0=1.
    \end{equation}
    Donc pour tout \( t\) nous avons \( W(y_1,y_2)(t)=1\). En particulier
    \begin{equation}
        \det(T)=W(y_1,y_2)(\pi)=1,
    \end{equation}
    et notons au passage que \( T\) est inversible.

    Nous écrivons le polynôme caractéristique de \( T\) sous la forme \( \chi_T=X^2-\tr(T)X+\det(T)\), c'est à dire
    \begin{equation}
        \chi_T=X^2-\tr(T)X+1,
    \end{equation}
    dont le discriminant est \( \Delta=\tr(A)^2-4\).

    Nous passons à présent aux différents points de la proposition.
    \begin{enumerate}
        \item
            Si \( | \tr(T) |<2\), alors \( \Delta<0\) et \( \chi_T\) a deux racines complexes conjuguées que nous notons \( \rho\) et \( \bar\rho\). De plus le produit des racines étant le terme indépendant, \( \rho\bar\rho=1\); en particulier \( | \rho |=| \bar \rho |=1\). Notons \( \{ u,v \}\) une base de vecteurs propres : \( Tu=\rho u\) et \( Tv=\bar \rho v\). Il est vite vu que la fonction \( | u |\) est \( \pi\)-périodique :
            \begin{equation}
                | u |(t+\pi)=| u(t+\pi) |=| (Tu)(t) |=| (\rho u)(t) |=| \rho | | u |(t)=| u |(t).
            \end{equation}
            La fonction \( | u |\) est continue\footnote{La fonction \( u\) elle-même n'est cependant pas garantie d'être périodique.} et périodique ergo bornée. La fonction \( | v |\) est bornée pour la même raison et par linéarité, toutes les fonctions de \( W\) sont bornées.

        \item

            Si \( \tr(T)=\pm 2\), alors \( \Delta=0\) et \( \chi_T\) a une racine réelle double\footnote{Ce qui n'implique pas le fait d'avoir deux vecteurs propres pour cette valeur propre, mais tout de même au moins un, voir l'exemple \ref{ExICOJcFp}.} qui doit être \( \pm 1\). Soit \( u\) un vecteur propre de \( T\) pour la valeur propre \( \pm 1\). Nous avons
            \begin{equation}
                | u |(t+\pi)=| Tu(t) |=| \pm u(t) |,
            \end{equation}
            ce qui prouve encore que \( | u |\) est périodique et donc bornée.

            Notons que nous n'avons pas d'informations sur le fait qu'une autre solution soit ou non bornée.

        \item

            Si \( | \tr(T) |>2\), alors \( \chi_T\) a deux racines réelles distinctes \( r\) et \( r'\) avec \( rr'=1\) (toujours les relations coefficients-racines). En raison de quoi \( r'=r^{-1}\) et quitte à échanger \( r\) et \( r'\) nous supposons \( | r |>1\). L'opérateur est maintenant diagonalisable et nous considérons \( \{ u,v \}\) une base de vecteurs propres pour les valeurs propres \( r\) et \( r'\). Une solution non nulle de l'équation s'écrit donc sous la forme
            \begin{equation}
                y=\alpha u+\beta v
            \end{equation}
            avec \( (\alpha,\beta)\neq (0,0)\).

            \begin{itemize}
                \item Si \( \alpha=0\), alors \( \beta\neq 0\) et nous choisissons une valeur \( t\) telle que \( v(t)\neq 0\). Dans ce cas,
                    \begin{equation}
                        y(t+n\pi)=\beta v(t+n\pi)=\beta(T^nv)(t)=\beta (r')^n v(t),
                    \end{equation}
                    et en faisant \( n\to -\infty\) nous obtenons \( \pm \infty\) suivant le signe de \( \beta\).

                \item Si \( \alpha\neq 0\), alors nous fixons\footnote{Mais pas trop hein; nous aurons encore besoin d'assigner à \( t\) d'autres valeurs dans d'autres théorèmes.} \( t\) tel que \( u(t)\neq 0\). Alors
                    \begin{equation}
                        y(t+n\pi)=\alpha r^nu(t)+\beta (r')^n(t).
                    \end{equation}
                    En faisant \( n\to \infty\), nous avons \( (r')^n\to 0\) tandis que le premier terme tend vers \( \pm\infty\) suivant le signe de \( \alpha\).
            \end{itemize}

        \item

            D'abord le théorème de Cayley-Hamilton \ref{ThoCalYWLbJQ} nous indique que \( \chi_T(T)=0\), c'est à dire que
            \begin{equation}    \label{EqFHVSsUO}
                T^2-\tr(T)T+1=0.
            \end{equation}
            Nous avons déjà mentionné le fait que \( T\) était inversible. Multiplions donc \eqref{EqFHVSsUO} par \( T^{-1}\) :
            \begin{equation}    \label{EqPNyjBOy}
                T+T^{-1}=\tr(T)\mtu_2.
            \end{equation}
            Vu que \( T^{-1}\) est l'endomorphisme \( T^{-1}u(t)=u(t-\pi)\), sa matrice est donnée par
            \begin{equation}
                T^{-1}=\begin{pmatrix}
                    y_1(-\pi)    &   y_2(-\pi)    \\ 
                    y'_1(-\pi)    &   y'_2(-\pi)    
                \end{pmatrix}=\begin{pmatrix}
                    y_1(\pi)    &   -y_2(\pi)    \\ 
                    -y'_1(\pi)    &   y'_2(\pi)    
                \end{pmatrix}
            \end{equation}
            où nous avons utilisé le fait que \( y_1\) était paire et \( y_2\) impaire (lemme \ref{IVLzNaU}). Si nous notons \( T=\begin{pmatrix}
                a    &   b    \\ 
                c    &   d    
            \end{pmatrix}\), alors \( T^{-1}=\begin{pmatrix}
                a    &   -b    \\ 
                -c    &   d    
            \end{pmatrix}\) et
            \begin{equation}
                T+T^{-1}=\begin{pmatrix}
                    2a    &     0  \\ 
                       0 &   2b    
                \end{pmatrix}.
            \end{equation}
            L'équation \eqref{EqPNyjBOy} donne alors, vu que \( \tr(T)=a+d\),
            \begin{equation}
                \begin{pmatrix}
                    2a    &   0    \\ 
                    0    &   2b    
                \end{pmatrix}=\begin{pmatrix}
                    a+d    &   0    \\ 
                    0    &   a+d    
                \end{pmatrix},
            \end{equation}
            ce qui donne immédiatement \( a=d\). La matrice de \( T\) a donc comme forme \( T=\begin{pmatrix}
                a    &   b    \\ 
                c    &   a    
            \end{pmatrix}\) et \( \tr(T)=2a\).

            Donc \( \tr(T)=\pm 2\) si et seulement si \( a=\pm 1\) et vu que \( 1=\det(T)=a^2-bc\), nous avons \( a=\pm 1\) si et seulement si \( bc=0\), ce qui signifie exactement \( y'_1(\pi)y_2(\pi)=0\).
    \end{enumerate}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Différents types d'équations différentielles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation homogène}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecEqDiffHomo}

Une équation différentielle \defe{homogène}{équation!différentielle!homogène} est une équation de la forme
\begin{equation}
	y'=f(t,y)
\end{equation}
où $f(\lambda t,\lambda y)=f(t,y)$ pour tout $\lambda\neq 0$.

Elle se présente sous la forme
\begin{equation}
    y'=\frac{ \text{degré } n \text{ en } t,y }{ \text{degré } n \text{ en } t,y },
\end{equation}
avec pas de $y'$ à droite : juste du $y$ et du $t$.

\begin{lemma}
L'équation $y'=f(t,y)$ est homogène si et seulement si $f(t,y)$ est une fonction de $y/t$ seulement.
\end{lemma}
Pour résoudre l'équation homogène, on pose
\begin{equation}		\label{EqDiffHomoPoser}
	z(t)=\frac{ y(t) }{ t },
\end{equation}
donc $tz=y$, et 
\begin{equation}
	y'(t)=tv'(t)+v(t),
\end{equation}
à remettre dans l'équation de départ.
%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation de Bernoulli}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecBernh}

C'est une équation du type
\begin{equation}	\label{EqBerNDiffalp}
	y'=a(t)y+b(t)y^{\alpha}
\end{equation}
où $\alpha\neq 0$ ou $1$. Pour la résoudre, on divise l'équation par $y^{\alpha}$, et on pose $u=y^{1-\alpha}$, et on tombe sur une équation linéaire
\begin{equation}
	u'=(1-\alpha)\big( a(t)u+b(t) \big).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation de \href{http://fr.wikipedia.org/wiki/Jacopo_Riccati}{Riccati}}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecRicatti}

C'est une équation de la forme 
\begin{equation}		\label{EqDiffGFeneRicatti}
	y'=a(t)y^2+b(t)y+c(t).
\end{equation}
\index{équation!de Riccati}

En général, on ne peut pas la résoudre, mais si on en connaît \emph{a priori} des solutions particulières, alors on peut s'en sortir.

\begin{enumerate}

\item 
Si on sait que $y_1(t)$ est une solution, alors on pose
\begin{equation}
	y(t)=y_1(t)+\frac{1}{ u(t) },
\end{equation}
et on obtient une équation linéaire
\begin{equation}
	u'=-\big( 2y_1(t)a(t)+b(t) \big)u-a(t).
\end{equation}

\item
Si $y_1$ et $y_2$ sont solutions, alors nous avons $y$ sous forme implicite
\begin{equation}
	\frac{ y-y_1 }{ y-y_2 }=K e^{\int a(t)\big( y_1(t)-y_2(t) \big)dt}.
\end{equation}
\end{enumerate}

Pour résoudre une équation de Ricatti, il faut donc d'abord deviner une ou deux solutions.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation différentielle exacte}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecEqDiffExacte}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Résolution lorsque tout va bien}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Avant de vous lancer dans les équations différentielles exacte, vous devez lire la section sur les formes différentielles \ref{SecFormDiffRappel}. Une équation différentielle exacte est de la forme $P(t,y)+Q(t,y)y'=0$ que nous allons écrire sous la forme
\begin{equation}		\label{EqExacteDiff}
	P(t,y)dt+Q(t,y)dy=0.
\end{equation}
Nous savons que si $\partial_yP=\partial_tQ$, alors il existe une fonction $f(t,y)$ telle que $Pdt+Qdy=df$. Pour trouver une telle fonction, nous pouvons simplement intégrer la forme $Pdt+Qdy$. En effet, si $\gamma\colon [0,1]\to \eR^2$ est un chemin tel que $\gamma(0)=(0,0)$ et $\gamma(1)=(t,y)$, alors en définissant
\begin{equation}
	f(t,y)=\int_{\gamma}[Pdt+Qdt]=\int_{0}^1\big[ (P\circ\gamma)(u)dt+(Q\circ\gamma)(u) \big]\big( \gamma'(u) \big)du,
\end{equation}
nous avons $df=Pdt+Qdy$. N'importe quel chemin fait l'affaire. Calculons avec $\gamma(u)=(tu,yu)$. La dérivée de ce chemin est donnée par
\begin{equation}
	\gamma'(u)=t\begin{pmatrix}
	1	\\ 
	0	
\end{pmatrix}+y\begin{pmatrix}
	0	\\ 
	1	
\end{pmatrix}.
\end{equation}
Étant donné que $dt\begin{pmatrix}
	a	\\ 
	b	
\end{pmatrix}=a$ et $dy\begin{pmatrix}
	a	\\ 
	b	
\end{pmatrix}=b$, nous avons
\begin{equation}
	\begin{aligned}[]
	f(t,y)&=\int_0^1[Pdt+Qdy]\big( \gamma(u) \big)\left( t\begin{pmatrix}
	1	\\ 
	0	
\end{pmatrix}+y\begin{pmatrix}
	0	\\ 
	1	
\end{pmatrix} \right)du\\
		&=\int_0^1P\big( \gamma(t) \big)tdu+\int_0^1Q\big( \gamma(t) \big)ydu\\
		&=\int_0^1\big[ tP(tu,uy)+yQ(tu,yu) \big]du.
	\end{aligned}
\end{equation}
Nous retrouvons exactement la formule \eqref{EqIMFormI33Fffdd}. Si ça t'étonne, c'est que tu n'as pas compris ;) Dans le cas où nous avons la fonction $f$ qui vérifie $P=\partial_tf$ et $Q=\partial_yf$, l'équation \eqref{EqExacteDiff} devient
\begin{equation}
	\frac{ \partial f }{ \partial t }+\frac{ \partial f }{ \partial y }\frac{ dy }{ dt }=0,
\end{equation}
c'est à dire 
\begin{equation}
	\frac{ d }{ dt }\Big[ f\big( t,y(t) \big) \Big]=0,
\end{equation}
dont la solution
\begin{equation}
	f\big( t,y(t) \big)=C
\end{equation}
donne la solution $y(t)$ sous forme implicite.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Facteur intégrant (quand tout ne va pas bien)}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Si la forme $Pdt+Qdy$ n'est pas exacte, il n'existe pas de fonction $f$ qui résolve l'affaire. Nous pouvons toutefois essayer de trouver un \defe{facteur intégrant}{facteur!intégrant}. Nous cherchons une fonction $M$ telle que
\begin{equation}
	(MP)dt+(MQ)dy
\end{equation}
soit exacte. Nous cherchons donc $M(t,y)$ telle que $\partial_y(MP)=\partial_t(MQ)$. En utilisant la règle de Leibnitz, nous trouvons l'équation suivante pour $M$ :
\begin{equation}		\label{EqDuFacteurIntegrant}
	M(\partial_yP-\partial_tQ)=Q(\partial_tM)-P(\partial_yM).
\end{equation}
Cette équation est en générale extrêmement difficile à résoudre, mais dans certains cas particuliers, il est possible d'en trouver une solution à tâtons.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Distributions pour les équations différentielles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecTNgeNms}

Nous commençons par définir l'espace \(  C^{\infty}\big( \eR,\swS'(\eR^d) \big)\)\nomenclature[Y]{$C^{\infty}\big( \eR,\swS'(\eR^d) \big)$}{Fonctions à valeurs dans les distributions.} en disant que \( t\mapsto u_t\) est dans cet espace si
\begin{enumerate}
    \item
        pour tout \( t\in \eR\) nous avons \( u_t\in \swS'(\eR^d)\),
    \item
        l'application \( t\mapsto u_t\) est de classe \(  C^{\infty}\).
\end{enumerate}
Pour définir ce que nous entendons par une fonction de classe \( C^k\) à valeurs dans \( \swS'(\eR^d)\) nous nous souvenons de la proposition \ref{PropQAuJstI}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Équation de Schrödinger}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Équation de Schrödinger\cite{KXjFWKA}]    \label{ThoLDmNnBR}
    Soit \( g\in\swS'(\eR^d)\) et le problème
    \begin{subequations}
        \begin{numcases}{}
            \partial_t\tilde u-i\Delta \tilde u=0   \label{EqIKhGuiq}\\
            u_0=g
        \end{numcases}
    \end{subequations}
    où \( \tilde u\in C^{\infty}\big( \eR,\swD'(\eR^d) \big)\) est lié à \( u\) par la remarque  \ref{RemZYVkHRT}. Alors
    \begin{enumerate}
        \item   \label{ItemVFracYji}
            Il existe une unique solution dans \( C^{\infty}\big( \eR,\swS'(\eR^d) \big)\).
        \item   \label{ItemVFracYjiii}
            Cette solution \( u\) vérifie de plus \( \tilde u\in\swS'(\eR\times \eR^d)\).
    \end{enumerate}
\end{theorem}
\index{Schrödinger}
\index{distribution!équation de Schrödinger}

\begin{proof}
    Nous allons donner explicitement une fonction \( u\in C^{\infty}\big( \eR,\swS'(\eR^d) \big)\) et nous allons vérifier l'équation \eqref{EqIKhGuiq} en testant sur une fonction \( \psi\in\swS'(\eR\times \eR^d)\). Cela prouvera le point \ref{ItemVFracYjiii} ainsi que la partie existence de \ref{ItemVFracYji}. Dans ce qui suit toutes les transformées de Fourier seront par rapport à la variable \( x\in \eR^d\) ou par rapport à \( \xi\). Jamais par rapport à \( t\in \eR\).

    \begin{subproof}
    \item[Existence]
        Pour \( t\in \eR\) nous posons\footnote{En utilisant la définition \eqref{DefTDkrqkA} du produit d'une distribution par une fonction.}
        \begin{equation}
            u_t=\TF^{-1}(f_t\hat g)
        \end{equation}
        où \( f_t\in\swS(\eR^d)\) est la fonction \( f_t(x)= e^{-it\| x \|^2}\). Pour toute fonction \( \varphi\in\swS(\eR^d)\) nous avons
        \begin{equation}
            u_t(\varphi)=(f\hat g)\big( \TF^{-1}(\varphi) \big)=\hat g\big( f\TF^{-1}(\varphi) \big)=g\Big( \TF\big( f\TF^{-1}(\varphi) \big) \Big).
        \end{equation}
        Le fait que \( \TF^{-1}(\varphi)\) soit une fonction Schwartz fait partie de la proposition \ref{PropKPsjyzT}. Pour chaque \( t\) nous avons bien \( u_t\in\swS'(\Omega)\).

        De plus la fonction \( h(t,x)= e^{-it\| x \|^2}(\TF^{-1}\varphi)(x)\) est dans \(  C^{\infty}(\eR\times \eR^d)\), et par conséquent l'application
        \begin{equation}
            t\mapsto \hat g\big( h(t,.) \big)
        \end{equation}
        est également \(  C^{\infty}\) par la proposition \ref{PropBQUOcyw}. Ceci pour dire que \( u\in C^{\infty}\big( \eR,\swS'(\eR^d) \big)\). Il faut encore vérifier que cette fonction est bien une solution de notre problème. Nous testons cette équation sur \( \psi\in\swS(\eR\times\eR^d)\). Pour alléger les notations nous posons \( \psi_t\colon x\mapsto \psi(t,x)\) et par conséquent aussi \( (\partial_t\psi_t)(x)=(\partial_t\psi)(t,x)\). Nous avons :
        \begin{subequations}
            \begin{align}
                \heartsuit&=(\partial_t\tilde u-i\Delta\tilde u)(\psi)\\
                &=-\tilde u(\partial_t\psi)-i\tilde u(\Delta\psi)\\
                &=-\int_{\eR}u_t\big( (\partial_t\psi_t)+i(\Delta\psi_t) \big)dt
            \end{align}
        \end{subequations}
        Ici nous nous souvenons du lemme \ref{LemYYjFZSa} qui nous dit que nous pouvons permuter \( \TF^{-1}\) et \( \partial_t\). Et pour l'autre terme il faut utiliser le lemme \ref{LemQPVQjCx} avec \( | \alpha |=2\) et une somme pour obtenir que
        \begin{equation}
            \widehat{\Delta\varphi}(x)=-\| x \|^2\hat\varphi(x),
        \end{equation}
        qui dans notre cas s'écrit sous la forme
        \begin{equation}
            \TF^{-1}\Big( (\Delta\psi_t) \Big)(x)=-\| x \|^2\TF^{-1}\psi(t,x).
        \end{equation}
        En remettant bout à bout,
        \begin{subequations}
            \begin{align}
                \heartsuit&=-\int_{\eR}(f_t\hat g)\Big( (\partial_t-i\| . \|^2)\TF^{-1}\psi_t \Big)dt\\
                &=-\int_{\eR}\hat g\Big( x\mapsto  e^{-it\| x \|^2}(\partial_t-i\| x \|^2)(\TF^{-1}\psi)(t,x) \Big)dt
            \end{align}
        \end{subequations}
        Pour alléger les notations nous notons \( \check{\psi_t}(x)=(\TF^{-1}\psi)(t,x)\). Nous avons
        \begin{equation}
            \partial_t\left(  e^{-it\| x \|^2}\check\psi_t(x) \right)=-i\| x \|^2 e^{-it\| x \|^2}\check{\psi_t}(x)+ e^{-it\| x \|^2}(\partial_t\check{\psi_t}),x)= e^{-it\| x \|^2}\big( \partial_t-i\| x \|^2 \big)\check{\psi_t}(x);
        \end{equation}
        cela nous permet d'un peu factoriser une dérivée dans \( \heartsuit\) :
        \begin{subequations}
            \begin{align}
                \heartsuit&=-\int_{\eR}\hat g\left( \partial_t\Big(  e^{-it\| . \|^2}\check{\psi_t}(.) \Big) \right)dt\\
                &=-\int_{\eR}\partial_t\hat g\left(  e^{-it\| . \|^2}\check{\psi_t}(.) \right)dt\\
                &=-\lim_{N\to \infty} \left[ \hat g\Big(  e^{-i\| . \|^2}\check{\psi_t}(.) \Big) \right]_{t=-N}^{t=N}.
            \end{align}
        \end{subequations}
        Histoire de bien comprendre les notations, il ne s'agit pas de calculer \( \hat g\big(  e^{-it\| . \|^2}\check\psi_t \big)\) pour un \( t\) général et de remplacer ensuite \( t\) par \( N\) et \( -N\). En effet la valeur de \( \hat g\big(  e^{-it\| . \|^2}\check\psi_t \big)\) pour un \( t\) donné est celle qu'on obtient en calculant \( \hat g(\ldots)\) après avoir remplacé \( t\) par ce que l'on veut. Par conséquent, en posant \( \varphi(t,\xi)= e^{-i\| \xi \|^2}\check\psi_t(\xi)\) nous avons :
        \begin{subequations}
            \begin{align}
                \heartsuit&=\lim_{N\to \infty} \left[ g\left( x\mapsto\int_{\eR} e^{-ix\xi}\varphi(t,\xi )d\xi \right) \right]_{t=-N}^{t=N}\\
                &=\lim_{N\to \infty} g\left( x\mapsto\int_{\eR} e^{-ix\xi}\varphi(N,\xi )d\xi \right)-\lim_{N\to \infty} g\left( x\mapsto\int_{\eR} e^{-ix\xi}\varphi(-N,\xi )d\xi \right)
            \end{align}
        \end{subequations}
        La limite commute avec \( g\) parce que cette dernière est une distribution (continue). De plus la limite commute avec l'intégrale parce que ce qui est dedans est Schwartz. La fonction \( \varphi\) étant Schwartz, la limite est nulle. Donc
        \begin{equation}
            \heartsuit=0
        \end{equation}
        et la fonction \( u\) proposée est bien une solution de l'équation de Schrödinger dans \(  C^{\infty}(\eR,\swS'(\eR^d))\).

    \item[Unicité]

        Nous considérons deux solutions \( u_1,u_2\in C^{\infty}\big( \eR,\swS'(\eR^d) \big)\) et la fonction \( u=u_1-u_2\) doit satisfaire au problème
        \begin{subequations}
            \begin{numcases}{}
                (\partial_t\tilde u-i\Delta\tilde u)(\psi)=0\\
                u_0=0.
            \end{numcases}
        \end{subequations}
        Nous allons montrer que seule la fonction \( u_t=0\) peut satisfaire à cela pour tout \( \psi\in\swS(\eR\times \eR^d)\). Nous allons même montrer qu'en imposant ces équations seulement sur la partie de \( \swS(\eR\times\eR^d)\) qui est à support compact par rapport à \( \eR\), la seule solution est \( u_t=0\). Soit donc \( \psi\in\swS(\eR\times \eR^d)\) à support compact vis-à-vis de sa variable \( t\). Alors
        \begin{equation}
            0=-\tilde u(\partial_t\psi+i\Delta\psi)=-\int_{\eR}u_t\Big( (\partial_t\psi_t)+i(\Delta\psi_t) \Big)dt
        \end{equation}
        où encore une fois \( \partial_t\psi_t\) est la fonction \( x\mapsto (\partial_t\psi)(t,x)\). Maintenant nous utilisons la proposition \ref{PropUDkgksG} pour dire que 
        \begin{equation}
            \frac{ d }{ dt }\Big( u_t(\psi_t) \Big)=u^{(1)}_t(\psi_t)+u_t\left( \frac{ \partial \psi }{ \partial t }(t,.) \right)
        \end{equation}
        pour écrire
        \begin{equation}
            0=-\int_{\eR}\frac{ d }{ dt }\big( u_t(\psi_t) \big)-u_{t}^{(1)}(\psi_t)+u_t\big( i(\Delta\psi)(t,.) \big)dt
        \end{equation}
        Le premier terme est facile :
        \begin{equation}
            \int_{\eR}\frac{ d }{ dt }\Big( u_t(\psi_t) \Big)dt=\lim_{N\to \infty} \Big[ u_t(\psi_t) \Big]_{t=-N}^{t=N}=0
        \end{equation}
        parce que \( \psi\) est à support compact par rapport à \( t\). Nous restons donc avec
        \begin{equation}
            \int_{\eR}u_{t}^{(1)}(\psi_t)-iu_t\big( (\Delta\psi)(t,.) \big)dt=0
        \end{equation}
        Nous traitons le terme en \( u_t^{(1)}\) en utilisant le fait évident \( T(\varphi)=(\TF T)(\TF^{-1}\varphi)\) et en remarquant le lemme \ref{LemWRoRPIX} :
        \begin{equation}
            u_t^{(1)}(\psi_t)=(\TF u_t^{(1)})(\TF^{-1}\psi_t)=(\TF u)_t^{(1)}(\TF^{-1}\psi_t).
        \end{equation}
        Pour l'autre terme on fait un peu la même chose en nous souvenant ce que fait la transformée de Fourier en traversant le laplacien :
        \begin{equation}
            u_t(\Delta\psi_t)=(\TF u_t)(\TF^{-1}\Delta\psi_t)=(\TF u_t)\big( x\mapsto -\| x \|^2(\TF^{-1}\psi_t)(x) \big).
        \end{equation}
        En recollant encore :
        \begin{equation}    \label{EqHOGaGpt}
            \int_{\eR}(\TF u)^{(1)}_t(\TF^{-1}\psi_t)+i(\TF u_t)\big( \| . \|^2\TF^{-1}\psi_t \big)dt=0.
        \end{equation}
        Cette équation est valable tant que \( \psi\in \swS(\eR\times\eR^d)\) avec support compact en \( t\). Nous allons nous en créer une super cool. D'abord nous choisissons \( \varphi\in\swS(\eR^d)\) et \( \chi\in\swD(\eR)\) et nous considérons\footnote{Le candidat qui parvient à effectivement présenter ça comme développement, il est fort.}
        \begin{equation}    \label{EqEVtJcnz}
            \psi(t,x)=\TF\Big( \xi\mapsto  e^{it\| \xi \|^2}\varphi(\xi)\chi(t) \Big)(x).
        \end{equation}
        Notons que la transformée de Fourier conserve le fait qu'une fonction soit Schwartz\footnote{Proposition \ref{PropKPsjyzT}.}, mais pas le fait d'avoir support compact. Cependant nous ne prenons que la transformée de Fourier par rapport à \( x\). Le résultat est donc une fonction \( \psi\) qui est Schwartz par rapport à \( x\) et support compact par rapport à \( t\). Nous pouvons donc écrire \eqref{EqHOGaGpt} en utilisant la fonction \eqref{EqEVtJcnz} :
        \begin{equation}    \label{EqHPUyZFz}
            0=\int_{\eR}(\TF u)_t^{(1)}\Big( x\mapsto e^{it\| x \|^2}\varphi(x)\chi(t) \Big)+i(\TF u_t)\Big( x\mapsto\| x \|^2 e^{it\| x \|^2}\varphi(x)\chi(t) \Big)dt.
        \end{equation}
        Là dedans, \( \chi(t)\) peut sortir à la fois de la transformée de Fourier et de l'application des distributions; il doit seulement rester dans l'intégrale. Dans le second terme nous allons utiliser l'égalité (due entre autre à la proposition \ref{PropUDkgksG}) :
        \begin{subequations}    \label{EqCRGfbLU}
            \begin{align}
            \frac{ d }{ dt }\big( \hat u_t( e^{it\| . \|^2}\varphi) \big)&=\frac{ d }{ dt }\left( u_t\big( \TF e^{it\| . \|^2}\varphi \big) \right)\\
            &=u_t^{(1)}\big( \TF  e^{it\| . \|^2}\varphi \big)+u_t\left( \frac{ \partial  }{ \partial t }\TF e^{it\| . \|^2}\varphi \right)\\
            &=(\TF u_t^{(1)})\big( x\mapsto  e^{it\| x \|^2}\varphi(x) \big)+(\TF u_t)\big( x\mapsto i\| x \|^2 e^{it\| x \|^2}\varphi(x) \big)\\
            &=(\TF u)_t^{(1)}\big( x\mapsto  e^{it\| x \|^2}\varphi(x) \big)+(\TF u_t)\big( x\mapsto i\| x \|^2 e^{it\| x \|^2}\varphi(x) \big).
            \end{align}
        \end{subequations}
        Et là, magie c'est exactement ce qui est dans \eqref{EqHPUyZFz}. Donc
        \begin{equation}
            \int_{\eR}\frac{ d }{ dt }\hat u_t\big( x\mapsto  e^{it\| x \|^2}\varphi(x) \big)\chi(t)dt=0
        \end{equation}
        pour toute fonctions à support compact \( \chi\). Donc la proposition \ref{PropAAjSURG} nous dit que
        % 13107277
        \begin{equation}
            \partial_t\hat u_t\big( x\mapsto e^{it\| x \|^2}\varphi(x) \big)=0.
        \end{equation}
        C'est zéro partout et non seulement presque partout parce qu'en plus nous avons la continuité. Par conséquent pour tout \( t\in \eR\) nous avons
        \begin{equation}
            \hat u_t\big( x\mapsto e^{it\| x \|^2}\varphi(x) \big)=\hat u_0\big( x\mapsto \varphi(x)\big)=0.
        \end{equation}
        Et cela est vrai pour toute fonction \( \varphi\in\swS(\eR^d)\). Nous considérons donc \( t_0\in \eR\) et une fonction \( \theta\in\swS(\eR^d)\) pour construire
        \begin{equation}
            \varphi(x)= e^{-it_0\| x \|^2}\theta(x).
        \end{equation}
        Nous avons alors \( \hat u_{t_0}\big( x\mapsto\theta(x) \big)=0\), ce qui signifie que \( \hat u_{t_0}=0\). Du coup pour tout \( \theta\in\swS(\eR^d)\) nous avons \( u_{t_0}(\TF\theta)=0\), mais comme la transformée de Fourier est une bijection de \( \swS(\eR^d)\) (proposition \ref{PropKPsjyzT}) nous avons en fait \( u_{t_0}(\theta)=0\) pour tout \( \theta\in\swS(\eR^d)\), c'est à dire \( u_{t_0}=0\) pour tout \( t_0\in \eR\) et au final \( u=0\).
    \end{subproof}
\end{proof}
