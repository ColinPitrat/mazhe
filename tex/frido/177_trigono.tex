% This is part of (everything) I know in mathematics
% Copyright (c) 2011-2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Très modeste approximation de \texorpdfstring{$ \pi$}{pi}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous sommes en droit de vouloir une valeur approchée de \( \pi\).
\begin{lemma}       \label{LEMooJWSGooExmtDA}
    Nous avons l'approximation numérique
    \begin{equation}
        2\sqrt{ 2 }<\pi<4.
    \end{equation}
\end{lemma}

\begin{proof}
    Grace au lemme~\ref{LEMooIGNPooPEctJy} nous savons que la fonction \( \sin\) passe de \( 0\) à \( \sqrt{ 2 }/2\) sur un intervalle de taille \( \pi/4\) avec une dérivé majorée par \( 1\). Par conséquent
    \begin{equation}
        \frac{ \pi }{ 4 }>\frac{ \sqrt{ 2 } }{2}
    \end{equation}
    et donc\footnote{Sérieusement, êtes vous capables de trouver une approximation de \( \sqrt{ 2 }\) en ne vous basant que sur des choses vues jusqu'ici ?}
    \begin{equation}
        \pi>2\sqrt{ 2 }\simeq 2.82
    \end{equation}
    De plus la fonction \( \sin\) passe de \( 0\) à \( \sqrt{ 2 }/2\) sur un intervalle de taille \( \pi/4\) avec une dérivée majorée par \( \sqrt{ 2 }/2\), donc
    \begin{equation}
        \frac{ \pi }{ 4 }<\frac{ \sqrt{ 2 }/2 }{ \sqrt{ 2 }/2 },
    \end{equation}
    ce qui donne
    \begin{equation}
        \pi<4.
    \end{equation}
\end{proof}

Pour avoir une meilleur approximation de \( \pi\), nous pouvons remarquer que \( \pi\in\mathopen] 2.82 , 4 \mathclose[\), et que cet intervalle est suffisamment petit pour ne pas recouvrir l'intervalle correspondant pour \( 2\pi\). L'équation \( \cos(x)=-1\) possède donc une unique solution dans cet intervalle (et cette solution est \( \pi\)). Nous pouvons donc faire une dichotomie pour trouver la valeur de \( \pi\), pourvu que nous ayons une façon d'évaluer des valeurs de \( \cos(x)\) de façon pas trop ridicule.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Cercle trigonométriques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition}[\cite{ooIEJXooIYpBbd}]      \label{PROPooWZFGooMVLtFz}
    Soient des fonctions \( f,g\colon I\to \eR\) de classe \(  C^{1}\) sur l'ouvert \( I\) de \( \eR\) telles que \( f^2+g^2=1\). Soit \( t_0\in I\) et \( \theta_0\) tel que \( f(t_0)=\cos(\theta_0)\) et \( g(t_0)=\sin(\theta_0)\).

    Alors il existe une unique fonction continue \( \theta\colon I\to \eR\) telle que
    \begin{subequations}
        \begin{numcases}{}
            \theta(t_0)=\theta_0\\
            f=\cos\circ \theta\\
            g=\sin\circ \theta.
        \end{numcases}
    \end{subequations}
\end{proposition}

\begin{proof}
    Nous commençons par l'existence, en passant par les nombres complexes. Soit \( h\colon I\to \eC\) définie par \( h=f+ig\). Nous avons \( h\bar h=1\) et nous définissons
    \begin{equation}
        \theta(t)=\theta_0-i\int_{t_0}^th'(s)\overline{ h(s) }ds.
    \end{equation}
    Cette intégrale existe pour tout \( t\) parce que les fonctions \( f\) et \( g\) étant de classe \(  C^{\infty}\), elles sont bornées sur le compact \( \mathopen[ t_0 , t  \mathclose]\). De plus \( \theta\) est une fonction continue parce que c'est une primitive (proposition~\ref{PropEZFRsMj})\footnote{En réalité nous appliquons la proposition~\ref{PropEQRooQXazLz} à chacune des parties réelles et imaginaires de la fonction $s\mapsto h'(s)\overline{ h(s) }$.}.

    La dérivée de \( \theta\) est la fonction \( s\mapsto -i h'(s)\overline{ h(s) }\).

    Utilisant la formule du lemme~\ref{LEMooHOYZooKQTsXW} sur la forme trigonométrique des nombres complexes, nous calculons :
    \begin{equation}
        \Dsdd{ h e^{-i\theta} }{t}{0}= e^{-i\theta}(h'-h\theta')= e^{-i\theta}(h'-ih(-i)h'\bar h)=0.
    \end{equation}
    Par conséquent il existe \( c\in \eC\) tel que \( h e^{-i\theta}=c\). Mais \( h(t_0)=f(t_0)+ig(t_0)=\cos(\theta_0)+i\sin(\theta_0)= e^{i\theta_0}\), du coup
    \begin{equation}
        h(t_0) e^{-i\theta(t_0)}=c
    \end{equation}
    donne immédiatement \( c=1\), ou encore \(  e^{i\theta(t)}=h(t)\), c'est-à-dire que
    \begin{equation}
        f+ig=\cos\circ\theta+i\sin\circ\theta,
    \end{equation}
    ce qu'il fallait pour l'existence.

    Pour l'unicité nous supposons avoir une autre fonction, \(\alpha\) qui satisfait aux exigences. Pour tout \( t\in I\) nous avons
    \begin{equation}
        e^{i\theta(t)}= e^{i\alpha(t)}.
    \end{equation}
    Il existe donc une fonction \( n\colon I\to \eN\) telle que \( \theta(t)=\alpha(t)+2n(t)\pi\). Par continuité de \( \theta\) et \( \alpha\), la fonction \( n\) doit être constante, mais vu que \( \theta(t_0)=\alpha(t_0)\) nous avons \( n=1\).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Les fonctions tangente et arc tangente}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    La fonction \defe{tangente}{tangente} est :
    \begin{equation}
        \tan(x)=\frac{ \sin(x) }{ \cos(x) }
    \end{equation}
    où \( \sin\) et \( \cos\) sont de la définition~\ref{PROPooZXPVooBjONka}.
\end{definition}
La fonction tangente n'est pas définie sur les points de la forme \( x=\frac{ \pi }{2}+k\pi\), \( k\in \eZ\). Une interprétation géométrique, qui justifie le nom, est donnée sur la figure~\ref{LabelFigTgCercleTrigono}.
\newcommand{\CaptionFigTgCercleTrigono}{Interprétation géométrique de la fonction tangente. La tangente de l'angle $\theta$ est positive (et un peu plus grande que $1$) tandis que celle de la tangente de l'angle $\varphi$ est négative.}
\input{auto/pictures_tex/Fig_TgCercleTrigono.pstricks}

\begin{proposition}
    La fonction
    \begin{equation}
        \begin{aligned}
        \tan\colon \mathopen] -\frac{ \pi }{ 2 } , \frac{ \pi }{2} \mathclose[&\to \eR \\
            x&\mapsto \tan(x)
        \end{aligned}
    \end{equation}
    est une bijection.
\end{proposition}

\begin{proof}
    Le cosinus ne s'annulant pas sur l'intervalle donné, la fonction est bien définie. Nous avons
    \begin{equation}
        \lim_{x\to \pi/2^-} \tan(x)=+\infty
    \end{equation}
    parce que la limite du sinus est \( 1\) est celle du cosinus est zéro par les valeurs positives. Le même raisonnement donne la limite en \( -\pi/2\) qui vaut \( -\infty\). Le théorème des valeurs intermédiaires\footnote{Théorème~\ref{ThoValInter}.} dit que la fonction tangente est alors surjective sur \( \eR\).

    Par ailleurs en utilisant les règles de calcul comme la dérivation du quotient~\ref{PROPooOUZOooEcYKxn}\ref{ITEMooMUNQooLiKffz} nous trouvons
    \begin{equation}
        \tan'(x)=\tan^2(x)+1,
    \end{equation}
    ce qui nous donne une dérivée partout strictement positive, et donc une fonction strictement croissante et donc injective.
\end{proof}

Le graphe de la fonction tangente est sur la figure~\ref{LabelFigPVJooJDyNAg}. % From file PVJooJDyNAg
\newcommand{\CaptionFigPVJooJDyNAg}{Le graphe de la fonction tangente.}
\input{auto/pictures_tex/Fig_PVJooJDyNAg.pstricks}

En ce qui concerne la bijection réciproque nous avons le théorème suivant.
\begin{theorem}     \label{THOooUSVGooOAnCvC}
    La fonction inverse de la tangente,
    \begin{equation}
        \begin{aligned}
        \arctan\colon \eR&\to \left] -\frac{ \pi }{2} , \frac{ \pi }{2} \right[ \\
            x&\mapsto \arctan(x)
        \end{aligned}
    \end{equation}
    nommée \defe{arc tangente}{arc tangente} est
    \begin{enumerate}
        \item
            impaire et strictement croissante sur \( \eR\).
        \item       \label{ITEMooMNHLooOVhIIb}
            dérivable sur \( \eR\) de dérivée
            \begin{equation}        \label{EQooGCHGooPlwYWt}
                \arctan'(x)=\frac{1}{ 1+x^2 }.
            \end{equation}
    \end{enumerate}
\end{theorem}

\begin{proof}
    Il est immédiatement visible sur son développement de définition \eqref{EQooCMRFooCTtpge} que la fonction sinus est impaire. Une vérification similaire montre que la fonction cosinus est paire. La fonction tangente est alors impaire et sa réciproque l'est tout autant.

    La fonction arc tangente est également dérivable (donc continue) par la proposition~\ref{PropMRBooXnnDLq} parce que la fonction tangente l'est. Notons qu'ici nous nous sommes restreint à \( \mathopen] -\pi/2 , \pi/2 \mathclose[\). Sinon, le résultat est faux.

    La formule proposée pour la dérivée provient également de la proposition~\ref{PropMRBooXnnDLq} et de la dérivée de la tangente :
\end{proof}

\begin{lemma}       \label{LEMooHRDCooGtnyeQ}
    Nous avons les limites
    \begin{enumerate}
        \item
            $\lim_{x\to \infty} \arctan(x)=\frac{ \pi }{2}$,
        \item
            \( \lim_{x\to -\infty} \arctan(x)=-\frac{ \pi }{2}\).
    \end{enumerate}
\end{lemma}

\begin{lemma}       \label{LEMooJKIUooEMMOrs}
    Nous avons la valeur remarquable
    \begin{equation}
        \arctan(1/\sqrt{ 3 })=\frac{ \pi }{ 6 }.
    \end{equation}
\end{lemma}

Le nombre \( \arctan(x_0)\) se calcule en cherchant l'angle \( \theta\in\mathopen[ -\frac{ \pi }{2} , \frac{ \pi }{2} \mathclose]\) dont la tangente vaut \( x_0\). Nous obtenons le tableau de valeurs suivant :
\begin{equation*}
    \begin{array}[]{|c|c|c|c|c|}
        \hline
        x&0&\frac{1}{ \sqrt{3} }&1&\sqrt{3}\\
        \hline
        \arctan(x)&0&\frac{ \pi }{ 6 }&\frac{ \pi }{ 4 }&\frac{ \pi }{ 3 }\\
        \hline
    \end{array}
\end{equation*}

En ce qui concerne la représentation graphique de la fonction \( x\mapsto\arctan(x)\), elle s'obtient «en retournant» la partie entre \( -\frac{ \pi }{2}\) et \( \frac{ \pi }{ 2 }\) du graphique de la fonction tangente :
\begin{center}
   \input{auto/pictures_tex/Fig_UQZooGFLNEq.pstricks}
\end{center}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{La fonction arc sinus}
%---------------------------------------------------------------------------------------------------------------------------

Nous voulons étudier la fonction
\begin{equation}
    \begin{aligned}
        \sin\colon \eR&\to \mathopen[ -1 , 1 \mathclose] \\
        x&\mapsto \sin(x)
    \end{aligned}
\end{equation}
et sa réciproque éventuelle.

La fonction sinus est continue sur \( \eR\) mais n'est pas bijective : elle prend une infinité de fois chaque valeur de \( J=\mathopen[ -1 , 1 \mathclose]\). Pour définir une bijection réciproque de la fonction sinus en utilisant le théorème~\ref{ThoKBRooQKXThd}, nous devons donc choisir un intervalle à partir duquel la fonction sinus est monotone. Nous choisissons l'intervalle
\begin{equation}
    I=\mathopen[ -\frac{ \pi }{ 2 } , \frac{ \pi }{2} \mathclose].
\end{equation}
La fonction
\begin{equation}
    \begin{aligned}
        \sin\colon \mathopen[ -\frac{ \pi }{2} , \frac{ \pi }{2} \mathclose]&\to \mathopen[ -1 , 1 \mathclose] \\
        x&\mapsto \sin(x)
    \end{aligned}
\end{equation}
est une bijection croissante et continue. Nous avons donc le résultat suivant.
\begin{theorem}[Définition et propriétés de arc sinus]
    Nous nommons \defe{arc sinus}{arc sinus} la bijection inverse de la fonction \( \sin\colon I\to J\). La fonction
    \begin{equation}
        \begin{aligned}
            \arcsin\colon \mathopen[ -1 , 1 \mathclose]&\to \mathopen[ -\frac{ \pi }{2} , \frac{ \pi }{2} \mathclose] \\
            x&\mapsto \arcsin(x)
        \end{aligned}
    \end{equation}
    ainsi définie est
    \begin{enumerate}
        \item
            continue et strictement croissante;
        \item
            impaire : pour tout \( x\in\mathopen[ -1 , 1 \mathclose]\) nous avons \( \arcsin(-x)=-\arcsin(x)\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous prouvons le fait que \( \arcsin\) est impaire. Un élément de l'ensemble de définition de \( \arcsin\) est de la forme \( y=\sin(x)\) avec \( x\in\mathopen[ -\pi/2 , \pi/2 \mathclose]\). La relation \eqref{EqHQRooNmLYbF} s'écrit dans notre cas
    \begin{equation}    \label{EqVUWooUwVxVp}
        x=\arcsin\big( \sin(x) \big).
    \end{equation}
    Nous écrivons d'une part cette équation avec \( -x\) au lieu de \( x\) :
    \begin{equation}    \label{EqRLYooIwOvSz}
        -x=\arcsin\big( \sin(-x) \big)=\arcsin\big( -\sin(x) \big)=\arcsin(-y);
    \end{equation}
    et d'autre part nous multiplions \eqref{EqVUWooUwVxVp} par \( -1\) :
    \begin{equation}    \label{EqTGIooDeRYyT}
        -x=-\arcsin\big( \sin(x) \big)=-\arcsin(y).
    \end{equation}
    En égalisant les valeurs \eqref{EqRLYooIwOvSz} et \eqref{EqTGIooDeRYyT} nous trouvons
    \begin{equation}
        \arcsin(-y)=-\arcsin(y),
    \end{equation}
    ce qui signifie que \( \arcsin\) est une fonction impaire.
\end{proof}
Notons que cette preuve repose sur le fait que tout élément de l'ensemble de définition de la fonction arc sinus peut être écrit sous la forme \( \sin(x)\) pour un certain \( x\).

Si \( x_0\in\mathopen[ -1 , 1 \mathclose]\) est donné, calculer \( \arcsin(x_0)\) revient à trouver un angle \( \theta_0\) dans \( \mathopen[ -\frac{ \pi }{2} , \frac{ \pi }{2} \mathclose]\) pour lequel \( \sin(\theta_0)=x_0\). Un tel angle sera forcément unique.

\begin{remark}
  La définition de arc sinus découle du choix de l'intervalle $I$, qui est une convention. Il aurait été possible de faire un choix différent : pourriez vous trouver la réciproque de la fonction sinus sur l'intervalle $[\pi/2, 3\pi/2]$ ? Le mieux est de l'écrire comme une translatée de arc sinus, en utilisant le fait que sinus est une fonction périodique.
\end{remark}

\begin{example}
    Pour calculer \( \arcsin(1)\), il faut chercher un angle entre \( -\frac{ \pi }{2}\) et \( \frac{ \pi }{ 2 }\) ayant \( 1\) pour sinus : résoudre \( \sin(\theta)=1\). La solution est \( \theta=\frac{ \pi }{2}\) et nous avons donc \( \arcsin(1)=\frac{ \pi }{2}\).
\end{example}

À l'aide des valeurs remarquables de la fonction sinus nous obtenons le tableau suivant de valeurs remarquables pour l'arc sinus.
\begin{equation*}
    \begin{array}[]{|c|c|c|c|c|c|}
        \hline
        x&0&\frac{ 1 }{2}&\frac{ \sqrt{2} }{2}&\frac{ \sqrt{3} }{2}&1\\
          \hline
          \arcsin(x)&0&\frac{ \pi }{ 6 }&\frac{ \pi }{ 4 }&\frac{ \pi }{ 3 }&\frac{ \pi }{ 2 }\\
          \hline
           \end{array}
\end{equation*}
Les autres valeurs remarquables peuvent être déduites du fait que l'arc sinus est une fonction impaire.

En ce qui concerne la dérivabilité de la fonction arc sinus, en application de la proposition~\ref{PropMRBooXnnDLq} elle est dérivable en tout \( y=\sin(x)\) tel que \( \sin'(x)\neq 0\), c'est-à-dire tel que \( \cos(x)\neq 0\). Or \( \cos(x)=0\) pour \( x=\pm\frac{ \pi }{2}\), ce qui correspond à \( y=\sin(\pm\frac{ \pi }{2})=\pm 1\). La fonction arc sinus est donc dérivable sur \( \mathopen] -1 , 1 \mathclose[\). Nous avons donc la propriété suivante pour la dérivabilité.

\begin{proposition}
    La fonction arc sinus est continue sur \( \mathopen[ -1 , 1 \mathclose]\) et dérivable sur \( \mathopen] -1 , 1 \mathclose[\). Pour tout \( y\in\mathopen] -1 , 1 \mathclose[\), la dérivée est donnée par la formule \eqref{EqWWAooBRFNsv}, qui dans ce cas s'écrit
        \begin{equation}
            \arcsin'(y)=\frac{1}{ \cos\big( \arcsin(y) \big) }=\frac{1}{ \sqrt{1-y^2} }.
        \end{equation}
\end{proposition}
La dernière égalité viens du fait que si $x=\arcsin(y)$ alors $y = \sin(x)$ et $\cos(x)= \sqrt{1-\sin^2(x)} = \sqrt{1-y^2}$.

Pour comprendre la dernière égalité, remarquer que dans le dessin suivant, \( \theta=\arcsin(y)\), donc $y = \sin(\theta)$, et \( x=\cos(\theta)\).
\begin{center}
    \input{auto/pictures_tex/Fig_BIFooDsvVHb.pstricks}
\end{center}

Notons enfin que le graphe de la fonction arc sinus est donné à la figure~\ref{LabelFigFGRooDhFkch}. % From file FGRooDhFkch
\newcommand{\CaptionFigFGRooDhFkch}{Le graphe de la fonction \( x\mapsto \arcsin(x)\)}
\input{auto/pictures_tex/Fig_FGRooDhFkch.pstricks}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{La fonction arc cosinus}
%---------------------------------------------------------------------------------------------------------------------------

Nous voulons étudier la fonction
\begin{equation}
        \cos\colon \eR\to \mathopen[ -1 , 1 \mathclose]
\end{equation}
et son éventuelle réciproque. Encore une fois il n'est pas possible d'en prendre la réciproque globale parce que ce n'est pas une bijection; ne fut-ce que parce qu'elle est périodique (proposition~\ref{PROPooFRVCooKSgYUM}). Nous choisissons de considérer l'intervalle \( \mathopen[ 0 , \pi \mathclose]\) sur lequel la fonction cosinus est continue et strictement monotone décroissante.

Nous avons alors le résultat suivant :

\begin{propositionDef}     \label{PROPooZOZHooSMoYQD}
    Pour définir la fonction arcsinus.

    \begin{enumerate}
        \item
    La fonction
    \begin{equation}
            \cos\colon \mathopen[ 0 , \pi \mathclose]\to \mathopen[ -1 , 1 \mathclose]
    \end{equation}
    est une bijection continue strictement décroissante.
    \item
    Sa bijection réciproque est la fonction
    \begin{equation}
            \arccos\colon \mathopen[ -1 , 1 \mathclose]\to \mathopen[ 0 , \pi \mathclose] \\
    \end{equation}
    nommée \defe{arc cosinus}{arc cosinus}.
    \item
        La fonction arc cosinus est continue, strictement décroissante.
    \item
        Elle est dérivable et pour tout \( y\in\mathopen] -1 , 1 \mathclose[\), sa dérivée est donnée par
    \begin{equation}
        \arccos'(y)=\frac{1}{ -\sin\big( \arccos(y) \big) }=\frac{ -1 }{ \sqrt{1-y^2} }.
    \end{equation}
    \end{enumerate}
\end{propositionDef}

\begin{proof}
    La fonction cosinus est continue et même de classe \(  C^{\infty}\) par la proposition~\ref{PROPooZXPVooBjONka}. Elle est strictement décroissant parce que sa dérivée (\( -\sin\)) y est strictement positive (strictement à dans l'intérieur du domaine).

    Le fait que arc cosinus soit une bijection continue strictement monotone est dans le théorème de la bijection~\ref{ThoKBRooQKXThd}. La dérivabilité et la formule sont de la proposition~\ref{PropMRBooXnnDLq}.
\end{proof}

Pour \( y_0\in\mathopen[ -1 , 1 \mathclose]\), trouver la valeur de \( \arccos(y_0)\) revient à résoudre l'équation \( \cos(x_0)=y_0\). Cela nous permet de construire une tableau de valeurs :
\begin{equation*}
    \begin{array}[]{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        x&-1&-\frac{ \sqrt{3} }{2}&-\frac{ \sqrt{2} }{2}&-\frac{ 1 }{2}&0&\frac{ 1 }{2}&\frac{ \sqrt{2} }{2}&\frac{ \sqrt{3} }{2}&1\\
          \hline
          \arccos(x)&\pi&\frac{ 5\pi }{ 6 }&\frac{ 3 }{ 4 }\pi&\frac{ 2 }{ 3 }\pi&\frac{ 1 }{2}\pi&\frac{ \pi }{ 3 }&\frac{1}{ 4 }\pi&\frac{1}{ 6 }\pi&0\\
          \hline
           \end{array}
\end{equation*}

\begin{remark}
    Certes la fonction cosinus est paire (vue sur \( \eR\)), mais la fonction arc cosinus ne l'est pas car elle est une bijection entre \(\mathopen[ -1 , 1 \mathclose]\) et \(\mathopen[ 0 , \pi \mathclose]\).
\end{remark}

\begin{example}
    Cherchons \( \arccos(\frac{ 1 }{2})\). Il faut trouver un angle \( \theta\in\mathopen[ 0 , \pi \mathclose]\) tel que \( \cos(\theta)=\frac{ 1 }{2}\). La solution est \( \theta=\frac{ \pi }{ 3 }\). Donc \( \arccos(\frac{ 1 }{2})=\frac{ \pi }{ 3 }\).

    Il n'est cependant pas immédiat d'en déduire la valeur de \( \arccos(-\frac{ 1 }{2})\). En effet \( \theta=\arccos(-\frac{ 1 }{2})\) si et seulement si \( \cos(\theta)=-\frac{ 1 }{2}\) avec \( \theta\in\mathopen[ 0 , \pi \mathclose]\). La solution est \( \theta=\frac{ 2\pi }{ 3 }\).
\end{example}

En ce qui concerne la représentation graphique, il suffit de tracer la fonction cosinus entre \( 0\) et \( \pi\) puis de prendre le symétrique par rapport à la droite \( y=x\).

\begin{center}
    \input{auto/pictures_tex/Fig_GMIooJvcCXg.pstricks}
\end{center}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Une meilleure approximation de \( \pi\)}
%---------------------------------------------------------------------------------------------------------------------------

Nous avions laissé le nombre \( \pi\) avec l'approximation assez minable de \( 2\sqrt{ 2 }<\pi<4\) en le lemme~\ref{LEMooJWSGooExmtDA}. Nous pouvons maintenant faire nettement mieux.

Le lemme~\ref{LEMooJKIUooEMMOrs} donne
\begin{equation}
    \arctan(1/\sqrt{ 3 })=\pi/6
\end{equation}
et l'idée est de donner un développement de \( \arctan\) autour de zéro, de l'évaluer en \( 1/\sqrt{ 3 }\) et d'égaliser le résultat à \( \pi/6\). Tout cela donne lieu à des calcules peut-être fastidieux, mais comme un gars l'a fait dès l'an 1424\cite{ooOMUNooGROVUu} pour trouver \( 16\) décimales correctes, nous faisons comme si c'était facile.

Pour trouver le développement en série de Taylor (théorème~\ref{ThoTaylor}) de arc tangente autour de \( x=0\), il faut partir de la formule \eqref{EQooGCHGooPlwYWt} et sans doute pas mal calculer et faire une récurrence\quext{Je n'ai pas fait le calcul, merci de me faire savoir si il y a une astuce.}. Le résultat est :
\begin{equation}
    \arctan(x)=\sum_{k=0}^{\infty}\frac{ (-1)^{k}x^{2k+1} }{ 2k+1 },
\end{equation}
valable pour \( x\in \mathopen] -1 , 1 \mathclose[\). Avec cela nous avons
\begin{equation}
    \arctan(\frac{1}{ \sqrt{ 3 } })=\sum_{k=0}^{\infty}\frac{ (-1)^k }{ (2k+1)3^k }\times \frac{1}{ \sqrt{ 3 } }=\frac{ \pi }{ 6 },
\end{equation}
et donc
\begin{equation}
    \pi=\frac{ 6 }{ \sqrt{ 3 } }\sum_{k=0}^{\infty}\frac{ (-1)k }{ (2k+1)3^k }.
\end{equation}

Pour donner une idée du fait que ça fonctionne pas mal, voici le calcul pour quelques termes :
\lstinputlisting{tex/sage/sageSnip012.sage}
Calculer \( 5\) termes donne déjà \( 3.15\). Et on est à \( 10^{-6}\) de la bonne réponse avec \( 20\) termes. Et avec $58$ termes, on n'est à \( 10^{-16}\).

\begin{probleme}
    Pour bien faire, il faudrait étudier le reste et donner un encadrement.
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
\subsection[Forme trigonométrique des nombres complexes]{Forme polaire ou trigonométrique des nombres complexes}
%---------------------------------------------------------------------------------------------------------------------------

Un nombre complexe étant représenté par deux nombres, on peut le représenter dans un plan appelé « plan de Gauss ». La plupart des opérations sur les nombres complexes ont leur interprétation géométrique dans ce plan.

Dans le plan de Gauss, le module d'un complexe $z$ représente la distance entre $0$ et $z$. On appelle \Defn{argument} de $z$ (noté $\arg z$) l'angle (déterminé à $2\pi$ près) entre le demi-axe des réels positifs et la demi-droite qui part de $0$ et passe par $z$. Le module et l'argument d'un complexe permettent de déterminer univoquement ce complexe puisqu'on a la formule
\[z = a + bi = \module z \left( \cos(\arg(z)) + i \sin(\arg(z)) \right)\]

L'argument de $z$ se détermine via les formules
\[\frac a {\module z} = \cos(\arg(z)) \quad \frac b {\module z} = \sin(\arg(z))\]
ou encore par la formule
\[
\frac b a = \tan(\arg(z)) \quad \text{en vérifiant le quadrant.}
\]
La vérification du quadrant vient de ce que la tangente ne détermine l'angle qu'à $\pi$ près.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Angle entre deux vecteurs}
%---------------------------------------------------------------------------------------------------------------------------

\begin{propositionDef} \label{DEFooSVDZooPWHwFQ}
    Soient des vecteurs \( X,Y\in \eR^2\). Il existe un unique \( \theta\in \mathopen[ 0 , \pi \mathclose]\) tel que
    \begin{equation}		\label{eqDefAngleVect}
        \cos(\theta)=\frac{ X\cdot Y }{ \| X \|\| Y \| }.
    \end{equation}
    Ce réel est appelé \defe{angle}{angle entre deux vecteurs} entre \( X\) et \( Y\).
\end{propositionDef}

\begin{proof}
    Si $a$ et $b$ sont des réels, l'inégalité $| a |\leq b$ peut se développer en une double inégalité
    \begin{equation}
        -b\leq a\leq b.
    \end{equation}
    L'inégalité de Cauchy-Schwarz \eqref{EQooZDSHooWPcryG} devient alors
    \begin{equation}
        -\| X \|\| Y \|\leq X\cdot Y\leq\| X \|\| Y \|.
    \end{equation}
    Si $X\neq 0$ et $Y\neq 0$, nous en déduisons
    \begin{equation}
        -1\leq\frac{ X\cdot Y }{ \| X \|\| Y \| }\leq 1.
    \end{equation}
    Il existe donc par la proposition~\ref{PROPooZOZHooSMoYQD} un angle $\theta\in\mathopen[ 0 , \pi \mathclose]$ tel que
    \begin{equation}	
        \cos(\theta)=\frac{ X\cdot Y }{ \| X \|\| Y \| }.
    \end{equation}
\end{proof}

\begin{normaltext}
    Certains n'hésitent pas à écrire la formule
    \begin{equation}		\label{eqPropCosThet}
        X\cdot Y=\| X \|\| Y \|\cos(\theta).
    \end{equation}
    comme une définition du produit scalaire. C'est ce qui arrive lorsqu'on défini les fonctions trigonométriques à partir de relations dans les triangles rectangles.
\end{normaltext}

Notez que les angles entre deux vecteurs sont toujours plus petits ou égaux à \unit{180}{\degree}.

La longueur de la projection du point $P$ sur la droite horizontale va naturellement être égale à $\cos(\theta)$. En effet, si nous notons $X$ un vecteur horizontal de norme $1$, cette projection est donné par $P\cdot X$. Mais en reprenant l'équation \eqref{eqPropCosThet}, nous voyons que
\begin{equation}
	P\cdot X=\| P \|\| X \|\cos(\theta),
\end{equation}
tandis qu'ici nous avons $\| P \|=\| X \|=1$.

Nous appelons $\sin(\theta)$ la longueur de la projection sur l'axe vertical.

Quelques dessins nous convainquent que
\begin{equation}
	\begin{aligned}[]
		\sin(\theta+2\pi)&=\sin(\theta)&\cos(\theta+2\pi)&=\sin(\theta),\\
		\sin(\theta+\frac{ \pi }{2})&=\cos(\theta)&\cos(\theta+\frac{ \pi }{2})&=-\sin(\theta),\\
		\sin(\pi-\theta)&=\sin(\theta)&\cos(\pi-\theta)&=-\cos(\theta).
	\end{aligned}
\end{equation}
Le théorème de Pythagore nous montre aussi l'importante relation
\begin{equation}
	\sin^2(\theta)+\cos^2(\theta)=1.
\end{equation}

Quelques valeurs remarquables pour les sinus et cosinus :
\begin{equation}
	\begin{aligned}[]
		\sin 0&=0,&\sin\frac{ \pi }{ 6 }&=\frac{ 1 }{2},&\sin\frac{ \pi }{ 4 }&=\frac{ \sqrt{2} }{2},&\sin\frac{ \pi }{ 3 }&=\frac{ \sqrt{3} }{2},&\sin\frac{ \pi }{2}&=1,&\sin\pi&=0\\
		\cos 0&=1,&\cos\frac{ \pi }{ 6 }&=\frac{ \sqrt{3} }{2},&\cos\frac{ \pi }{ 4 }&=\frac{ \sqrt{2} }{2},&\cos\frac{ \pi }{ 3 }&=\frac{ 1 }{2},&\cos\frac{ \pi }{2}&=0,&\cos\pi&=-1
	\end{aligned}
\end{equation}

Nous pouvons prouver simplement que $\sin(\unit{30}{\degree})=\frac{ 1 }{2}$ et $\cos(\unit{30}{\degree})=\frac{ \sqrt{3} }{2}$ en s'inspirant de la figure~\ref{LabelFigGVDJooYzMxLW}. % From file GVDJooYzMxLW
\newcommand{\CaptionFigGVDJooYzMxLW}{Un triangle équilatéral de côté $1$.}
\input{auto/pictures_tex/Fig_GVDJooYzMxLW.pstricks}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Aire du parallélogramme}
%---------------------------------------------------------------------------------------------------------------------------

% TODO. Il faut revoir ce calcul d'aire à la lumière du fait que nous définissons le mot «aire» avec des intégrales.
% TODOooWBFOooORrGjZ

\begin{remark}      \label{RemaAireParalProdVect}
    Le nombre $\| a \|\| b \|\sin(\theta)$ est l'aire du parallélogramme formé par les vecteurs $a$ et $b$, comme cela se voit sur la figure~\ref{LabelFigBNHLooLDxdPA}. % From file BNHLooLDxdPA
\newcommand{\CaptionFigBNHLooLDxdPA}{Calculer l'aire d'un parallélogramme.}
\input{auto/pictures_tex/Fig_BNHLooLDxdPA.pstricks}
\end{remark}

\begin{proposition}     \label{PropNormeProdVectoabsint}
    Nous avons
    \begin{equation}
        \| a\times b \|=\| a \|\| b \|\sin(\theta)
    \end{equation}
    où $\theta\in\mathopen[ 0.\pi \mathclose]$ est l'angle formé par $a$ et $b$.
\end{proposition}

\begin{proof}
    En utilisant la décomposition du produit vectoriel, nous avons
    \begin{equation}
        \begin{aligned}[]
            \| a\times b \|^2&=\begin{vmatrix}
                a_2    &   a_3    \\
                b_2    &   b_3
            \end{vmatrix}^2+\begin{vmatrix}
                a_1    &   a_3    \\
                b_1    &   b_3
            \end{vmatrix}^2+\begin{vmatrix}
                a_1    &   a_2    \\
                b_1    &   b_2
            \end{vmatrix}^2\\
            &=(a_2b_3-b_2a_3)^2+(a_1b_3-a_3b_1)^2+(a_1b_2-a_2b_1)^2\\
            &=(a_1^2+a_2^2+a_3^2)(b_1^2+b_2^2+b_3^2)-(a_1b_1+a_2b_2+a_3b_3)^2\\
            &=\| a \|^2\| b \|^2-(a\cdot b)^2\\
            &=\| a \|^2\| b \|^2-\| a \|^2\| b \|^2\cos^2(\theta)\\
            &=\| a \|^2\| b \|^2\big( 1-\cos^2(\theta) \big)\\
            &=\| a \|^2\| b \|^2\sin^2(\theta).
        \end{aligned}
    \end{equation}
    D'où le résultat. Nous avons utilisé la formule de la définition \eqref{DEFooSVDZooPWHwFQ} donnant l'angle en fonction du produit scalaire.
\end{proof}

\begin{normaltext}      \label{NORMooWWOKooWzScnZ}
Si les vecteurs $a$, $b$ et $c$ ne sont pas coplanaires, alors la valeur absolue du produit mixte (voir équation \eqref{EqProduitMixteDet}) $a\cdot(b\times c)$ donne le volume du parallélépipède construit sur les vecteurs $a$, $b$ et $c$.

En effet si $\varphi$ est l'angle entre $b\times c$ et $a$, alors la hauteur du parallélépipède vaut $\| a \|\cos(\varphi)$ parce que la direction verticale est donnée par $b\times c$, et la hauteur est alors la «composante verticale» de $a$. Par conséquent, étant donné que $\| b\times c \|$ est l'aire de la base, le volume du parallélépipède vaut\footnote{Le calcul de ce volume mériterait une certaine réflexion, surtout à partir du moment où nous avons décidé de définir les fonctions trigonométriques à partir de son développement (définition~\ref{PROPooZXPVooBjONka}).}
\begin{equation}
    V=\| b\times c\|  \| a \|\cos(\varphi).
\end{equation}
Or cette formule est le produit scalaire de $a$ par $b \times c$; ce dernier étant donné par le déterminant de la matrice formée des composantes de $a$, $b$ et $c$ grâce à la formule \eqref{EqProduitMixteDet}.
\end{normaltext}

La valeur absolue du déterminant
\begin{equation}        \label{EqDeratb}
    \begin{vmatrix}
        a_1    &   a_2    \\
        b_1    &   b_2
    \end{vmatrix}
\end{equation}
est l'aire du parallélogramme déterminé par les vecteurs $\begin{pmatrix}
    a_1    \\
    a_2
\end{pmatrix}$ et $\begin{pmatrix}
    b_1    \\
    b_2
\end{pmatrix}$. En effet, d'après la remarque~\ref{RemaAireParalProdVect}, l'aire de ce parallélogramme est donnée par la norme du produit vectoriel
\begin{equation}
    \begin{pmatrix}
        a_1    \\
        a_2    \\
        0
    \end{pmatrix}\times
    \begin{pmatrix}
          b_1  \\
        b_2    \\
        0
    \end{pmatrix}=\begin{vmatrix}
        e_x    &   e_y    &   e_z    \\
        a_1    &   a_2    &   0    \\
        b_1    &   b_2    &   0
    \end{vmatrix}=
    \begin{vmatrix}
        a_1    &   a_2    \\
        b_1    &   b_2
    \end{vmatrix}e_z,
\end{equation}
donc la norme $\| a\times b \|$ est bien donnée par la valeur absolue du déterminant \eqref{EqDeratb}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Paramétrisation du cercle}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous allons parler de paramértisation du cercle. L'ensemble \( S^1\) sera vu tantôt comme le cercle dans \( \eR^2\), tantôt comme le cercle dans \( \eC\). Nous n'allons pas pousser le vice jusqu'à écrire explicitement les isomorphismes lorsque nous passons d'une représentation à l'autre. Parmi les identifications que nous allons faire sans ménagement, il y a l'identification entre les applications
\begin{equation}
    \begin{aligned}
        \gamma\colon \mathopen[ 0 , 2\pi \mathclose]&\to \eR^2 \\
        t&\mapsto \big( \cos(t),\sin(t) \big) 
    \end{aligned}
\end{equation}
et
\begin{equation}
    \begin{aligned}
        \varphi\colon \mathopen[ 0 , 2\pi \mathclose[&\to \eC \\
            t&\mapsto  e^{it}. 
    \end{aligned}
\end{equation}
C'est évidemment la formule \(  e^{ti}=\cos(t)+i\sin(t)\) (lemme \ref{LEMooHOYZooKQTsXW}) qui permet de transformer \( \gamma\) en \( \varphi\) et inversement. De plus \( \eR^2\) et \( \eC\) sont isomorphes en tant qu'espaces vectoriels normés (et aussi donc topologiques).

Nous allons voir deux choses à propos de cette application :
\begin{itemize}
\item 
    Elle est continue, mais son inverse n'est pas continue. En considérant seulement la restriction \( \varphi\colon \mathopen] 0 , 2\pi \mathclose[\to S^2\setminus\{ (1,0) \}\) nous avons un difféomorphisme, et donc une possibilité de changement de variables dans l'intégrale (théorème \ref{THOooUMIWooZUtUSg}).

    Le fait qu'il manque un point est sans importante parce que nous n'allons considérer que la mesure de Lebesgue ou des variations simples autour de la mesure de Lebesgue.        

\item
    La fonction \( \varphi\colon \mathopen[ 0 , 2\pi \mathclose[\to S^1\) est une bijection borélienne d'inverse borélien\footnote{Proposition \ref{PROPooQFYHooEajmbW}.}. Donc nous pouvons transposer toute la théorie de la mesure de \( S^1\) à \( \mathopen[ 0 , 2\pi \mathclose[\) sans «triche».
\end{itemize}

Tout cela pour dire que nous allons donner un tas de justifications pour écrire des égalités du type
\begin{equation}
    \int_{S^1}f=\int_{0}^{2\pi}f\circ\varphi.
\end{equation}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Bijection continue}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooKSGXooOqGyZj}
    L'application
    \begin{equation}
        \begin{aligned}
            \gamma\colon \mathopen[ 0 , 2\pi \mathclose[&\to S^1\subset \eR^2 \\
            t&\mapsto \big( \cos(t),\sin(t) \big)
        \end{aligned}
    \end{equation}
    est une bijection continue.
\end{proposition}

\begin{proof}
    La continuité découle de la continuité des composantes. Le fait que l'image de \( \gamma\) soit dans \( S^1\) découle immédiatement du fait que \( \sin^2+\cos^2=1\).

    Pour la bijection, il faut injectif et surjectif.
    \begin{subproof}
        \item[Injectif]
            Soient \( x_1<x_2\) tels que \( \sin(x_1)=\sin(x_2)\) et \( \cos(x_1)=\cos(x_2)\). Supposons pour fixer les idées que \( \sin(x_1)>0\) et \( \cos(x_1)>0\) : si ce n'est pas le cas, il faut traiter séparément les \( 4\) possibilités de combinaisons de signes.

            Nous avons obligatoirement \( x_1,x_2\in\mathopen[ 0 , \frac{ \pi }{ 2 } \mathclose[\). Vu que \( \sin(x_1)=\sin(x_2)\), il existe par le théorème de Rolle~\ref{ThoRolle} un élément \( c\in \mathopen] x_1 , x_2 \mathclose[\) tel que \( \sin'(c)=0\), c'est-à-dire \( \cos(c)=0\). Cela contredirait la proposition~\ref{PROPooMWMDooJYIlis}\ref{ITEMooQKPKooEPeHER} à moins que \( x_1=x_2\).

            \item[Surjectif]

                Soient \( x,y\) tels que \( x^2+y^2=1\). Supposons pour varier les plaisirs que \( x<0\) et \( y>0\). Vu que la fonction \( \cos\) va de \( 0\) à \( -1\) lorsque \( x\) va de \( \pi/2\) à \( \pi\), le théorème des valeurs intermédiaires donne \( t\in\mathopen[ \pi/2 , \pi \mathclose]\) tel que \( \cos(t)=x\). Pour cette valeur de \( x\) nous avons
                \begin{equation}
                    \cos^2(x)+\sin^2(x)=1,
                \end{equation}
                et donc \( \sin^2(x)=y^2\), ce qui donne \( \sin(x)=\pm y\). Mais pour \( x\in \mathopen[ \pi/2 , \pi \mathclose]\) nous avons \( \sin(t)>0\). Par conséquent \( \sin(t)=y\).
    \end{subproof}
\end{proof}

\begin{example} \label{EXooJFDPooBZADKs}
    L'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon \mathopen] 0 , 2\pi \mathclose[&\to S^1 \\
                x&\mapsto \begin{pmatrix}
                    \cos(x)    \\
                    \sin(x)
                \end{pmatrix}
        \end{aligned}
    \end{equation}
est un continue par la proposition \ref{PROPooKSGXooOqGyZj}. Vu que \( \mathopen] 0 , 2\pi \mathclose[\) est connexe (proposition~\ref{PropInterssiConn}) la proposition~\ref{PropGWMVzqb} implique que le cercle privé d'un point est connexe.
\end{example}


Allez\ldots Dans l'intro nous avions dit que nous n'allions pas faire explicitement les isomorphismes. Faisons-le quand même une fois, mais c'est bien parce que c'est vous hein.
\begin{proposition}     \label{PROPooZEFEooEKMOPT}
    L'application
    \begin{equation}
        \begin{aligned}
            f\colon \mathopen[ 0 , 2\pi \mathclose[&\to S^1 \\
                x&\mapsto  e^{ix}
        \end{aligned}
    \end{equation}
    est une bijection. Ici, \( S^1\) est l'ensemble des nombres complexes de norme \( 1\).
\end{proposition}

\begin{proof}
    Nous savons que
    \begin{equation}
        \begin{aligned}
            \varphi\colon \eR^2&\to \eC \\
            (x,y)&\mapsto x+iy
        \end{aligned}
    \end{equation}
    est une bijection isométrique. C'est pour cela que nous allons nous permettre de noter \( S^1\) le cercle unité dans \( \eR^2\) aussi bien que l'ensemble des nombres complexes de norme \( 1\).

    Sur \( \eR^2\) nous avons l'application
    \begin{equation}
        \begin{aligned}
            \gamma\colon \mathopen[ 0 , 2\pi \mathclose[&\to S^1\subset \eR^2 \\
                t&\mapsto \begin{pmatrix}
                    \cos(t)    \\
                    \sin(t)
                \end{pmatrix}
        \end{aligned}
    \end{equation}
    qui est une bijection continue (c'est la proposition~\ref{PROPooKSGXooOqGyZj}). Et enfin le lemme~\ref{LEMooHOYZooKQTsXW} nous donne \(  e^{ix}=\cos(x)+i\sin(x)\).

    Avec tout ça, l'application \( \varphi^{-1}\circ f\colon \mathopen[ 0 , 2\pi \mathclose[\to S^1 \) est une bijection continue. Et comme \( \varphi\) l'est également, \( f\) est une bijection continue.
\end{proof}

La proposition suivante donne les coordonnées polaires sur \( \eC\). La régularité est l'objet du théorème \ref{THOooBETSooXSQhdX} (à part le fait que ce dernier parle de \( \eR^2\) et non de \( \eC\)).
\begin{proposition}     \label{PROPooRFMKooURhAQJ}
    Pour tout nombre complexe \( z\), il existe un unique \( \theta\in\mathopen[ 0 , 2\pi \mathclose[\) tel que
        \begin{equation}
            z=| z | e^{i\theta}.
        \end{equation}
\end{proposition}

\begin{proof}
    Soit \( z\in \eC\). Nous considérons \( z'=z/| z |\) qui est de norme \( 1\). Donc il existe un unique \( \theta\in\mathopen[ 0 , 2\pi \mathclose[\) tel que \( z'= e^{i\theta}\) (proposition \ref{PROPooZEFEooEKMOPT}).

    Pour ce \( \theta\) nous avons \( z=| z | e^{i\theta}\).
\end{proof}
Bien entendu, le \( \theta\) est unique dans \( \mathopen[ 0 , 2\pi \mathclose[\), mais il n'est pas du tout unique dans \( \eR\).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Inverse}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooWFNMooOuZBRN}

Nous pouvons écrire un inverse de la fonction \( \varphi\) grâce à la fonction arc tangente introduite au théorème \ref{THOooUSVGooOAnCvC}. 
La fonction que nous écrivons à présent est la fonction \( \arg_{0^{-}} \) définie par \eqref{EQooNKKDooOuJxXe}. Elle n'est pas exactement la fonction argument définie par \eqref{EQooPJVFooSEKTny}.

Nous avons :
\begin{equation}        \label{EQooSAYFooRFVSPc}
    \begin{aligned}
        \varphi^{-1}\colon S^1&\to \mathopen[ 0 , 2\pi \mathclose[ \\
        x+iy&\mapsto  
    \begin{cases}
        \arctg(y/x)    &   \text{si } x>0,y\geq 0\\
        \frac{ \pi }{2}    &    \text{si }(x,y)=(0,1)\\
        \pi-\arctg(-y/x)    &    \text{si }x<0,y\geq 0\\
        \pi+\arctg(y/x)    &    \text{si }x<0,y<0\\
        2\pi-\arctg(-y/x)    &    \text{si }x>0,y<0
    \end{cases}
    \end{aligned}
\end{equation}
Chacune des branche est continue parce que la fonction arc tangente l'est. Trois des raccords sont également continus grâce aux limites du lemme \ref{LEMooHRDCooGtnyeQ}.

L'application \( \varphi^{-1}\) n'est cependant pas continue au point \( (1,0)\)\footnote{Vu que nous avons considéré \( S^1\subset \eC\), nous aurions dû noter «\( 1\)» ce point. Mais vous vous imaginez le clash de notation avec le \( 1\in \mathopen[ 0 , 2\pi \mathclose[\subset \eR\)?}. C'est l'objet du lemme suivant.

\begin{lemma}       \label{LEMooEQVRooMAffCw}
    L'application \( \varphi^{-1}\colon S^1\to \mathopen[ 0 , 2\pi \mathclose[\) n'est pas continue en \( (1,0)\). Mais elle est continue ailleurs. Autrement dit,
        \begin{equation}
        \varphi^{-1}\colon S^1\setminus\{ (1,0) \}\to \mathopen] 0 , 2\pi \mathclose[
        \end{equation}
        est continue.
\end{lemma}

\begin{proof}
    En effet, \( \varphi^{-1}\) serait continue si l'image de tout ouvert de \( \mathopen[ 0 , 2\pi \mathclose[\) par \( \varphi\) serait ouverte dans \( S^1\) (topologie induite de \( \eC\)). Prenons un petit ouvert \( \mathopen[ 0 , \epsilon \mathclose[\) (si vous êtes étonnés, c'est que vous n'avez pas bien la topologie induites en tête). Son image contient le point \( (1,0)\), mais aucun point \( (x,y)\) avec \( y<0\).
       
    Montrons que tout voisinage de \( (1,0)\) dans \( \eC\) contient des points \( x+iy\) de \( S^1\) avec \( y<0\). Un point de \( S^1\) est de la forme \( \cos(t)+i\sin(t)\). Nous avons :
    \begin{equation}
        | \cos(t)+i\sin(t)-1 |^2=\big( \cos(t)-1 \big)^2+\sin^2(t)=2\big( 1-\cos(t) \big).
    \end{equation}
    Soit \( \delta>0\), et montrons que \( B\big( (1,0),\delta \big)\cap S^1\) contient des points d'ordonnées négatives. D'abord il existe \( \epsilon>0\) tel que pour \( t=2\pi-\epsilon\),
    \begin{equation}
        2\big( 1-\cos(t) \big)<\delta.
    \end{equation}
    Ensuite pour de tels \( t\), nous avons \( \sin(t)<0\). Donc les points de \( S^1\) correspondant à \( 2\pi-\epsilon\) sont dans \( S^1\cap B\big( (1,0),\delta \big)\).

    Bref, l'image de \( \mathopen[ 0 , \epsilon \mathclose[\) n'est pas un ouvert de \( S^1\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Cercle trigonométrique}
%---------------------------------------------------------------------------------------------------------------------------

Le \href{http://fr.wikiversity.org/wiki/Trigonométrie/Cosinus_et_sinus_dans_le_cercle_trigonométrique}{cercle trigonométrique} est le cercle de rayon $1$ représenté à la figure~\ref{LabelFigCercleTrigono}. Sa longueur est $2\pi$.
\newcommand{\CaptionFigCercleTrigono}{Le cercle trigonométrique.}
\input{auto/pictures_tex/Fig_CercleTrigono.pstricks}

Nous verrons plus tard que la longueur de l'arc de cercle intercepté par un angle $\theta$ est égal à $\theta$. Les radians sont donc l'unité d'angle les plus adaptés au calcul de longueurs sur le cercle.

%TODOooLMZOooWNDjgq : remettre ce lien après le fork
%Voir exercice~\ref{exoGeomAnal-0034}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Du point de vue de la tribu, mesure et co.}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons considéré sur \( S^1\) la topologie induite de \( \eC\). Nous allons y mettre la tribu induite de celle de Lebesgue de \( \eC\). Mais nous n'allons pas y mettre la \emph{mesure} induite de \( \eC\); sinon tout serait toujours de mesure nulle.

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooQFYHooEajmbW}
    L'application \( \varphi\) est borélienne d'inverse borélien, c'est-à-dire
    \begin{equation}
        \Borelien(S^1)=\varphi\big( \Borelien(\mathopen[ 0 , 2\pi \mathclose[) \big).
    \end{equation}
\end{proposition}

\begin{proof}
    L'inclusion  \(\Borelien(S^1)\subset\varphi\big( \Borelien(\mathopen[ 0 , 2\pi \mathclose[) \big) \) est la plus simple : si \( A\in\Borelien(S^1)\), alors \( \varphi^{-1}(A)\in\Borelien\big( \mathopen[ 0 , 2\pi \mathclose[ \big)\) parce que \( \varphi\colon \mathopen[ 0 , 2\pi \mathclose[\to S^1\) est continue et donc borélienne (théorème \ref{ThoJDOKooKaaiJh}).

    Pour l'autre inclusion, il faudra faire par étapes.
    \begin{subproof}
        \item[Ouvert ne contenant pas zéro]
            Si \( A\) est un ouvert de \( \mathopen[ 0 , 2\pi \mathclose[\) ne contenant pas \( 0\), il est un ouvert de \( \eR\) ou de \( \mathopen] 0 , 2\pi \mathclose[\). Le lemme \ref{LEMooEQVRooMAffCw} nous indique que son image par \( \varphi\) est ouverte dans \( S^1\). En particulier, \( \varphi(A)\in\Borelien(S^1)\).
            \item[Ouvert de la forme \( \mathopen[ 0 , \epsilon \mathclose[\)] 
                Nous supposons que \( \epsilon\) est petit. Disons pour fixer les idées, plus petit que \( \pi/2\). Nous avons :
                \begin{equation}
                    \varphi\big( \mathopen[ 0 , \epsilon \mathclose[ \big)=\varphi\big( \mathopen] 0 , \epsilon \mathclose[ \big)\cup\varphi\big( \{ 0 \} \big). 
                \end{equation}
                Le premier élément de l'union est un ouvert, et le second un unique point. L'union est un borélien.
            \item[Ouvert général]
                Si un ouvert de \( \mathopen[ 0 , 2\pi\mathclose[\) ne contient pas \( 0\), son image est ouverte. Nous nous penchons sur le cas d'un ouvert contenant \( 0\).

                Si un ouvert de \( \mathopen[ 0 , 2\pi \mathclose[\) contient \( 0\), alors il contient un ouvert de la forme \( \mathopen[ 0 , \epsilon \mathclose[\), parce qu'un ouvert contient une boule autour de chacun de ses points (théorème \ref{ThoPartieOUvpartouv} couplé au fait que nous sommes dans la topologie induite de \( \eR\)).

                Si \( A\) est un ouvert contenant zéro, alors
                \begin{equation}
                    A=\mathopen[ 0 , \epsilon \mathclose[\cup\big( A\setminus\mathopen[ 0 , \frac{ \epsilon }{2} \mathclose] \big).
                \end{equation}
                Nous avons déjà vu que l'image du premier élément de l'union est un borélien. Étant donné que \( A\setminus \mathopen[ 0 , \frac{ \epsilon }{2} \mathclose]\) est un ouvert ne contenant pas zéro, son image est un ouvert. Donc le l'image de \( A\) est un borélien.

            \item[Pause]
                Nous avons déjà vu que l'image par \( \varphi\) de tout ouvert de \( \mathopen[ 0 , 2\pi \mathclose[\) était un borélien de \( S^1\). Nous devons en déduire que l'image de tout borélien de \( \mathopen[ 0 , 2\pi \mathclose[\) est un borélien de \( S^1\).

                    C'est ce que nous faisons maintenant

                \item[Boréliens]
                
                    Nous utilisons le lemme de transport \ref{LemOQTBooWGYuDU} avec l'application \( \varphi^{-1}\) et l'ensemble des ouverts :
                    \begin{equation}
                        \varphi\big( \sigma(\tribC) \big)=\sigma\big( \varphi(\tribC) \big)
                    \end{equation}
                    où \( \tribC\) est la tribu des ouverts dans \( \mathopen[ 0 , 2\pi \mathclose[\). L'ensemble \( \sigma(\tribC)\) est par définition l'ensemble \( \Borelien\big( \mathopen[ 0 , 2\pi \mathclose[ \big)\). D'autre part nous avons vu que l'image d'un ouvert est un borélien : \( \varphi(\tribC)\subset\Borelien(S^1)\). Nous avons donc
                        \begin{equation}
                                \varphi\big( \Borelien(\mathopen[ 0 , 2\pi \mathclose[) \big)=\sigma\big( \varphi(\tribC) \big)\subset\sigma\big( \Borelien(S^1) \big)\subset\Borelien(S^1).
                        \end{equation}
    \end{subproof}
    La preuve est terminée. 
\end{proof}

\begin{proposition}[Boréliens sur \( S^1\)\cite{MonCerveau}]      \label{PROPooHMSCooRIjcJq}
    Soit la structure usuelle d'espace mesurable \( (\eC,\Borelien(\eC))\). Nous considérons
    \begin{itemize}
        \item la tribu \( \Borelien(\eC)_{S^1}\) induite de la tribu des boréliens  de \( \eC\) vers \( S^1\),
        \item la tribu \( \Borelien(S^1)\) des boréliens de \( S^1\) construite à partir de la topologie induite de \( \eC\) vers \( S^1\).
        \item la bijection \( \varphi\colon \mathopen[ 0 , 2\pi \mathclose[\to S^1\),
            \item la mesure de Lebesgue sur \( \mathopen[ 0 , 2\pi \mathclose[\) (induite de celle sur \( \eR\)) et sur \( \eC\), que nous noterons toutes deux \( \lambda\).
    \end{itemize}
    Alors 
    \begin{enumerate}
        \item       \label{ITEMooSUNEooRhAdep}
            Nous avons les expressions
            \begin{subequations}
                \begin{align}
                    \Borelien(\eC)_{S^1}&=\{A\in\Borelien(\eC)\tq A\subset S^1\} \\
                    &=\{A\cap S^1\tq A\in\Borelien(\eC)\}       \label{SUBEQooYZGCooDqXmft}
                \end{align}
            \end{subequations}
        \item       \label{ITEMooGYPNooRaZbNW}
            Nous avons
            \begin{equation}
                \Borelien(S^1) = \Borelien(\eC)_{S^1}=\varphi\Big( \Borelien\big( \mathopen[ 0 , 2\pi \mathclose[ \big) \Big).
            \end{equation}
       \item\label{ITEMooFUXKooFQdoaw}
           En définissant \( \mu\colon \Borelien(S^1)\to \eR\) par
           \begin{equation}         \label{EQooKHZRooSrFMdo}
               \mu(A)=\frac{ \lambda\big( \varphi^{-1}(A) \big) }{ 2\pi },
           \end{equation}
           le triple \( \big( S^1,\Borelien(S^1), \mu \big)\) est un espace mesuré.
       \item\label{ITEMooBQLRooOsqesg}
           L'espace mesuré \( \big( S^1,\Borelien(S^1), \mu \big)\) est fini et
            \begin{equation}
                \mu(S^1)=1.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point.
    \begin{subproof}
        \item[Pour \ref{ITEMooSUNEooRhAdep}]
            C'est la proposition \ref{PROPooUNNSooMUQKfp}.
        \item[Pour \ref{ITEMooGYPNooRaZbNW}]
            La première égalité est le lemme \ref{LEMooUPYDooPVjscA}. Le fait que \( \Borelien(S^1)=\varphi\Big( \Borelien\big( \mathopen[ 0 , 2\pi \mathclose[ \big) \Big)\) est déjà la proposition \ref{PROPooQFYHooEajmbW}.
            \item[Pour \ref{ITEMooFUXKooFQdoaw}]
                Nous devons d'abord nous assurer que la formule ait un sens. Cela est chose aisée; si \( A\in \Borelien(S^1)\), le point \ref{ITEMooGYPNooRaZbNW} nous indique que \( \varphi^{-1}(A)\in \Borelien\big( \mathopen[ 0 , 2\pi \mathclose[ \big)\). Ensuite, nous devons vérifier les deux conditions de la définition \ref{DefBTsgznn} pour avoir un espace mesuré.

                En premier lieu,
                \begin{equation}
                    \mu(\emptyset)=\frac{1}{ 2\pi }\lambda\big( \varphi^{-1}(\emptyset) \big)=\frac{1}{ 2\pi }(\emptyset)=0.
                \end{equation}
                En en second lieu, si les \( A_i\in \Borelien(S^1)\) sont disjoints, les \( \varphi^{-1}(A_i)\) sont également disjoints parce que \( \varphi^{-1}\) est une bijection. Donc
                \begin{subequations}
                    \begin{align}
                        \mu(\bigcup_iA_i)&=\frac{1}{ 2\pi }\lambda\big( \bigcup_i\varphi^{-1}(A_i) \big)\\
                        &=\frac{1}{ 2\pi }\sum_i\lambda\big( \varphi^{-1}(A_i) \big)\\
                        &=\sum_i\frac{ \lambda\big( \varphi^{-1}(A_i) \big) }{ 2\pi }\\
                        &=\sum_i\mu(A_i).
                    \end{align}
                \end{subequations}
                D'accord.
            \item[Pour \ref{ITEMooGYPNooRaZbNW}]
                En ce qui concerne la mesure de \( S^1\) pour \(\mu\) nous avons simplement
                \begin{equation}
                    \mu(S^1)=\frac{ \lambda\big( \mathopen[ 0 , 2\pi \mathclose[ \big) }{ 2\pi }=1.
                \end{equation}
    \end{subproof}
\end{proof}

Maintenant que \( (S^1,\Borelien(S^1), \mu)\) est un espace mesuré, nous pouvons compléter la tribu \( \Borelien(S^1)\) pour la mesure \( \mu\).

\begin{definition}
    La \defe{tribu de Lebesgue}{tribu de Lebesgue sur $ S^1$} sur \( S^1\) est la mesure complétée pour
    \begin{equation}
        \big( S^1,\Borelien(S^1),\mu \big)
    \end{equation}
    où \( \mu\) est la mesure définie par la proposition \ref{PROPooHMSCooRIjcJq}. Nous notons \( \Lebesgue(S^1)\) la tribu et encore \( \mu\) la mesure.
\end{definition}

\begin{proposition}[Lebesgue sur \( S^1\)\cite{MonCerveau}]     \label{PROPooDLBCooUfQZOa}
    Soit la structure d'espace mesuré complet \( \big( S^1,\Lebesgue(S^1), \mu \big)\). Nous considérons
    \begin{itemize}
        \item la tribu \( \Lebesgue(\eC)_{S^1}\) induite de la tribu des boréliens  de \( \eC\) vers \( S^1\),
        \item la bijection \( \varphi\colon \mathopen[ 0 , 2\pi \mathclose[\to S^1\),
    \end{itemize}
    Alors 
    \begin{enumerate}
        \item               \label{ITEMooQMHDooHEThPf}
            La tribu \( \Lebesgue(\eC)_{S^1}\) est la tribu de toutes les parties de \( S^1\).
        \item       \label{ITEMooNIRNooKSeyCa}
            La tribu \( \Lebesgue(S^1)\) est donnée par 
            \begin{equation}
                \Lebesgue(S^1)=\varphi\big( \Lebesgue(\eR)_{\mathopen[ 0 , 2\pi \mathclose[} \big)=\varphi\big( \Lebesgue(\mathopen[ 0 , 2\pi \mathclose[) \big)
            \end{equation}
            où \( \Lebesgue(\mathopen[ 0 , 2\pi \mathclose[)\) est la tribu sur \( \mathopen[ 0 , 2\pi \mathclose[\) obtenue par completion de la tribu des boréliens de la topologie induite.
        \item       \label{ITEMooXDBTooYnauyi}
            Nous avons l'inclusion stricte
            \begin{equation}
                \Lebesgue(S^1)\subsetneq\Lebesgue(\eC)_{S^1}.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Point par point.
    \begin{subproof}
        \item[Pour \ref{ITEMooQMHDooHEThPf}]
            Si \( A\subset S^1\), alors \( A\) est une partie de \( S^1\) qui est mesurable et de mesure nulle pour \( \eC\). Donc \( A\) est \( \lambda\)-négligeable et par conséquent mesurable.
        \item[Pour \ref{ITEMooNIRNooKSeyCa}]
            Il s'agit de prouver que
            \begin{equation}
                \widehat{\Borelien(S^1)}=\varphi\big( \widehat{\Borelien\big( \mathopen[ 0 , 2\pi \mathclose[ \big)} \big).
            \end{equation}
            Ce n'est rien d'autre que la proposition \ref{PROPooORDCooJEsjzR}. La seconde partie de l'égalité est la proposition \ref{PROPooAMIEooRomnMG}
        \item[Pour \ref{ITEMooXDBTooYnauyi}]
            Comme indiqué au point \ref{ITEMooQMHDooHEThPf}, la tribu \( \Lebesgue(\eC)_{S^1}\) est la tribu de toutes les parties de \( S^1\); l'incusion est donc évidente. Le point pas tout à fait évident à prouver est l'existence de parties de \( S^1\) à n'être pas dans \( \Lebesgue(S^1)\).

            Soit \( V\) non mesurable dans \( \mathopen[ 0 , 2\pi \mathclose[\) (prenez quelque chose comme l'ensemble de Vitali de l'exemple \ref{EXooCZCFooRPgKjj}). Vu que, par le point \ref{ITEMooNIRNooKSeyCa},
                \begin{equation}
                    \Lebesgue(S^1)=\varphi\big( \Lebesgue(\eR)_{\mathopen[ 0 , 2\pi \mathclose[} \big),
                \end{equation}
                la partie \( \varphi^{-1}(V)\) ne peut pas être dans \( \Lebesgue(S^1)\).
    \end{subproof}
\end{proof}

Si vous en voulez plus à propos de \( S^1\) et la façon dont on passe la structure depuis \( \mathopen[ 0 , 2\pi \mathclose[\), vous pouvez lire la proposition \ref{PROPooDJERooYirMru} qui donne la structure de
\begin{equation}
    L^2\big( S^1,\Lebesgue(S^1), \mu \big)
\end{equation}
qui sera, sans surprises la même que celle de
\begin{equation}
    L^2\big( \mathopen[ 0 , 2\pi \mathclose[,\Lebesgue\big( \mathopen[ 0 , 2\pi \mathclose[ \big), \lambda \big).
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Exemples trigonométriques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous mettons ici quelques exemples concernant les fonctions trigonométriques, qui n'ont pas pu être mis dans les chapitres le plus adapté, parce que ces derniers sont plus haut dans la table des matière.

\begin{example}     \label{EXooSPFDooSluUGV}
    Prouvons que la fonction\footnote{La définition de la fonction sinus est \ref{PROPooZXPVooBjONka}.} $f(x)=x\sin(x)$ tend vers zéro lorsque $x$ tend vers $0$. D'abord, nous coinçons la fonction entre deux fonctions connues :
	\begin{equation}
		0\leq| x\sin(x) |=| x | |\sin(x) |\leq | x |.
	\end{equation}
	Donc $| x\sin(x) |$ est coincé entre $g(x)=0$ et $h(x)=| x |$. Ces deux fonctions tendent vers $0$ lorsque $x\to 0$, et donc $f(x)$ tend vers zéro.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Développements en série}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Taylor pour cosinus]     \label{PROPooNPYXooTuwAHP}
    Le développement du cosinus est donné par
	\begin{equation}
		\cos(x)=1-\frac{ x^2 }{ 2 }+\frac{ x^4 }{ 4! }-\frac{ x^6 }{ 6! }\cdots
	\end{equation}
    C'est-à-dire que pout tout \( n\in  \eN\), il existe une fonction \( \alpha\colon \eR\to \eR\) telle que \( \lim_{t\to 0} \alpha(t)=0\) et
    \begin{equation}        \label{EQooGQOIooIkwbJV}
        \cos(x)=\sum_{k=0}^{n}\frac{ (-1)^{2k} }{ (2k)! }x^{2k}+\alpha(x)x^{2n+1}.
    \end{equation}
    En ce qui concerne le sinus, pour tout \( n\) nous avons une fonction \( \alpha\colon \eR\to \eR\) telle que \( \lim_{t\to 0} \alpha(t)=0\) et
    \begin{equation}        \label{EQooKYJAooRebHgc}
        \sin(x)=\sum_{k=0}^n\frac{ (-1)^kx^{2k+1} }{ (2k+1)! }+x^{2n+2}\alpha(x).
    \end{equation}
\end{proposition}

\begin{proof}
    Il s'agit d'utiliser la proposition \ref{PROPooQLHNooRsBYbe}, en faisant attention à l'ordre. Le fait est que dans \eqref{EQooGQOIooIkwbJV}, nous avons écrit le polynôme de degré \( 2n+1\) (et non seulement \( 2n\)), en sachant que le terme d'ordre \( 2n+1\) est nul.

    C'est pour cela que nous avons pu écrire \( \alpha(x)x^{2n+1}\) au lieu de \( \alpha(x)x^{2n}\) qui aurait été attendu.

    Même raisonnement pour le développement du sinus.
\end{proof}

\begin{remark}
    Quelques remarques concernant l'ordre du polynôme.
    \begin{enumerate}
        \item
            
Notons que nous aurions aussi pu écrire le reste sous la forme \( \alpha(x)x^{2n}\), mais ça aurait été avec une autre fonction \( \alpha\) : celle correspondant au développement à l'ordre \( 2n\) au lieu de \( 2n+1\).
\item

Les développements de sinus et de cosinus ont un terme sur deux qui est nul. C'est pour cela qu'en ayant une polynôme de degré \( 2p\), nous avons le développement d'ordre \( 2p+1\).

\item

   Nous aurions pu utiliser les dérivées données dans la proposition \ref{LEMooBBCAooHLWmno} et les valeurs spéciales \eqref{SUBEQooTTNNooXzApSM}.
    \end{enumerate}
\end{remark}

\begin{corollary}
    Il existe une fonction \( \alpha\colon \eR\to \eR\) telle que \( \lim_{t\to 0} \alpha(t)/t=0\) et 
    \begin{equation}        \label{EQooDLGIooXyfmtC}
        \sin(x)=x+\alpha(x).
    \end{equation}
    Nous avons la limite
    \begin{equation}
        \lim_{x\to 0} \frac{ \sin(x) }{ x }=1.
    \end{equation}
\end{corollary}

\begin{proof}
    Il s'agit de prendre la formule \eqref{EQooKYJAooRebHgc} avec \( n=0\). Cela donne tout de suite \eqref{EQooDLGIooXyfmtC}. Pour la limite, on divise par \( x\), ce qui donne (pour tout \( x\neq 0\)) 
    \begin{equation}
        \frac{ \sin(x) }{ x }=1+\frac{ \alpha(x) }{ x }.
    \end{equation}
    Et justement la fonction \( \alpha\) la la propriété que \( \lim_{x\to 0} \alpha(x)/x=0\).
\end{proof}

\begin{example}
    Cherchons le développement limité à l'ordre \( 5\) de \( \tan(x)=\frac{ \sin(x) }{ \cos(x) }\). Nous utilisons les développements de la proposition \ref{PROPooNPYXooTuwAHP} : 
    \begin{subequations}
        \begin{align}
            \sin(x)&=x-\frac{ x^3 }{ 6 }+\frac{ x^5 }{ 120 }+x^5\alpha_1(x)\\
            \cos(x)&=1-\frac{ x^2 }{ 2 }+\frac{ x^4 }{ 24 }+x^5\alpha_2(x).
        \end{align}
    \end{subequations}
    Nous calculons alors la division des deux polynômes, en classant les puissances dans l'ordre croissant (c'est le sens inverse de ce qui est fait pour la divisions euclidienne !) :
    \begin{equation*}
        \begin{array}[]{ccccccccccc|c}
            &x&-&\frac{1}{ 6 }x^3&+&\frac{1}{ 120 }x^5&&&&&&1-\frac{ 1 }{2}x^2+\frac{1}{ 24 }x^4\\
            \cline{12-12}
            -\Big( &x&-&\frac{ 1 }{2}x^3&+&\frac{1}{ 24 }x^5&\Big)& & && &x+\frac{1}{ 3 }x^3+\frac{ 2 }{ 15 }x^5\\
            \cline{2-6}
            & & &\frac{1}{ 3 }x^3&-&\frac{1}{ 30 }x^5& & & & && \\
            &&-\Big(  &\frac{1}{ 3 }x^3&-&\frac{1}{ 6 }x^5&+&\frac{1}{ 72 }x^7&\Big) & & \\
            \cline{4-8}
            & & & & &\frac{ 2 }{ 15 }x^5&-&\frac{1}{ 72 }x^7& & & \\
            & & &  &-\Big(  &\frac{ 2 }{ 15 }x^5&-&\frac{1}{ 15 }x^7&+&\frac{1}{ 180 }x^9&\Big)& \\
            \cline{6-10}
            & & & & & & &\frac{ 29 }{ 360 }x^7&-&\frac{1}{ 180 }x^9&& \\
        \end{array}
    \end{equation*}
    Nous avons continué la division jusqu'à obtenir un reste de degré plus grand que \( 5\). Le développement à l'ordre $5$ de la fonction tangente autour de zéro est alors (proposition \ref{PROPooMANAooXhuanS})
    \begin{equation}
        \tan(x)=x+\frac{1}{ 3 }x^3+\frac{ 2 }{ 15 }x^5+x^5\alpha(x).
    \end{equation}
    Notons que, vu que le reste ne nous intéresse pas vraiment, nous aurions pu ne pas calculer les coefficients des termes en \( x^7\) et \( x^8\). La dernière soustraction était également inutile.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Intégration}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    Comme nous le voyons sur le dessin suivant,
    \begin{equation}
        \int_{-3\pi/2}^{3\pi/2}\sin(x)\,dx=0
    \end{equation}
    parce que les deux parties bleues s'annulent avec les deux parties rouges (qui sont comptées comme des aires négatives).
    \begin{center}
       \input{auto/pictures_tex/Fig_JSLooFJWXtB.pstricks}
    \end{center}
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Changement de variables dans une intégrale}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}     \label{EXooNIOZooWxciAC}
    Cet exemple util

    Soit $V$ la région trapézoïdale de sommets $(0,-1)$, $(1,0)$, $(2,0)$, $(0,-2)$, comme à la figure~\ref{LabelFigZTTooXtHkcissLabelSubFigZTTooXtHkci0}. Calculons ensemble l'intégrale double
    \[
    \int_{V}e^{\frac{x+y}{x-y}}\,dV,
    \]
    avec le changement de variable $\psi(x,y)=(x+y,x-y)$. C'est-à-dire que nous considérons les nouvelles variables
    \begin{subequations}
        \begin{numcases}{}
            u=x+y\\
            v=x-y.
        \end{numcases}
    \end{subequations}
    Il faut remarquer d'abord que le changement de variable proposé est dans le mauvais sens. On écrit alors $\phi(u,v)=\psi^{-1}(u,v)=\big((u+v)/2, (u-v)/2\big)$, c'est-à-dire
    \begin{subequations}
        \begin{numcases}{}
            x=\frac{ u+v }{ 2 }\\
            y=\frac{ u-v }{2}.
        \end{numcases}
    \end{subequations}
    La région qui correspond à $V$ est $U$, le trapèze de sommets  $(-1,1)$, $(1,1)$, $(2,2)$ et $(-2,2)$, qu'on voit sur la figure~\ref{LabelFigZTTooXtHkcissLabelSubFigZTTooXtHkci1} et qu'on décrit par
    \[
    U=\{ (u,v)\in\eR^2\,\vert\, 1\leq v\leq 2, \, -v\leq u\leq v\}.
    \]

    % Celui-ci a été supprimée le 17 juillet 2014
    %\ref{LabelFigexamplechangementvariables}
    %\newcommand{\CaptionFigexamplechangementvariables}{Avant et après le changement de variables}
    %\input{auto/pictures_tex/Fig_examplechangementvariables.pstricks}

    %The result is on figure~\ref{LabelFigZTTooXtHkci}. % From file ZTTooXtHkci
    %See also the subfigure~\ref{LabelFigZTTooXtHkcissLabelSubFigZTTooXtHkci0}
    %See also the subfigure~\ref{LabelFigZTTooXtHkcissLabelSubFigZTTooXtHkci1}
    \newcommand{\CaptionFigZTTooXtHkci}{Avant et après le changement de variables}
    \input{auto/pictures_tex/Fig_ZTTooXtHkci.pstricks}

    On observe que $U$ est une région du premier type tandis que $V$ n'est pas du premier ou du deuxième type. Le déterminant de la  matrice  jacobienne de $\psi^{-1}$ est  $J_{\psi^{-1}}$,
    \begin{equation}
     J_{\psi^{-1}}(u,v)= \left\vert\begin{array}{cc}
    \frac{1}{2} & \frac{1}{2} \\
    \frac{1}{2}  & -\frac{1}{2}
    \end{array}\right\vert= -\frac{1}{2}.
    \end{equation}
    On a alors, en utilisant le fait que \( F(x)=a e^{x/a}\) est une primitive de \( f(x)= e^{x/a}\) (proposition \ref{ThoKRYAooAcnTut}) ainsi que le théorème fondamental de l'analyse (théorème \ref{ThoRWXooTqHGbC}),
    \[
    \int_{V}e^{\frac{x+y}{x-y}}\,dV=\int_{U}e^{\frac{u}{v}}\,\frac{1}{2}\,dV=\int_1^2\int_{-v}^{v}e^{\frac{u}{v}}\,\frac{1}{2}\, du\,dv= \frac{3}{4}(e-e^{-1}).
    \]
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Classification des isométries dans \( \eR^2\)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Réflexions}
%---------------------------------------------------------------------------------------------------------------------------

Soit un espace vectoriel \( E\) de dimension \( 2\) muni d'un produit scalaire\footnote{Définition~\ref{DefVJIeTFj}.}. Cela pourrait très bien être \( \eR^2\), mais nous allons nous efforcer de l'appeler \( E\) pour rester un peu général.

\begin{lemmaDef}[Caractérisation des réflexions]        \label{DEFooLJKDooUaamen}
    Soit une droite \( \ell\) de \( \eR^2\). Il existe une unique application \( f\colon \eR^2\to \eR^2\) telle que
    \begin{enumerate}
        \item
            \( f(x)=x\) pour tout \( x\in \ell\).
        \item
            \( f\) échange les côtés de \( \ell\).
        \item
            \( f\) laisse invariants les droites perpendiculaires à \( \ell\) et les cercles dont le centre est sur \( \ell\).
    \end{enumerate}
    Cette application est la \defe{réflexion}{réflexion!dans \( \eR^2\)} d'axe \( \ell\).
\end{lemmaDef}

\begin{proof}
    Soit \( x\) hors de \( \ell\) et \( p\) la droite perpendiculaire à \( \ell\) et passant par \( x\). Nous avons \( f(x)\in p\). En nommant \( P\) l'intersection entre \( \ell\) et \( p\), nous considérons le cercle \( S(P,\| Px \|)\) qui est un cercle dont le centre est sur \( \ell\). Il contient \( x\) et donc \( f(x)\in S(P,\| Px \|)\).

    Donc \( f(x)\in p\cap S(P,\| Px \|)\). L'intersection entre un cercle et une droite contient de façon générique deux points. L'un est \( x\), mais \( f(x)=x\) n'est pas possible parce que \( x\) est hors de \( \ell\) et \( f\) doit inverser les côtés de \( \ell\). Donc \( f(x)\) est l'autre.

    Cela prouve l'unicité. En ce qui concerne l'existence, il suffit de noter que la réflexion \( \sigma_{\ell}\) satisfait les contraintes.
\end{proof}

\begin{lemma}       \label{LEMooZSDRooUkNYer}
    Soit une droite \( \ell\) est \( A\in E\). Alors
    \begin{equation}        \label{EQooVUQDooKuwszl}
        \sigma_{\ell}(A)=2\pr_{\ell}(A)-A
    \end{equation}
    où \( \pr_{\ell}\) est l'opération de projection orthogonale sur la droite \( \ell\).
\end{lemma}

\begin{proof}
    Nous posons \( f(X)=2\pr_{\ell}(X)-X\) et nous allons montrer que \( f=\sigma_{\ell}\) en vérifiant les conditions de la définition~\ref{DEFooLJKDooUaamen}. Nous nous gardons bien de faire un raisonnement du type «nous allons montrer que \( f\) et \( \sigma_{\ell}\) coïncident sur deux points, et sont donc égales par le corollaire~\ref{CORooZHZZooDgTzsW}» parce que nous ne savons pas encore que \( \sigma_{\ell}\) est une application affine, ni même que c'est une isométrie.

    Si \( X\in\ell\) alors \( \pr_{\ell}(X)=X\) et nous avons \( f(X)=2X-X=X\). Donc \( \ell\) est conservée.

    En ce qui concerne les deux côtés de \( \ell\), il existe une application linéaire \( s\colon E\to \eR\) et une constante \( c\in \eR\) telles qu'en posant \( \ell(X)=s(X)+c\), la droite \( \ell\) soit le lieux des points \( X\) tels que \( \ell(X)=0\). Un côté de la droite est \( \ell<0\) et l'autre côté est \( \ell>0\). Nous avons :
    \begin{subequations}
        \begin{align}
            \ell\big( f(A) \big)&=\ell\big( 2\pr_{\ell}(A)-A \big)\\
            &=s(2\pr_{\ell}(A)-A)+c\\
            &=2s\big( \pr_{\ell}(A) \big)-s(A)+c\\
            &=s\big( \pr_{\ell}(A) \big)-s(A)\\
            &=-c-s(A)\\
            &=-\ell(A)
        \end{align}
    \end{subequations}
    où nous avons utilisé le fait que, \( \pr_{\ell}(A)\) étant sur \( \ell\), \( s\big( \pr_{\ell}(A) \big)+c=0\). Nous avons donc \( \ell\big( f(A) \big)=-\ell(A)\), ce qui indique que \( A\) et \( f(A)\) sont de part et d'autre de \( \ell\).

    Si \( d\) est une droite perpendiculaire à \( \ell\) et si \( A\in d\) alors \( f(A)=2\pr_{\ell}(A)-A=  \big( \pr_{\ell}(A)-A \big)+A\in d  \) parce que \( \pr_{\ell}(A)\in d\) du fait que \( d\) soit précisément perpendiculaire à \( \ell\). Nous avons aussi utilisé le fait que si \( A,B,C\in d\) alors \( (B-A)+C\in d\); pensez que \( B-A\) est un vecteur directeur et que \( C\) est un point de \( d\).

    Enfin soit \( K\in\ell\) et un cercle \( S(K,r)\) centré en \( K\). Soit \( A\in S(K,r)\); nous devons vérifier que \( f(A)=S(K,r)\). Le segment \( [A,f(A)]\) est par définition perpendiculaire à \( \ell\). Soit \( M\), le milieu, qui est sur la droite \( \ell\). Les triangles \( AMK\) et \( f(A)MK\) sont rectangles en \( M\), et \( \| AM \|=\| Mf(A) \|\). Le théorème de Pythagore donne \( \| AK \|=\| f(A)K \|\). Donc le cercle centré en \( K\) est donc préservé par \( f\).

    Nous en déduisons que \( f=\sigma_{\ell}\).
\end{proof}

\begin{proposition}[\cite{ooIIMKooJpdFyk}]      \label{PROPooFSVEooWmJsnv}
    Une réflexion est une isométrie de \( (E,d)\) où \( d(A,B)=\| A-B \|\).
\end{proposition}

\begin{proof}
    Soient \( A,B\in E\); il faut vérifier que \( \| A-B \|=\| \sigma_{\ell}(A)-\sigma_{\ell}(B) \|\). Pour cela nous écrivons
    \begin{equation}
        B-A=\underbrace{B-\pr_{\ell}(B)}_{=a}+\underbrace{\pr_{\ell}(B)-\pr_{\ell}(A)}_{=b}+\underbrace{\pr_{\ell}(A)-A}_{=c}.
    \end{equation}
    Vu que \( b\perp a\) et \( b\perp c\) nous avons
    \begin{equation}
        \| B-A \|=\langle B-A, B-A\rangle =\| a \|^2+2\langle a, c\rangle +\| b \|^2+\| c \|^2.
    \end{equation}
    Nous pouvons faire le même jeu avec \( \sigma_{\ell}(B)-\sigma_{\ell}(A)\) en tenant compte du fait que \( \pr_{\ell}\big( \sigma_{\ell}(X) \big)=\pr_{\ell}(X)\) et que
    \begin{equation}
        \sigma_{\ell}(A)-\pr_{\ell}(A)=2\pr_{\ell}(A)-A-\pr_{\ell}(A)=-\big( A-\pr_{\ell}(A) \big).
    \end{equation}
    Là nous avons utilisé le lemme~\ref{LEMooZSDRooUkNYer}. Ce que nous trouvons est que
    \begin{equation}
        \sigma_{\ell}(B)-\sigma_{\ell}(A)=-a+b-c,
    \end{equation}
    et donc encore une fois
    \begin{equation}
        \| \sigma_{\ell}(B)-\sigma_{\ell}(A) \|=\| a \|^2-2\langle a, c\rangle +\| b \|^2+\| c \|^2.
    \end{equation}
\end{proof}

\begin{remark}
    Il faut bien comprendre que si l'axe de la réflexion ne passe par par \( 0\) (le zéro de l'espace vectoriel normé \( (E,\| . \|)\)), la réflexion n'est pas une isométrie de \( (E,\| . \|)\) au sens où nous n'avons pas \( \| \sigma_{\ell}(x) \|=\| x \|\).
\end{remark}

\begin{lemma}       \label{LEMooTCIEooXdyuHu}
    Si \( A'\) est l'image de \( A\) par \( \sigma_{\ell}\) alors \( \ell\) est la médiatrice du segment \( [A,A']\).
\end{lemma}

\begin{proof}
    Soit \( M\in\ell\). Nous avons
    \begin{equation}
        \| A-M \|^2=\| \pr_{\ell}(A)-A \|^2+\| \pr_{\ell}(A)-M \|^2
    \end{equation}
    parce que \( A-\pr_{ell}(A)\perp M-\pr_{\ell}(A)\). Par ailleurs, vu que \( \sigma_{\ell}(A)=2\pr_{\ell}(A)-A\) et que \( \pr_{\ell}(A)=\pr_{\ell}(A')\),
    \begin{equation}
        \| \pr_{\ell}(A)-A \|=\| \pr_{\ell}(A')-A' \|.
    \end{equation}
    Nous avons donc
    \begin{equation}
        \| \sigma_{\ell}(A)-M \|^2=\| A-M \|^2,
    \end{equation}
    ce qui prouve que \( M\) est sur la médiatrice de \( [A',A]\) par le lemme~\ref{LEMooSZZWooPDHnGl}.
\end{proof}

\begin{normaltext}
    Si \( l\) est une droite dans \( \eR^2\), nous avons la réflexion \( \sigma_l\in\Isom(\eR^2)\) d'axe \( l\). Cela est une isométrie et donc une application affine par le théorème~\ref{ThoDsFErq}. Le lemme suivant détermine comment la réflexion \( \sigma_{\ell}\) se décompose en une translation et une application linéaire.
\end{normaltext}

\begin{lemma}   \label{LEMooVOJLooCFgdNG}
    Soit une droite \( \ell\). Alors
    \begin{equation}
        \sigma_{\ell}=\tau_{2w}\circ\sigma_{\ell_0}
    \end{equation}
    où \( \ell_0\) est la droite parallèle à \( \ell\) passant par l'origine, et \( w\) est le vecteur perpendiculaire à \( \ell\) tel que \( \ell_0=\ell+v\).
\end{lemma}

\begin{proof}
    Il faut trouver trois points non alignés sur lesquels les deux applications coïncident; cela suffira par le corollaire~\ref{CORooZHZZooDgTzsW}.

    Pour tous les points de \( \ell_0\), l'égalité fonctionne parce que si \( x\in\ell_0\),
    \begin{equation}
        \sigma_{\ell}(x)=x+2w,
    \end{equation}
    tandis que
    \begin{equation}
        \sigma_{\ell_0}(x)+2w=x+2w
    \end{equation}
    du fait que \( \sigma_{\ell_0}(x)=x\).

    Si \( x\in\ell\), alors
    \begin{equation}
        \sigma_{\ell}(x)=x
    \end{equation}
    tandis que
    \begin{equation}
        \sigma_{\ell_0}(x)+2w=x-2w+2w=x.
    \end{equation}
    Donc les applications affines \( \sigma_{\ell}\) et \( x\mapsto \sigma_{\ell_0}(x)+2w\) coïncident sur \( \ell\) et \( \ell_0\). Elles coïncident donc partout.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Rotations}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Rotation en dimension \( 2\)]        \label{DEFooFUBYooHGXphm}
    Une \defe{rotation}{rotation!en dimension \( 2\)} d'un espace euclidien de dimension \( 2\) est une composée de deux réflexions d'axes non parallèles. L'identité est une rotation.
\end{definition}

\begin{normaltext}
    Quelques remarques à propos de cette définition.
    \begin{enumerate}
        \item
            Attention : nous ne parlons pas encore de rotations «vectorielles» : ici le centre de la rotation (que nous n'avons pas encore défini) peut ne pas être \( 0\).
        \item
            Dans la même veine : plus tard, lorsque nous saurons que les rotations sont des isométries de \( (E,d)\) où \( d(X,Y)=\| X-Y \|\), nous allons en réalité beaucoup plus souvent parler de rotations centrées en l'origine qu'en un point quelconque. C'est pourquoi à partir de~\ref{NORMooOUDJooRfbDEX} nous dirons le plus souvent «rotation»  pour «rotation centrée en \( 0\)». D'où les énoncés comme «les rotations sont les matrice orthogonales» (corollaire~\ref{CORooVYUJooDbkIFY}), qui \emph{stricto senus} de la définition~\ref{DEFooFUBYooHGXphm} sont faux.
        \item
            Une rotation est composée de deux réflexions d'axes non parallèles. Il est cependant trop tôt pour décréter que l'intersection de ces axes est le centre de la rotation. Rien ne dit en effet pour l'instant que deux décompositions différentes de la même rotation, avec des axes différents donnent le même point d'intersection.
        \item
            Pourquoi ajouter l'identité  ? Pour avoir un groupe. Dans le cas vectoriel, il est suffisant de demander d'être une composée de deux réflexions, parce que toutes les réflexions vectorielles ont des axes qui s'intersectent en \( 0\). Le cas des axes parallèles est seulement le cas des axes confondus et revient à l'identité.

            Si nous voulons avoir un groupe même pour les rotations centrées ailleurs qu'en zéro, nous devons ajouter «à la main» l'identité.
    \end{enumerate}

    Toutes ces remarques se résument par : «tout devient compliqué du fait que nous voulons considérer également les rotations centrées ailleurs qu'en zéro». En se contentent du cas vectoriel, de nombreuses choses sont plus simples.
\end{normaltext}

\begin{corollary}       \label{CORooNKKIooPGOUJl}
    Si \( A\neq B\) dans \( E\) alors il existe une unique réflexion envoyant \( A\) sur \( B\).
\end{corollary}

\begin{proof}
    En ce qui concerne l'existence, la réflexion dont l'axe est la médiatrice de \( [A,B]\) fait l'affaire. En ce qui concerne l'unicité, le lemme~\ref{LEMooTCIEooXdyuHu} nous dit que si \( A\) est envoyé sur \( B\), l'axe est forcément la médiatrice de \( [A,B]\).
\end{proof}

\begin{lemmaDef}[\cite{ooYPVPooYGSlNU}]        \label{LEMooIJELooLWqBfE}
    Soit une rotation \( r=\sigma_1\circ\sigma_2\) différente de l'identité. 
    \begin{enumerate}
        \item
            Elle admet un unique point fixe.
        \item
            Ce point fixe est l'intersection des axes \( \ell_1\cap\ell_2\).
    \end{enumerate}

    Le \defe{centre}{centre!d'une rotation} d'une rotation (autre que l'identité) est cet unique point fixe.
\end{lemmaDef}

\begin{proof}
    Nous nommons \( O=\ell_1\cap\ell_2\). Soit \( A\in E\), et supposons que \( r(A)=A\). Nous avons \( \sigma_1\circ r=s_2\) et donc
    \begin{equation}
        \sigma_1(A)=(\sigma_1\circ r)(A)=s_2(A).
    \end{equation}
    On pose \( B=\sigma_1(A)\). Alors \( \sigma_1\) et \( \sigma_2\) envoient tout deux \( A\) sur \( B\).

    Si \( A=B\) alors \( A\) est fixé par \( \sigma_1\) et donc appartient à \( \ell_1\). Même chose pour \( A\) est fixé par \( \sigma_2\) et donc \( A\in\ell_2\). Cela donne \( A=B=O\), et donc le point fixé par \( r\) est \( O\).

    Si \( A\neq B\) alors il existe une unique réflexion envoyant \( A\) sur \( B\) (corollaire~\ref{CORooNKKIooPGOUJl}). L'unicité signifie que \( \sigma_1=\sigma_2\). Dans ce cas, \( r=\sigma_1\circ\sigma_2=\id\).
\end{proof}

\begin{normaltext}      \label{NORMooDPBOooKkRuTn}
    La rotation \( \sigma_1\circ\sigma_2\) laisse évidemment fixé le point \( \ell_1\cap \ell_2\). Si \( \sigma_1\circ\sigma_2=\sigma_a\circ\sigma_b\) alors rien n'oblige les axes de \( \sigma_1\) et \( \sigma_2\) d'être identiques à ceux que \( \sigma_a\) et \( \sigma_b\). Mais l'intersection \( \ell_1\cap\ell_2\) doit être la même que l'intersection \( \ell_a\cap \ell_b\) parce que c'est l'unique point fixé par la composée. Cela nous permet de poser les définition suivante.
\end{normaltext}


\begin{lemma}       \label{LEMooTZNWooTVOklu}
    Les rotation sont des isométries pour la distance : \( \| X-Y \|=\| r(X)-r(Y) \|\).
\end{lemma}

\begin{proof}
    Si \( r=\sigma_1\circ\sigma_2\), en utilisant le fait que \( \sigma_1\) et \( \sigma_2\) sont des isométries de \( (E,d)\) (\ref{PROPooFSVEooWmJsnv}) nous avons :
    \begin{equation}
        d(X,Y)=d\big( \sigma_2(X),\sigma_2(Y) \big)=d\big( \sigma_1\sigma_2(X),\sigma_1\sigma_2(Y) \big)=d\big( r(X),r(Y) \big).
    \end{equation}
\end{proof}

Ce lemme nous dit qu'une rotation de centre \( O\) vérifie \( \| OX \|=\| Or(X) \|\) pour tout \( X\).

\begin{proposition}[\cite{ooYPVPooYGSlNU}]      \label{PROPooNXJKooEDOczh}
    Soient \( A,B,O\in E\) tels que \( \| AO \|=\| BO \|\neq 0\). Alors il existe une unique rotation \( r\) centrée en \( O\) telle que \( r(A)=B\).
\end{proposition}

\begin{probleme}
    Attention : la preuve qui suit contient de nombreuses galipettes et improvisations personnelles. Relisez-la attentivement avant de la prendre pour argent comptant.

    La difficulté tient essentiellement à ce que cette preuve traite de façon vectorielle (tous les points sont des éléments de \( E\)) un énoncé qui est essentiellement affine : tous les points doivent être vus comme vecteurs partant de \( O\).

    Si vous comparez la preuve donnée ici avec celle de \cite{ooYPVPooYGSlNU}, vous remarquerez que dans ce dernier, seule la partie «\( A\) et \( O\) son alignés» est présente. C'est parce que lui, il se met directement dans le cas vectoriel et \( O=0\). Il a donc une preuve un tout petit peu moins générale, mais au moins ses isométries sont linéaires et non affines.
\end{probleme}

\begin{proof}
    Existence et unicité séparément.
    \begin{subproof}
        \item[Existence]
            Si \( A=B\), l'identité fait l'affaire. Sinon, \( \| A-O \|=\| B-O \|\) implique que la médiatrice de \( [A,B]\) contient \( O\). Soit \( \sigma_m\) la réflexion selon cette médiatrice. La rotation \( \sigma_m\circ\sigma_{(AO)}\) convient.

        \item[Unicité]

            Soit \( r\) une rotation de centre \( O\) et telle que \( r(A)=B\). Si \( A=B\) alors \( r=\id\) parce qu'une rotation autre que l'identité ne fixe que son centre par le lemme~\ref{LEMooIJELooLWqBfE}. Nous supposons que \( A\neq B\).

            Nous posons \( g=\sigma_m\circ r\). Alors \( g(A)=\sigma_m(B)=A\) parce que \( \sigma_m(B)=A\) et \( r(A)=B\). Cela signifie que \( g\) est une isométrie qui fixe \( A\).

            \begin{subproof}
                \item[Si \( A\) et \( O\) ne sont pas alignés]

                    Attention : ici \( O\) est un point de \( E\), pas le zéro de l'espace vectoriel \( E\). Lorsqu'on dit que \( A\) et \( O\) ne sont pas alignés, nous parlons bien d'alignement avec le zéro de \( E\).

                    Nous avons \( g(A)=A\) et \( g(O)=O\). Donc \( g\) coïncide avec \( \sigma_{(AO)}\) en deux points non alignés, c'est-à-dire en deux points pour lesquels l'espace engendré est tout \( E\). Nous en déduisons que \( g=\sigma_{(AO)}\).

                \item[Si \( A\) et \( O\) sont alignés]


            Soit maintenant un point \( C\) tel que \( A-O\perp C-O\) et
            \begin{equation}
                \| OC \|=\| OA \|=\| OB \|.
            \end{equation}
            Vu que \( g\) est une isométrie pour la distance sur \( E\), pas pour la norme, nous ne pouvons pas écrire \( g(C-O)\perp g(A-O)\) à partir de \( C-O\perp A-O\). Nous décomposons \( g(X)=s(X)+G\) où \( s\) est linéaire sur \( E\). Il est vite vu que \( s\) est une isométrie de \( (E,\| . \|)\) :
            \begin{equation}
                \| X-Y \|=\| g(X)-g(Y) \|=\| s(X)+G-s(y)-G \|=\| s(X)-s(Y) \|=\| s(X-Y) \|
            \end{equation}
            pour tout \( X,Y\in E\). Nous avons de plus \( g(A)=A\) et \( g(O)=O\), ce qui donne \( O=s(O)+G\) et \( A=s(A)+G\). En égalisant les valeurs de \( G\) nous avons
            \begin{equation}        \label{EQooPEWGooABHUvu}
                O-s(O)=A-s(A).
            \end{equation}
            Vu que \( s\) est une isométrie (une vraie) nous avons
            \begin{equation}
                s(A-O)\perp s(C-O),
            \end{equation}
            mais \( s(A-O)=s(A)-s(O)=A-O\) par \eqref{EQooPEWGooABHUvu}. Donc
            \begin{equation}
                A-O\perp s(C-O).
            \end{equation}
            Nous en concluons que \( s(C-O)=\pm (C-O)\). Parce que les vecteurs \( \pm(C-O)\) sont les deux seuls de norme \( \| AO \| =\| CO \|\) à être perpendiculaire à \( A-O\). Rappel : la définition de \( C\) et le fait que nous soyons en dimension \( 2\).

            Est-il possible d'avoir \( s(C-O)=C-O\) ? Cela donnerait
            \begin{subequations}
                \begin{align}
                    g(A-O)&=s(A)-s(O)+G=s(A)-O+O-s(O)+G=A-O+G\\
                    g(C-O)&=C-O+G,
                \end{align}
            \end{subequations}
            ce qui signifierait que \( g\) et \( \tau_G\) coïncideraient sur les points \( A-O\) et \( C-O\), et donc seraient égaux par le corollaire~\ref{CORooZHZZooDgTzsW}. Cela est cependant impossible parce que \( g\) fixe au moins les points \( A\) et \( O\) alors que la translation ne fixe aucun point. Nous en déduisons \( s(C-O)=-(C-O)\).

            Nous avons aussi, parce que \( (AO)\) est une droite passant par l'origine que
            \begin{equation}
                \sigma_{(AO)}(A-O)=A-O
            \end{equation}
            et parce que \( C-O\) est perpendiculaire à cette droite :
            \begin{equation}
                \sigma_{(AO)}(C-O)=-(C-O).
            \end{equation}
            Nous avons donc quand même que \( g\) et \( \sigma_{(AO)}\) coïncident sur deux points non alignés : \( A-O\) et \( C-O\).

            \end{subproof}

            Dans tous le cas, \( g=\sigma_{(AO)}\). Nous avons donc
            \begin{equation}
                \sigma_{(OA)}=\sigma_m\circ r,
            \end{equation}
            et donc \( r\) est fixé à
            \begin{equation}
                r=\sigma_m\circ\sigma_{(OA)}.
            \end{equation}
    \end{subproof}
\end{proof}

\begin{normaltext}
    Anticipons un peu et faisons semblant de déjà connaitre les matrices et les fonctions trigonométriques. La proposition~\ref{PROPooNXJKooEDOczh} nous dit qu'il existe une seule rotation amenant \( A\) sur \( B\). Vous pourriez objecter que le point \( (1,0)\) peut être amené sur \( (0,-1)\) soit par la rotation d'angle \( 3\pi/2\), soit par celle d'angle \( -\pi/2\). Il n'en est rien parce que ces deux rotations sont les mêmes ! Pensez-y. En tant qu'application \( \eR^2\to \eR^2\), la rotation \( R_{3\pi/2}\) est égale à \( R_{-\pi/2}\).
\end{normaltext}

Une rotation donnée peut être écrite de beaucoup de façons comme composée de deux réflexions. En fait d'autant de façons qu'il y a de réflexions.
\begin{proposition}[\cite{ooYPVPooYGSlNU}]      \label{PROPooKAZEooLTHWKe}
    Soit une rotation \( r\) de \( E\) centrée en \( O\). Pour toute réflexion \( \sigma_{\ell}\) telle que le centre de \( r\) soit sur \( \ell\), il existe une réflexion \( \sigma_1\) tells que \( r=\sigma_1\circ\sigma_{\ell}\). Il existe aussi une réflexion \( \sigma_2\) telle que \( r=\sigma_{\ell}\circ s_2\).
\end{proposition}

\begin{proof}
    Si \( r=\id\) c'est bon avec \( s_1=s_2=\sigma_{\ell}\). Sinon nous considérons \( A\neq O\) sur \( \ell\), et \( B=r(A)\). Nous savons que \( B\neq A\) parce que \( O\) est le seul point de \( E\) fixé par \( r\) (proposition~\ref{LEMooIJELooLWqBfE}). Il existe une réflexion (unique) \( \sigma_1\) faisant \( \sigma_1(A)=B\), et c'est le réflexion dont l'axe est la médiatrice de \( [A,B]\). Le point \( O\) est sur cette médiatrice parce que les rotations sont des isométries de \( (E,d)\) (lemme~\ref{LEMooTZNWooTVOklu}).

    La rotation \( \sigma_1\circ \sigma_{\ell}\) vérifie
    \begin{equation}
        (\sigma_1\circ\sigma_{\ell})(A)=\sigma_1(A)=B.
    \end{equation}
    Or \( \| OA \|=\| OB \|\), donc il y a unicité de la rotation centrée en \( O\) portant \( A\) sur \( B\) (proposition~\ref{PROPooNXJKooEDOczh}); nous avons donc \( r=\sigma_1\circ\sigma_{\ell}\).

    En ce qui concerne \( r=\sigma_{\ell}\circ\sigma_2\), il faut appliquer ce que nous venons de faire à la rotation \( r^{-1}\): il existe \( \sigma_2\) tel que \( r^{-1}=\sigma_2\circ\sigma_{\ell}\), ce qui donne
    \begin{equation}
        r=\sigma_{\ell}\circ\sigma_2.
    \end{equation}
\end{proof}

\begin{proposition}    \label{PROPooJTEXooEeOihO}
    Soit une droite \( \ell\) ainsi que \( a\in \eR^n\). Nous considérons la translation \( t_a\). Alors
    \begin{enumerate}
        \item
            Nous avons 
    \begin{equation}
        t_a\circ s_{f_a^{-1}(\ell)}\circ t_a^{-1}=s_{\ell}
    \end{equation}
    où \( t^{-1}_a(\ell)\) est la droite \( \ell\) translatée par \( a\).
        \item       \label{ITEMooEOWBooPjDavw}
            Si \( r\) est une rotation basée en \( a\), alors il existe une rotation \( r_0\) basée en \( 0\) telle que
            \begin{equation}
                r=t_a\circ r_0\circ t_a^{-1}.
            \end{equation}
            \end{enumerate}
            % Pour ce second point, la démonstration est celle de TODOooEZCRooAQsRkZ.
\end{proposition}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Rotations vectorielles}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooTFNSooFjiWHG}
    Les rotations de \( \eR^2\) centrées en \( O\) sont des applications linéaires.
\end{proposition}

\begin{proposition}[\cite{ooYPVPooYGSlNU}]      \label{PROPooWMESooNJMdxf}
    Les rotations basées en \( O\) forment un groupe abélien.
\end{proposition}

\begin{proof}
    L'identité est une rotation par définition. En ce qui concerne l'inverse, si \( r=\sigma_1\sigma_2\) alors \( r^{-1}=\sigma_2\sigma_1\). Nous commençons maintenant les choses pas tout à fait évidentes.
    \begin{subproof}
        \item[Composition]
            Soient des rotations \( r,r'\) centrées en \( O\). Soit également une réflexion \( \sigma\) dont l'axe contient \( O\). Alors la proposition~\ref{PROPooKAZEooLTHWKe} nous donne l'existence de \( \sigma_1\) et \( \sigma_2\) tels que \( r=\sigma_1\sigma\) et \( r'=\sigma\sigma_2\). Avec ça, la composition donne
            \begin{equation}
                rr'=\sigma_1\sigma\sigma\sigma_2=\sigma_1\sigma_2,
            \end{equation}
            qui est encore une rotation.
        \item[Commutativité]
            Soient deux rotations \( r\) et \( r'\) ainsi que des décompositions \( r=\sigma_1\sigma\), \( r'=\sigma\sigma_2\). Nous avons
            \begin{subequations}
                \begin{align}
                    rr'&=\sigma_1\sigma_2\\
                    r'r&=\sigma\sigma_2\sigma_2\sigma.
                \end{align}
            \end{subequations}
            Vu que \( t=\sigma_2\sigma_1\) est une rotation nous pouvons encore appliquer la proposition~\ref{PROPooKAZEooLTHWKe} pour avoir \( t=\sigma_2\sigma_1=\sigma\sigma_3\). Avec ça,
            \begin{equation}
                r'r=\sigma\sigma\sigma_3\sigma=\sigma_3\sigma.
            \end{equation}
            Mais aussi \( rr'=\sigma_1\sigma_2=t^{-1}=\sigma_3\sigma\). Nous avons donc bien \( rr'=r'r\), et le groupe est commutatif.
    \end{subproof}
\end{proof}

\begin{normaltext}      \label{NORMooOUDJooRfbDEX}
    Jusqu'à présent nous avons parlé de rotations «affines». Parmi elles, Les rotations centrées en \( 0\) (zéro, l'origine de \( E\) comme espace vectoriel) sont de particulière importance. Ce sont des applications linéaires, et même des isométries. Dans la suite, nous allons souvent dire simplement «rotation» pour dire «rotation centrée en \( 0\)».

    Vu que nous allons maintenant prendre un point de vue plus vectoriel, nous allons noter les points de \( E\) avec des lettres comme \( x\), \( y\), \( u\), \( v\) et plus avec des majuscules, comme quand on avait un point de vue affin. En même temps, nous allons noter les applications \( E\to E \) par des lettres comme \( A\) et ne plus écrire les parenthèses. Bref, nous écrivons \( Au\) au lieu de \( r(A)\).
\end{normaltext}

\begin{lemma}       \label{LEMooSYZYooWDFScw}
    En dimension \( 2\), les réflexions vectorielles (c'est-à-dire dont l'axe passe par \( 0\)) ont un déterminant \( -1\).
\end{lemma}

\begin{proof}
    Soit une réflexion d'axe \( \ell\). Prenons une base orthonormale de \( E\) constituée de \( e_1\) sur \( \ell\) et de \( e_2\perp \ell\). Alors \( \sigma_{\ell}(e_1)=e_1\) et \( \sigma_{\ell}(e_2)=-e_2\). La formule du déterminant donne
    \begin{equation}
        \det(\sigma_{\ell})=e_1^*\big( \sigma_{\ell}(e_1) \big)e_1^*\big( \sigma_{\ell}(e_2) \big)-e_2^*\big( \sigma_{\ell}(e_1) \big)e_1^*\big( \sigma_{\ell}(e_2) \big)=1\times (-1)-0\times 0=-1.
    \end{equation}
    Nous utilisons de façon cruciale le fait que le calcul du déterminant ne dépende pas de la base choisie, lemme~\ref{LEMooQTRVooAKzucd}.
\end{proof}

\begin{proposition}     \label{PROPooTUJWooAjtEnQ}
    Les rotations\footnote{Centrées en \( 0\), nous ne le répéterons pas !} sont
    \begin{enumerate}
        \item
            des applications linéaires orthogonales au sens de la définition~\ref{DEFooYKCSooURQDoS},
        \item
             des applications de déterminant \( 1\),
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le fait qu'elles soient linéaires est la proposition \ref{PROPooTFNSooFjiWHG}.

    Nous avons, pour tout \( u\in E\) l'égalité de la norme \( \| u \|\) et \( \| Au \|\) par le lemme~\ref{LEMooTZNWooTVOklu} appliqué à \( Y=0\). En terme de produits scalaires nous avons alors \( \langle Au, Au\rangle =\langle u, u\rangle \), et donc
    \begin{equation}
        \langle A^*Au, u\rangle =\| u \|^2.
    \end{equation}
    En particulier si \( \{ e_i \}_{i=1,\ldots, n}\) est une base orthonormée de \( E\) nous avons
    \begin{equation}
        (A^*Ae_i)_i=\| e_i \|^2=1,
    \end{equation}
    ce qui donne \( \| A^*Ae_i \|\geq 1\), avec égalité si et seulement si \( A^*Ae_i=e_i\). Ici nous avons utilisé le fait que \( \langle x, e_i\rangle =x_i\), et le fait que pour tout \( i\) nous ayons \( \| x \|\geq | x_i |\), avec égalité seulement si \( x\) est un multiple de \( e_i\).

    Par ailleurs l'inégalité de Cauchy-Schwarz~\ref{ThoAYfEHG} nous donne
    \begin{equation}
        \| u \|^2=| \langle A^*Au, u\rangle  | \leq \| A^*Au \|\| u \|
    \end{equation}
    et donc
    \begin{equation}
        \| u \|\leq \| A^*Au \|.
    \end{equation}
    Encore une fois, en appliquant cela à \( u=e_i\) nous trouvons \( 1\leq \| A^*Ae_i \|\). Vu que nous avions déjà l'inégalité dans l'autre sens, \( \| A^*Ae_i \|=1\). Et le cas d'égalité est uniquement possible avec \( A^*Ae_i=e_i\).

    Donc pour tout \( i\) de la base nous avons \( A^*Ae_i=e_i\). Nous avons donc \( A^*A=\id\) et l'application \( A\) est orthogonale.

    En ce qui concerne le déterminant, les réflexions sont de déterminant \( -1\) par le lemme~\ref{LEMooSYZYooWDFScw}, donc \( A=\sigma_1\circ\sigma_2\) est de déterminant \( 1\). Nous avons utilisé le fait que le déterminant était un morphisme : proposition~\ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy}.
\end{proof}

\begin{remark}
    Nous ne savons pas encore que les rotations forment tout le groupe \( \SO(2)\) des endomorphismes orthogonaux de déterminant \( 1\). Il faudra attendre le corollaire~\ref{CORooVYUJooDbkIFY} pour le savoir.
\end{remark}

\begin{lemma}       \label{LEMooMIJXooCjiQqP}
    L'application \( -\id\) est une rotation de \( \eR^2\).
\end{lemma}

\begin{proof}
    Soit une base orthonormée \( \{ e_1,e_2 \}\) de \( E\) et la rotation \( r=\sigma_1\sigma_2\) où \( \sigma_i\) est la réflexion le long de l'axe \( \ell_i=\{ te_i \}_{t\in \eR}\). Faut-il vous prouver que \( r=-\id\) ? La réflexion \( \sigma_2\) retourne la composante \( y\) d'un vecteur écrit dans la base \( \{ e_1,e_2 \}\) sans toucher à la composante \( x\). La réflexion \( \sigma_1\) fait le contraire.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Rotation et application affine}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons à nouveau la définition \ref{DEFooUAWZooXcMKve} d'une application affine ainsi que sa décomposition en application linéaire et translation donnée par le lemme \ref{LEMooYJCDooOGAHkF}. Nous voyons maintenant comment ces choses se mettent dans le cas d'une rotation non centrée en l'origine.

\begin{example}
    Soit \( A\in \eR^2\) ainsi qu'une rotation \( f\) autour de \( A\), c'est à dire une composition de deux symétries dont les axes se coupent en \( A\)\footnote{Voir la définition \ref{DEFooFUBYooHGXphm}.}. Nous allons extraire de \( f\) la partie linéaire définie en \ref{LEMooYJCDooOGAHkF}.

    Il existe des axes \( \ell_1\) et \( \ell_2\) tels que \( \ell_1\cap\ell_2=\{ A \}\) et tels que
    \begin{equation}
        f=s_{\ell_1}\circ s_{\ell_2}.
    \end{equation}
    En utilisant la proposition \ref{PROPooJTEXooEeOihO},
    \begin{equation}
        f=t_A\circ s_{t^{-1}_A(\ell_1)}\circ t_A^{-1}\circ t_A\circ s_{t_A^{-1}(\ell_2)}\circ t_A^{-1}=t_A\circ s_{t^{-1}_A(\ell_1)}\circ s_{t_A^{-1}(\ell_2)}\circ t_A^{-1}.
    \end{equation}
    Vu que \( A\in\ell_i\), nous avons \( O=(0,0)\in t_A^{-1}(\ell_i)\). Donc les axes \( t_A^{-1}(\ell_1)\) et \( t_A^{-1}(\ell_2)\) se coupent en \( O\) et nous pouvons écrire
    \begin{equation}        %TODOooEZCRooAQsRkZ
        f=t_A\circ R\circ t_A^{-1}
    \end{equation}
    où \( R\) est une rotation centrée en \( O\); donc une application linéaire par la proposition \ref{PROPooTFNSooFjiWHG}.

    Nous avons
    \begin{subequations}
        \begin{align}
            f(x)&=(t_A\circ R\circ t_A^{-1})(x)\\
            &=(t_A\circ R)(x-A)\\
            &=t_A\big( R(x)-R(A) \big)\\
            &=R(x)+t_{R(A)+A}\\
            &=(t_{R(A)+A}\circ R)(x).
        \end{align}
    \end{subequations}
    Donc 
    \begin{equation}
        f=t_{R(A)+A}\circ R.
    \end{equation}
    Il est maintenant aisé de montrer que \( R\) est la partie linéaire de \( f\). Pour tout \( M,x\in \eR^2\) nous avons
    \begin{subequations}
        \begin{align}
            f(M+x)&=R(M+x)+R(A)+A\\
            &=R(M)+R(x)+R(A)+A\\
            &=R(x)+f(M).
        \end{align}
    \end{subequations}
    Donc ok pour la formule
    \begin{equation}
        f(M+x)=R(x)+f(M)
    \end{equation}
    et \( R\) est la partie linéaire de \( f\), voir la définition \ref{LEMooYJCDooOGAHkF}.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Angle}
%---------------------------------------------------------------------------------------------------------------------------

Avant d'aborder la classification des isométries, nous devons parler de l'angle entre deux droites. Si \( \ell_1\) et \( \ell_2\) sont deux droites, alors il est bien clair deux angles peuvent prétendre être «l'angle entre \( \ell_1\) et \( \ell_2\)». De plus chacun de ces deux angles sont doubles parce que si \( \alpha\) peut prétendre être l'angle entre \( \ell_1\) et \( \ell_2\), alors \( -\alpha\) peut également prétendre.

\begin{lemmaDef}        \label{DEFooEGKOooRPGOAs}
    Si \( \ell_1\) et \( \ell_2\) sont deux droites sécantes au point \( O\) et si \( x\in\ell_1\) n'est pas \( O\), alors il existe un unique \( \alpha\in \mathopen[ 0 , \pi \mathclose[\) tel que \( R_O(\alpha)x\in \ell_2\). La valeur de \( \alpha\) ne dépend pas du choix du point \( x\in \ell_1\).

        Cet angle \( \alpha\) est l'\defe{angle}{angle!entre deux droites} de \( \ell_1\) à \( \ell_2\).
\end{lemmaDef}

\begin{remark}
    Nous ne parlons pas de l'angle entre \( \ell_1\) et \( \ell_2\) mais bien de l'angle \emph{de} \( \ell_1\) \emph{à} \( \ell_2\). L'ordre des droites est important.
\end{remark}

\begin{normaltext}
    Pour la suite, \( R_O(\alpha)\) est la rotation d'angle \( \alpha\) autour du point \( O\) tandis que \( R(\alpha)\) est la rotation d'angle \( \alpha\) autour de l'origine. En termes matriciels, la rotation d'angle \( \alpha\) est donnée par
    \begin{equation}        \label{EQooQBEJooAHaBbJ}
        R(\alpha)=\begin{pmatrix}
            \cos(\alpha)    &   -\sin(\alpha)    \\
            \sin(\alpha)    &   \cos(\alpha)
        \end{pmatrix},
    \end{equation}
\end{normaltext}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooJLHGooQIpKIE}
    Soit \( A\in \eR^2\) et une droite \( \ell_1\). Soit \( \ell_2\) une droite passant par \( A\) et intersectant \( \ell_1\) en \( O\). Alors
    \begin{equation}
        \sigma_{\ell_1}(A)=R_O(-2\alpha)A
    \end{equation}
    où \( \alpha\) est l'angle de \( \ell_1\) à \( \ell_2\).
\end{lemma}

\begin{proof}
    Nous allons utiliser des coordonnées autour de \( O\). Il existe un vecteur \( v\) tel que
    \begin{equation}
        A=O+v
    \end{equation}
    Par définition de l'angle \( \alpha\), la droite \( \ell_2\) s'obtient par rotation d'angle \( \alpha\) depuis la droite \( \ell_1\). Donc le point
    \begin{equation}
        B=R_O(-\alpha)A
    \end{equation}
    est sur \( \ell_1\).

    Nous allons prouver que le point
    \begin{equation}
        D=R_O(-2\alpha)A
    \end{equation}
    est \( D=\sigma_{\ell_1}A\).

    Nous commençons par montrer que la droite \( (DA)\) est perpendiculaire à \( \ell_1\), c'est-à-dire que
    \begin{equation}
        (D-A)\cdot (B-O)=0.
    \end{equation}
    En utilisant le fait que
    \begin{equation}
        R_O(\alpha)(O+X)=O+R(\alpha)X,
    \end{equation}
    nous avons
    \begin{equation}
        D-A=R_O(-2\alpha)(O+v)-(O+v)=O+R(-2\alpha)v-O-v=R(-2\alpha)v-v
    \end{equation}
    et de la même façon,
    \begin{equation}
        B-O=R(-\alpha)v.
    \end{equation}
    Notons que tous les \( O\) se sont simplifiés et qu'il ne reste que des rotations usuelles. En utilisant le fait que \( R(\alpha)\) est une isométrie, nous pouvons alors calculer
    \begin{subequations}
        \begin{align}
            (D-A)\cdot (B-O)&=\langle R(-2\alpha)v-v, R(-\alpha)v\rangle \\
            &=\langle R(-\alpha)v-R(\alpha)v, v\rangle.
        \end{align}
    \end{subequations}
    En utilisant la matrice de rotation \eqref{EQooQBEJooAHaBbJ} nous trouvons
    \begin{equation}
        \big( R(-\alpha)-R(\alpha) \big)v=\begin{pmatrix}
            2\sin(\alpha)v_2    \\
            -2\sin(\alpha)v_1
        \end{pmatrix}
    \end{equation}
    et donc
    \begin{equation}
        \langle  \big( R(-\alpha)-R(\alpha) \big)v  , v\rangle =0.
    \end{equation}

    Le point \( D\) est bien sûr la droite perpendiculaire à \( \ell_1\) et passant par \( A\). Mais vu que \( D\) est obtenu à partir de \( A\) par une rotation, le point \( D\) est également sur le cercle de rayon \( \| OA \|\) et centré en \( O\). Ce cercle possède exactement deux intersections avec cette droite. Le premier est \( A\) et le second est \( \sigma_{\ell_1}(A)\). Vu que \( D\) n'est pas \( A\), nous avons \( D=\sigma_{\ell}(A)\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Classification}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[\cite{ooZYLAooXwWjLa}]      \label{THOooRORQooTDWFdv}
    Toute isométrie du plan \( (\eR^2,d)\) est une composition d'au plus \( 3\) réflexions.
\end{theorem}

\begin{proof}
    Encore une fois nous décomposons la preuve en fonction du nombre de points fixes.
    \begin{subproof}
        \item[Si \( f\) n'a pas de points fixes]
            Soit \( x\in \eR^2\). Nous considérons le segment \( [f,f(x)]\) et nous nommons \( l\) sa médiatrice. Par construction, \( f(x)=\sigma_l(x)\). Nous posons \( g=\sigma_l\circ f\), et nous avons
            \begin{equation}
                g(x)=x.
            \end{equation}
            Donc nous avons \( f=\sigma_l\circ g\) avec \( x\in\Fix(g)\).
        \item[Si \( f\) a un unique point fixe]
            Soit \( x\) cet unique point fixe. Soit \( y\neq x\) et \( l\) la médiatrice de \( [y,f(y)]\). En posant \( g=\sigma_l\circ f\) nous avons
            \begin{equation}
                g(y)=y
            \end{equation}
            et \( g(x)=x\) parce que
            \begin{equation}
                d\big( x,f(y) \big)=d\big( f(x),f(y) \big)=d(x,y),
            \end{equation}
            ce qui donne que \( x\) est à égale distance de \( y\) et de \( f(y)\), c'est-à-dire que \( x\in l\) et par conséquent \( g(x)=(\sigma_l\circ f)(x)=\sigma_l(x)=x\).

            Donc \( g\) fixe \( x\) et \( y\) et donc toute la droite \( (xy)\).
        \item[Si \( f\) fixe une droite]
            Soit \( l\) une droite fixée par \( f\), et soient \( x,y\in l\) et \( z\notin l\) (avec \( x\neq y\)). Le fait que \( x\) et \( y\) soient des points fixes de \( f\) implique
            \begin{subequations}
                \begin{numcases}{}
                    d\big( x,f(z) \big)=d(x,z)\\
                    d\big( y,f(z) \big)=d(y,z)
                \end{numcases}
            \end{subequations}
            ce qui signifie que \( f(z)\) est sur l'intersection des deux cercles\footnote{L'intersection existe pare que \( d(x,z)+d(y,z)>d(x,y)\).} \( S\big( x,d(x,z) \big)\) et \( S\big( y, d(y,z) \big)\), et comme ce sont deux cercles centrés sur la droite \( l\), les intersections sont liées par \( \sigma_l\). Autrement dit, les intersections sont \( z\) et \( \sigma_l(z)\).

            Si \( f(z)=z\) alors \( f\) fixe trois points non alignés et fixe dont \( \eR^2\), c'est-à-dire \( f=\id\).

            Si par contre \( f(z)=\sigma_l(z)\) alors les isométries \( f\) et \( \sigma_l\) coïncident sur trois points et coïncident donc partout par le corollaire~\ref{CORooZHZZooDgTzsW} : \( f=\sigma_l\).
        \item[Conclusion]

            Nous avons montré que si \( \Fix(f)\) a dimension \( m\), alors il existe une droite pour laquelle \( f=\sigma_l\circ g\) avec \( \dim\big( \Fix(g) \big)>m\). Donc il faux au maximum trois pas pour avoir \( \dim\big( \Fix(g) \big)=2\) c'est-à-dire pour avoir \( g=\id\).
    \end{subproof}
\end{proof}

\begin{definition}      \label{DEFooJEOYooNwYtuQ}
    Une \defe{réflexion glissée}{réflexion!glissée} est une transformation du plan de la forme \( \tau_v\circ\sigma_{\ell}\) où le vecteur \( v\) est parallèle à la droite \( \ell\).
\end{definition}

\begin{theorem}[\cite{ooZYLAooXwWjLa}]      \label{THOooVRNOooAgaVRN}
    Les isométries du plan \( (\eR^2,d)\) sont exactement
    \begin{enumerate}
        \item
            l'identité (composée de \( 0\) réflexions),
        \item
            les réflexions,
        \item
            les translations (composées de \( 2\) translations d'axes parallèles),
        \item
            les rotations (composées de \( 2\) réflexions d'axes non parallèles),
        \item
            les réflexions glissées (composées de \( 3\) réflexions)
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous savons déjà que \( f\in \Isom(\eR^2)\) est une composée de \( 0\), \( 1\), \( 2\) ou \( 3\) réflexions.
    \begin{subproof}
        \item[Zéro réflexions]
            Alors c'est l'identité. Ce n'est pas très profond.
        \item[Une réflexion]
            Alors \( f\) est une réflexion. Toujours pas très profond.
        \item[Deux réflexions]
            Soit \( f=\sigma_{\ell_1}\circ\sigma_{\ell_2}\). Maintenant ça s'approfondit un bon coup.

            Nous supposons d'abord que \( \ell_1\parallel\ell_2\). Dans ce cas nous allons prouver que \( f=\tau_{2v}\) où \( v\) est le vecteur perpendiculaire à \(  \ell_1 \) tel que \( \ell_1+v=\ell_2\). Nous allons utiliser le lemme~\ref{LEMooVOJLooCFgdNG} pour montrer que \( \sigma_{\ell_1}\circ\sigma_{\ell_2}=\tau_{2v}\). Nous avons
            \begin{subequations}
                \begin{align}
                    \ell_1=\ell_0+w\\
                    \ell_2=\ell_0+w+v
                \end{align}
            \end{subequations}
            où \( w\) est un vecteur perpendiculaire à \( \ell_1\) et \( \ell_0\) est la droite passant par l'origine et parallèle à \( \ell_1\) et \( \ell_2\). Avec cela,
            \begin{subequations}
                \begin{align}
                    (\sigma_{\ell_1}\circ\sigma_{\ell_2})(x)&=\sigma_{\ell_1}\big( \sigma_{\ell_0}(x)+2w \big)\\
                    &=\sigma_{\ell_0}\big( \sigma_{\ell_0}(x)+2w \big)+2(v+w)\\
                    &=x+\underbrace{\sigma_{\ell_0}(2w)}_{-2w}+2v+2w\\
                    &=x+2v.
                \end{align}
            \end{subequations}
            Donc si \( f\) est composée de deux réflexions d'axes parallèles, alors \( f\) est une translation.

            Toujours dans le cas où \( f\) est composée de deux réflexions, nous supposons que \( f=\sigma_{\ell_2}\circ\sigma_{\ell_1}\) avec \( \ell_1\) et \( \ell_2\) non parallèles. Nous notons \( O\) le point d'intersection, et nous allons voir que \( f=R_O(2\alpha)\) où \( \alpha\) est l'angle de \( \ell_1\) à \( \ell_2\) donné par le lemme~\ref{DEFooEGKOooRPGOAs}.

            Soit \( x\in \ell_1\). Alors
            \begin{equation}
                f(x)=\sigma_{\ell_2}(x),
            \end{equation}
            et le lemme~\ref{LEMooJLHGooQIpKIE} nous donne un moyen de calculer \( \sigma_{\ell_2}(x)\) parce que \( \ell_1\) est une droite passant par \( x\) et coupant \( \ell_1\) au point \( O\). Le lemme dit que \( \sigma_{\ell_2}(x)=R_O(2\alpha)\). Remarque : c'est bien \( 2\alpha\) et non \( -2\alpha\) parce qu'il s'agit de l'angle de \( \ell_2\) à \( \ell_2\); il y a inversion des numéros entre ici et l'énoncé du lemme.

            Nous avons donc bien \( f(x)=R_O(2\alpha)x\) pour \( x\in \ell_1\).

            Si \( y\in\ell_2\) alors
            \begin{equation}
                f(y)=\sigma_{\ell_2}\big( R_O(-2\alpha)y \big)
            \end{equation}
            Nous posons \( z=\sigma_{\ell_1}(y)=R_O(-2\alpha)y\). Soit la droite \( \ell_3\) passant par \( O\) et \( z\). Vu que \( R_O(2\alpha)z=y\in \ell_2\), l'angle de \( \ell_3\) à \( \ell_2\) est \( 2\alpha\). Par conséquent
            \begin{equation}
                \sigma_{\ell_2}(z)=R_O\big( -2\times (-2\alpha) \big)z=R_O(4\alpha)z=R_O(4\alpha)R_O(-2\alpha)y=R_O(2\alpha)y.
            \end{equation}

            Donc les transformations \( f\) et \( R_O(2\alpha)\) coïncident pour tous les points des droites \( \ell_1\) et \( \ell_2\), qui ne sont pas parallèles. Cela prouve que \( f=R_{O}(2\alpha)\).

        \item[Trois réflexions]
            Nous écrivons \( f=\sigma_{\ell_3}\circ\sigma_{\ell_2}\circ\sigma_{\ell_1}\). Nous allons transformer cela progressivement en une symétrie glissée en passant par plusieurs étapes :
            \begin{enumerate}
                \item       \label{ITEMooHVYCooPhFMiv}
                    \( f=\sigma_{\ell}\circ\tau_v\),
                \item       \label{ITEMooUKGLooFlCcjt}
                    \( f=\tau_v\circ\sigma_{\ell}\),
                \item       \label{ITEMooWUCWooZSjofe}
                    \( f=\tau_v\circ\sigma_{\ell} \) avec \( v\parallel\ell\).
            \end{enumerate}
            À chacune de ces étapes, \( v\) et \( \ell\) vont changer. La dernière est une réflexion glissée.

            Nous commençons par supposer \( \ell_2\parallel\ell_3\). Dans ce cas, \( \sigma_{\ell_3}\circ\sigma_{\ell_2}\) est une translation, comme nous l'avons déjà vu. Alors \( f= \tau_v\circ\sigma_{\ell_1}\) et nous sommes déjà dans le cas~\ref{ITEMooUKGLooFlCcjt}.

            Nous supposons que \( \ell_2\) n'est pas parallèle à \( \ell_3\). Dans ce cas, si \( O=\ell_2\cap\ell_3\) nous avons
            \begin{equation}
                \sigma_{\ell_3}\circ\sigma_{\ell_2}=R_O(2\alpha)
            \end{equation}
            où \( \alpha\) est l'angle de \( \ell_2\) à \( \ell_3\). En réalité tant que l'angle de \( \ell'_3\) à \( \ell'_2\) est \( \alpha\) nous avons
            \begin{equation}
                \sigma_{\ell'_3}\circ\sigma_{\ell'_2}= \sigma_{\ell_3}\circ\sigma_{\ell_2}=R_O(2\alpha).
            \end{equation}
            Nous choisissons \( \ell'_2\) parallèle à \( \ell_1\), de telle sorte à ce que \( \sigma_{\ell'_2}\circ\sigma_{\ell_1}\) soit une translation. Alors nous avons
            \begin{equation}
                f=\sigma_{\ell_3}\circ\sigma_{\ell_2}\circ\sigma_{\ell_1}=\sigma_{\ell_3}\circ\sigma_{\ell'_2}\circ\sigma_{\ell'_1}=\sigma_{\ell_3}\circ\tau_v.
            \end{equation}
            où \( v\) est le vecteur de la translation en question.

            Nous avons donc prouvé que toute composition de trois réflexions peut être écrite soit sous la forme~\ref{ITEMooHVYCooPhFMiv} soit sous la forme~\ref{ITEMooUKGLooFlCcjt}.

            Nous prouvons à présent que toute transformation de la forme~\ref{ITEMooHVYCooPhFMiv} peut être écrite sous la forme~\ref{ITEMooUKGLooFlCcjt}. Plus précisément nous allons prouver que si \( \ell\) est une droite, \( v\) un vecteur et \( \ell_0\) la droite parallèle à \( \ell\) passant par l'origine, alors
            \begin{equation}
                \sigma_{\ell}\circ\tau_v=\tau_{\sigma_{\ell_0}(v)}\circ\sigma_l
            \end{equation}
            D'abord nous savons que \( \sigma_{\ell}(x)=\sigma_{\ell_0}(x)+2w\) où \( w\) est le vecteur tel que \( \ell=\ell_0+w\). Ensuite c'est un simple calcul utilisant le fait que \( \sigma_{\ell_0}\) est linéaire :
            \begin{equation}
                (\sigma_{\ell}\circ\tau_v)(x)=\sigma_l(x+v)=\sigma_{\ell_0}(x)+\sigma_{\ell_0}(v)+2w,
            \end{equation}
            et
            \begin{equation}
                (\tau_{\sigma_{\ell_0}(v)}\circ\sigma_{\ell})(x)=\sigma_{\ell_0}(v)+\sigma_{\ell}(x)=\sigma_{\ell_0}(v)+\sigma_{\ell_0}(x)+2w.
            \end{equation}
            L'égalité est faite.

            Nous montrons maintenant que toute transformation de la forme~\ref{ITEMooUKGLooFlCcjt} peut être mise sous la forme~\ref{ITEMooWUCWooZSjofe}. Soit donc \( f=\tau_v\circ\sigma_{\ell}\) où \( v\) et \( \ell\) ne sont pas spécialement parallèles.

            Pour cela nous décomposons \( v=v_1+v_2\) avec \( v_1\perp \ell\) et \( v_2\parallel\ell\) et nous posons \( \ell'=\ell+\frac{ 1 }{2}v_1\). Nous montrons que
            \begin{itemize}
                \item \( \tau_v\circ\sigma_{\ell}=\tau_{v_2}\circ\sigma_{\ell'}\)
                \item \( v_2\parallel \ell'\).
            \end{itemize}
            Pour le deuxième point, \( v_2\parallel\ell\) et bien entendu \( \ell'\parallel\ell\). Donc \( v_2\parallel\ell'\).

            Soit \( \ell_0\) la droite parallèle à \(  \ell\) et \( \ell'\) et passant par l'origine. Soit aussi le vecteur \( w\) tel que \( \ell=\ell_0+w\). Alors nous avons
            \begin{subequations}
                \begin{numcases}{}
                    \sigma_{\ell}=\sigma_{\ell_0}+2w\\
                    \sigma_{\ell'}=\sigma_{\ell_0}+2w+v_1
                \end{numcases}
            \end{subequations}
            Nous avons
            \begin{equation}
                (\tau_v\circ\sigma_{\ell})(x)=v+\sigma_{\ell_0}(x)+2w
            \end{equation}
            et
            \begin{subequations}
                \begin{align}
                    (\tau_{v_2}\circ\sigma_{\ell'})(x)&=v_2+\sigma_{\ell_0}(x)+2w+v_1\\
                    &=\sigma_{\ell_0}(x)+v+2w
                \end{align}
            \end{subequations}
            où dans la dernière ligne, nous avons regroupé \( v_1+v_2=v\). Et voilà.
    \end{subproof}
\end{proof}

\begin{corollary}[\cite{MonCerveau}] \label{CORooVYUJooDbkIFY}
    Les rotations vectorielles forment exactement le groupe \( \SO(2)\).
\end{corollary}

\begin{proof}
    Nous savons déjà de la proposition~\ref{PROPooTUJWooAjtEnQ} que les rotations (composées de \( 2\) réflexions, par la définition~\ref{DEFooFUBYooHGXphm}) sont toutes des applications de \( \SO(2)\).

    Pour l'autre sens, une application de \( \SO(2)\) est une isométrie et est donc, par le théorème~\ref{THOooVRNOooAgaVRN} une des choses suivantes :
    \begin{itemize}
        \item identité
        \item réflexion
        \item translation
        \item réflexion glissée
        \item rotation (composée de réflexions d'axes non parallèles).
    \end{itemize}
    L'identité est possible et c'est une rotation. La réflexion est impossible parce qu'une réflexion linéaire (non affine) est de déterminant \( -1\) (lemme~\ref{LEMooSYZYooWDFScw}). Une translation c'est impossible parce que les éléments de \( \SO(2)\) laissent invariant un point alors que les translations ne laissent aucun point invariant.

    Quid des réflexions glissées ? Vue la forme \eqref{EQooVUQDooKuwszl} de la réflexion, la réflexion glissée \( \tau_v\circ \sigma_{\ell}\) a pour formule
    \begin{equation}
        \alpha(x)=(\tau_v\circ\sigma_{\ell})(x)=v+2\pr_{\ell}(x)-x.
    \end{equation}
    Nous allons étudier les conditions nécessaires pour que cela décrive un élément de \( \SO(2)\), et nous allons voir que c'est impossible.

    D'abord si \( \ell\) passe par l'origine, alors \( \alpha(0)=v\) et nous devons avoir \( v=0\). Dans ce cas \( \alpha=\sigma_{\ell}\), ce qui n'est pas possible parce que \( \alpha\) doit avoir un déterminant \( 1\) alors que la réflexion a un déterminant \( -1\).

    Donc \( \ell\) ne passe pas par l'origine. La condition \( \alpha(0)=0\) impose \( v=-2\pr_{\ell}(0)\). Alors
    \begin{equation}
        \alpha(x)=-2\pr_{\ell}(0)+2\pr_{\ell}(x)-x.
    \end{equation}
    Le calcul de \( \alpha\big( \pr_{\ell}(0) \big)\) donne :
    \begin{equation}
        \alpha\big( \pr_{\ell}(0) \big)=-2\pr_{\ell}(0)+2\pr_{\ell}\big( \pr_{\ell}(0) \big)-\pr_{\ell}(0).
    \end{equation}
    Vu que la projection de la projection est la projection, cela se réduit à
    \begin{equation}
        \alpha\big( \pr_{\ell}(0) \big)=-\pr_{\ell}(0).
    \end{equation}
    Le vecteur \( \pr_{\ell}(0)\) est donc renversé par l'action de \( \alpha\).

    Considérons une base orthonormale \( \{ e_1,e_2 \}\) de \( E\) où \(e_1 \) est un multiple de norme \( 1\) de \( \pr_{\ell}(0)\). Le déterminant de \( \alpha\) pour cette base est\footnote{La définition~\ref{DEFooODDFooSNahPb} avec la formule \eqref{EQooOJEXooXUpwfZ}.} :
    \begin{equation}
        \det(\alpha)=\langle e_1, \alpha(e_1)\rangle \langle e_2, \alpha(e_2)\rangle -\langle e_1, \alpha(e_2)\rangle \langle e_2, \alpha(e_1)\rangle.
    \end{equation}
    Là dedans nous avons \( \langle e_1, \alpha(e_1)\rangle =-1\) et \( \langle e_2, \alpha(e_1)\rangle =0\), et le tout doit faire \( 1\). Donc
    \begin{equation}
        \langle e_2, \alpha(e_2)\rangle =-1,
    \end{equation}
    ce qui implique que \( \alpha(e_2)=-e_2\). En particulier \( \alpha=-\id\). Nous avons prouvé jusqu'ici que si \( \alpha\) est une réflexion glissée dans \( \SO(2)\) alors \( \alpha=-\id\).

    Même pas besoin de se poser de grandes questions sur savoir si il est possible de construire une symétrie glissée égale à \( -\id\) (c'est pas possible), il suffit de dire que de toutes façons, \( -\id\) est tout de même une rotation (lemme~\ref{LEMooMIJXooCjiQqP}).

    Il reste les rotations. Tout élément de \( \SO(2)\) est une rotation.
\end{proof}

\begin{theorem}[\cite{BIBooULRWooPsjtBE}]       \label{THOooKDMUooUxQqbB}
    Soit un groupe fini \( G\) d'isométries de \( (\eR^2,d)\) contenant \( n \) éléments.
    \begin{enumerate}
        \item       \label{ITEMooYEONooCOMpeb}
            Il existe un point \( C\in \eR^2\) fixé par tous les éléments de \( G\).
        \item       \label{ITEMooGELWooFFAqkc}
            Si \( G\) ne contient pas de réflexions, alors il est cyclique\footnote{Définition \ref{DefHFJWooFxkzCF}.} et engendré par la rotation d'angle \( 2\pi/n\) autour de \( C\).
        \item       \label{ITEMooDHKEooFpCfmX}
            Si \( G\) contient au moins une réflexion, et si \( C\) est un point fixe de \( G\), alors
            \begin{enumerate}
                \item       \label{ITEMooGQZTooJIPPLtyf}
                    toutes les réflexions ont un axe qui passe par \( C\),
                \item       \label{ITEMooKPQRooLquSiQ}
                    \( n\) est pair,
                \item       \label{ITEMooCHSWooHpDGHf}
                    Si \( \sigma\) est une réflexion dans \( G\), alors nous avons $G=\gr\big(\sigma,R_C(4\pi/n)\big)$ où \( R_C(\theta)  \) est la rotation d'angle \( \theta\) autour de \( C\),
                \item       \label{ITEMooROUYooRghvMv}
                    \( G\) est isomorphe au groupe diédral \( D_{n/2}\).
            \end{enumerate}
    \end{enumerate}
\end{theorem}

\begin{proof}
    Soit un groupe fini \( G\) constitué d'isométries de \( (\eR^2,d)\). Nous prouvons le théorème point par point.
    \begin{subproof}
        \item[Pour \ref{ITEMooYEONooCOMpeb}]
            C'est la proposition \ref{PROPooLAEBooWdcBoe}.
        \item[Questions de réflexions]
            Le théorème \ref{THOooRORQooTDWFdv} nous dit que les éléments de \( G\) sont des compositions d'au maximum \( 3\) réflexions. 
        \item[Exclure trois réflexions]
            Il n'est pas possible que \( G\) contienne un élément composé de trois réflexions. En effet, les composées de trois réflexions, par le théorème \ref{THOooVRNOooAgaVRN} sont des réflexions glissées\footnote{Définition \ref{DEFooJEOYooNwYtuQ}.}, c'est à dire des transformations de la forme \( g=\tau_v\circ \sigma_{\ell}\) où \( v\) est un vecteur parallèle à la droite \( \ell\). Si \( x\in \ell\), alors
            \begin{equation}
                g(x)=\tau_v(x)=x+v,
            \end{equation}
            de telle sorte que \( g^k(x)=x+kv\), qui signifie que tous les \( g^k\) sont différents. Le groupe \( G\) ne peut pas être fini si il contient une réflexion glissée.

        \item[\( G^+\) et \( G^-\)]
            Pour la même raison que celle qui exclu les réflexions glissées, \( G\) ne peut pas contenir de translations. Le théorème \ref{THOooVRNOooAgaVRN} nous donne la liste des possibilités. Après exclusion des translations et des réflexions glissées, il reste :
            \begin{itemize}
                \item l'identité
                \item les rotations,
                \item les réflexions.
            \end{itemize}
            Nous notons \( G^+\) la partie de \( G\) contenant l'identité et les rotations et \( G^-\) celle contenant les réflexions. Notons que \( G^+\) n'est pas vide parce qu'il contient au moins l'identité, tandis que \( G^-\) peut être vide, mais n'est certainement pas un groupe.
        \item[Même nombre d'éléments]
            Nous prouvons à présent que si \( G^-\) est non vide, alors il a le même nombre d'éléments que \( G\). Un élément de \( G^-\) est une réflexion. Soit \( \sigma\in G^-\). Nous prouvons que
            \begin{equation}        \label{EQooWRVVooBQCtPg}
                \begin{aligned}
                    \varphi\colon G^+&\to G^- \\
                    f&\mapsto \sigma\circ f 
                \end{aligned}
            \end{equation}
            est une bijection.

            \begin{subproof}
                \item[Surjective]
                    Soit \( s\in G^-\). Posons \( f=\sigma^{-1}\circ s\). Vu que \( \sigma^{-1}\) et \( s\) sont des réflexions, \( f\) est une rotation. Donc \( f\in G^+\) et \( \varphi(f)=s\).
                \item[Injective]
                    La condition \( \varphi(f)=\varphi(g)\) dit que \( \sigma\circ f=\sigma\circ g\). En composant par \( \sigma^{-1}\) nous obtenons \( f=g\).
            \end{subproof}
        \item[\( G=\gr\big(R_C(2\pi /p)\big)\)]
            Nous nommons \( p\) le nombre d'éléments de \( G^+\). Si \( G^-\) est vide, \( p=n\), et sinon \( p=n/2\). Dans les deux cas, \( G^+\) est un groupe de rotations à \( p\) éléments. 

            Le groupe \( G^+\) contient seulement des rotations; or le centre d'une rotation est l'unique point fixe. Donc tous les éléments de \( G^+\) sont des rotations autour de \( C\).
            
            Le corollaire \ref{CorpZItFX} au théorème de théorème de Lagrange nous indique que tous les éléments de \( G^+\) vérifient \( g^p=\id\). Seules les rotations d'angle \( 2k\pi/p\) autour de \( C\) satisfont la condition \( g^p=\id\). Or il n'y a que \( p\) telles rotations. Donc elles sont toutes dans \( G^+\). Nous en déduisons que
            \begin{equation}        \label{EQooUWTVooEMqkVH}
                G^+=\gr\big( R_C(2\pi/p) \big).
            \end{equation}
        \item[Pour \ref{ITEMooGELWooFFAqkc}]
            Dans le cas où \( G\) ne contient pas de réflexions, \( G^-\) est vide et \( G\) contient \( n\) éléments. La relation \eqref{EQooUWTVooEMqkVH} devient
            \begin{equation}
                G=G^+=\gr\big( R_C(2\pi/n) \big).
            \end{equation}
        \item[Pour \ref{ITEMooDHKEooFpCfmX}]
            Nous supposons maintenant que \( G\) contienne au moins une réflexion. De la sorte \( G^-\neq \emptyset\).
            \begin{subproof}
            \item[Pour \ref{ITEMooGQZTooJIPPLtyf}]
                Les seuls points fixes d'une réflexions sont ceux de l'axe. Donc \( C\) soit être sur tous les axes des réflexions contenues dans \( G^-\).

                Notons au passage que deux réflexions d'axes qui se coupent forment une rotation. Donc \( G^-\) ne forme pas un groupe, mais même pas en rêve.
            \item[Pour \ref{ITEMooKPQRooLquSiQ}]
                Vu que l'union \( G=G^+\cup G^-\) est disjointe et que \( G^+\) et $G^-$ ont le même nombre d'éléments par la bijection \ref{EQooWRVVooBQCtPg}, si \( G^-\) est non vide, \( G\) possède un nombre pair d'éléments.
            \item[Pour \ref{ITEMooCHSWooHpDGHf}]
                Si \( \sigma\in G\) est une réflexion, nous savons que \( G^+\) possède \( p=n/2\) éléments et que
                \begin{equation}
                    G^+=\{  R_C(2k\pi/p)  \}=\{  R_C(4k\pi/n) \}_{k=1,\ldots, n/2}.
                \end{equation}
                L'élément \( \sigma\in G^- \) étant fixé, la bijection \eqref{EQooWRVVooBQCtPg} nous indique que tous les éléments de \( G^-\) sont de la forme \( \sigma\circ f\) avec \( f\in G^+\). Donc
                \begin{equation}
                    G^{-}\subset \gr\big( \sigma, R_C(4\pi/n)  \big).
                \end{equation}
                Nous avons aussi
                \begin{equation}
                    G^+\subset \gr\big( \sigma,  R_C(4\pi/n)  \big).
                \end{equation}
                Et comme \( \sigma\) et \(  R_C(4\pi/n)  \) sont dans \( G\) nous avons \( \gr\big( \sigma ,  R_C(4\pi/n)  \big)\subset G\). Tout cela pour dire que 
                \begin{equation}
                    G=\gr\big( \sigma,  R_C(4\pi/n) \big).
                \end{equation}
            \item[Table de multiplication]
                Nous restons dans le cas où \( G^-\) n'est pas vide. Nous considérons une réflexion \( \sigma\in G\). Les éléments de \( G^+ \) sont des rotations autour de \( C\) et ceux de \( G^-\) de la forme \( \sigma R\) où \( R\) est une rotation autour de \( C\). Pour savoir la table de multiplication de \( G\), nous devons écrire
                \begin{equation}
                    (\sigma^{\epsilon_1}R^k)(\sigma^{\epsilon_2}R^l)=\sigma^{\epsilon}R^m
                \end{equation}
                où \( \epsilon_1,\epsilon_2\in \{ 0,1 \}\), \( R\) est la rotation d'angle \( 4\pi/n\) autour de \( C\) et \( \alpha\) et \( m\) sont des constantes à exprimer en fonction de \( \epsilon_1\), \( \epsilon_2\), \( k\) et \( l\).

                Si \( R_0\) est la rotation d'angle \( 4i\pi/n\) autour de \( (0,0)\), nous avons
                \begin{equation}
                    R=\tau_C\circ R_0\circ \tau_C^{-1},
                \end{equation}
                et si \( \sigma_0\) est la symétrie d'axe parallèle à l'axe de \( \sigma\), mais passant par \( (0,0)\) nous avons : 
                \begin{equation}
                    \sigma=\tau_C\circ\sigma_0\circ\tau_C^{-1}.
                \end{equation}
                Si \( v\) est le vecteur directeur de la réflexion \( \sigma_0\), nous considérons enfin \( \alpha\), la rotation qui fait \( \alpha(v)=(1,0)\). Nous avons alors
                \begin{equation}
                    \sigma_0=\alpha^{-1}\circ s\circ \alpha
                \end{equation}
                où \( s\) est la symétrie autour de l'axe horizontal. En n'ayant pas peur d'identifier \( \eR^2\) à \( \eC\), l'applicaiton \( s\) est la conjugaison complexe. Avec tout ça nous avons
                \begin{equation}
                    R\sigma=\tau_CR_0\tau_C^{-1}\tau_C\sigma_0\tau_C^{-1}=\tau_CR_0\alpha^{-1}s\alpha\tau_C^{-1}=\tau_C\alpha^{-1}R_0s\alpha\tau_C^{-1}
                \end{equation}
                où nous avons utilisé le fait que les rotations autour de \( (0,0)\) forment un groupe abélien pour commuter \( \alpha^{-1}\) avec \( R_0\). Nous utilisaons à présent le lemme \ref{LEMooBNJFooAbhsUa} pour commuter \( R\) avec \( s\) :
                \begin{subequations}
                    \begin{align}
                        R\sigma&=\tau_X\alpha^{-1}sR_0^{-1}\alpha\tau_C^{-1}\\
                        &=\tau_C\underbrace{\alpha^{-1}s\alpha}_{\sigma_0} R_0^{-1}\tau_C^{-1}\\
                        &=\tau_C\sigma_0\tau_C^{-1}\tau_CR_0^{-1}\tau_C^{-1}\\
                        &=\sigma R^{-1}.
                    \end{align}
                \end{subequations}
                Nous avons utilisé le fat que \( \tau_CR_0^{-1}\tau_C^{-1}=R^{-1}\) comme on peut s'en convaincre en calculant le produit.
            \item[Pour \ref{ITEMooROUYooRghvMv}]
                Nous venons de prouver que \( R\sigma=\sigma R^{-1}\).
            \end{subproof}
            
    \end{subproof}
\end{proof}
 

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Pavages du plan}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooHPKFooSIDhCM}
    Une application affine \( f\colon \eR^n\to \eR^n\) est un \defe{déplacement}{déplacement} lorsqu'elle est une isométrie de \( (\eR^n,d)\) qui préserve l'orientation\footnote{Définition \ref{DEFooOTFPooIVkHFP}.}.
\end{definition}

\begin{definition}[\cite{NHXUsTa}]
    Un \defe{groupe de pavage}{pavage du plan} de \( \eR^2\) est une paire \( (G,K)\) où \( G\) est un groupe de déplacements\footnote{Définition \ref{DEFooHPKFooSIDhCM}.} de \( \eR^2\) et \( K\) un compact de \( \eR^2\) d'intérieur non vide telle que
    \begin{enumerate}
        \item
            \( G\cdot K=\eR^2\),
        \item
            Si \( g,g'\in G\) satisfont \( g\cdot \Int(K)\cap g'\cdot\Int(K)\neq 0\), alors \( g\cdot K=g'\cdot K\).
    \end{enumerate}
\end{definition}

\begin{lemma}[\cite{MonCerveau}]        \label{LEMooEKWZooYbcGBp}
    Soient \( u_1,u_2\in \eR^2\) non colinéaires, ainsi que \( \alpha,\beta\in \mathopen[ 0 , 1 \mathclose]\). Nous considérons \( v=\alpha u_1+\beta u_2\).

    Nous supposons pour fixer les idées que \( \| u_1 \|\geq \| u_2 \|\). Alors
    \begin{equation}
        \min\{ \| v \|, \| u_1+u_2-v \| \}\leq \| u_1 \|.
    \end{equation}
    Autrement dit, tout point intérieur d'un parallélogramme est plus proche d'un angle que la longueur du plus long côté.
\end{lemma}

\begin{proof}
    Les points \( \alpha u_1+\beta u_2\) (\( \alpha,\beta\in \mathopen[ 0 , 1 \mathclose]\)) se divisent en deux parties : ceux avec \( 0\leq\alpha+\beta\leq 1\) et ceux avec \( 1\leq\alpha+\beta\leq 2\).

    Si \( \alpha+\beta\leq 1\) alors
    \begin{equation}
        \| \alpha u_1+\beta u_2 \|<\| \alpha u_1 \|+\beta\| u_2 \|= \alpha\| u_1 \|+\beta\| u_2 \|\leq (\alpha+\beta)\| u_1 \|\leq \| u_1 \|.
    \end{equation}
    L'inégalité est stricte parce que \( u_1\) et \( u_2\) ne sont pas colinéaires.

    Si au contraire \( \alpha+\beta\geq 1\) nous avons
    \begin{equation}
        \| u_1+u_2-\alpha u_1-\beta u_2 \|<\| (1-\alpha)u_1 \|+\| (1-\beta)u_2 \|<(2-\alpha-\beta)\| u_1 \|\leq \| u_1 \|.
    \end{equation}
\end{proof}

\begin{lemma}       \label{LEMooWKTGooQlfuxm}
    Soient un ensemble \( E\), et deux bijections \( r,s\colon E\to E\) ayant chacune un unique point fixe. Si elles commutent, alors leurs points fixes sont égaux. 
\end{lemma}

\begin{proof}
    Nous nommons \( a\) le point fixe de \( r\) et \( b\) celui de \( s\). Pour tout \( x\in E\) nous avons \( (rs)(x)=(sr)(x)\). En particulier pour \( x=s^{-1}(a)\). D'une part
    \begin{equation}
        (rs)(x)=r(a)=a.
    \end{equation}
    Et d'autre part,
    \begin{equation}
        (sr)(a)=(srs^{-1})(a)
    \end{equation}
    Si nous imposons \( (srs^{-1})(a)=a\), nous avons, en appliquant \( s^{-1}\) est deux côtés : \( (rs^{-1})(a)=s^{-1}(a)\). Cela prouve que \( s^{-1}(a)\) est un point fixe de \( r\). Donc \( s^{-1}(a)=a\).

    Nous en déduisons que \( a\) est un point fixe de \( s\) et donc que \( a=b\).
\end{proof}

\begin{lemma}       \label{LEMooDGSJooCiBhZz}
    Soit un sous groupe fini \( T\) de \( (\eR^2,+)\) tel que \( \| v \|>\delta \) pour tout \( v\neq 0\) dans \( T\).

    Alors il existe \( u_1,u_2\in \eR^2\) tels que
    \begin{equation}
        T=\eZ u_1+\eZ u_2.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous décomposons en plusieurs parties.

    \begin{subproof}
        \item[\( \eR^+v\cap T=\eN v_m\)]
            Soit \( v\in T\). L'ensemble \( \{ \lambda \in \eR^+\tq \lambda v\in T\}\) a un minimum parce que tous les éléments de \( T\) sont en norme plus grands que \( \delta>0\). Soit \( \lambda_m\) ce minimum et \( v_m=\lambda_mv\).

            Nous prétendons à présent que \( \eR^+v\cap T=\eN v_m\). Nous ne faisons d'ailleurs pas que prétendre; nous \emph{prouvons}. En effet, doit \( \lambda v\in T\). Nous devons prouver que \( \lambda = l\lambda_m\) pour un certain \( l\in \eN\). Soit \( k\in \eN\) tel que \( k\leq \lambda<k+1\).

            Nous avons
            \begin{equation}
                (k+1)\lambda_m-\lambda\leq (k+1)\lambda_m-k\lambda_m=\lambda_m.
            \end{equation}
            Vu que \( T\) est un groupe pour l'addition, les faits que \( \lambda_mv\in T\) et que \( \lambda v\in T\) impliquent que \( \big( (k+1)\lambda_m-\lambda \big)v\in T\). Mais
            \begin{equation}
                | \big( (k+1)\lambda_m-\lambda \big)v |\leq \lambda_m.
            \end{equation}
            Vu la propriété de minimalité de \( \lambda_m\) nous avons forcément
            \begin{equation}
                (k+1)\lambda_m-\lambda = \lambda_m.
            \end{equation}
            Cela prouve que \( \lambda=k\lambda_m\). 
            
            Jusqu'ici nous avons prouvé que
            \begin{equation}
                \eR^+v\cap T=\eN v_m
            \end{equation}
            pour un certain multiple \( v_m\) de \( v\).

        \item[\( \| u_1 \|\leq \| v \|\) pour tout \( v\)]

            Nous montrons à présent qu'il existe \( u_1\in T\) tel que \( \| u_1 \|\leq \| v \|\) pour tout \( v\in T\). Si tel n'était pas le cas, il existerait une suite \( v_k\in T\) telle que \( \| v_{k+1} \|<\| v_k \|\). Toute cette suite serait contenue dans le couronne (compacte) de rayons \( \delta\) et \( \| u_1 \|\). Quitte à prendre une sous-suite, nous pouvons supposer que \( (v_k)\) converge\footnote{Proposition \ref{THOooRDYOooJHLfGq}.}. Cette suite serait de Cauchy et pour tout \( \epsilon\) (en particulier \( \epsilon<\delta\)), il existeraient \( p,q\) tels que \( \| v_p-v_q \|<\epsilon\). Vu que \( T\) est un groupe pour l'addition, nous aurions \( v_p-v_q\in T\) avec \( \| v_p-v_q \|<\epsilon\leq \delta\).

        \item[Première pause]

            Si \( T\) est engendré seulement par \( u_1\), nous avons fini. Autrement dit, si tout \( T\) est dans un sous-espace de dimension \( 1\) de \( \eR^2\) nous avons terminé.

            Dans la suite, nous supposons donc que \( T\) n'est pas contenu dans un sous-espace de dimension \( 1\).

        \item[\( T=\eZ u_1 + \eZ u_2\)]

            Soient \( u_1\) et \( u_2\) les deux plus petits éléments de \( T\) en norme (peut-être ex-aequo). Ces deux élements ne sont pas colinéaires, sinon leur différence serait plus petite.

            Soit \( v\in T\). Vu que \( \{ u_1,u_2 \}\) est une base de \( \eR^2\), il existe \( \alpha,\beta\in \eR\) tels que
            \begin{equation}
                v=\alpha u_1+\beta u_2.
            \end{equation}
            Notre but est à présent de prouver que \( \alpha,\beta\in \eZ\).

            Si ce n'était pas le cas, une simple translation nous mènerait dans les circonstances du lemme \ref{LEMooEKWZooYbcGBp}. Nous aurions alors que soit \( \| v \|\) soit \(\| u_1+u_2-v \|\) serait strictement plus petit que le plus grand entre \( \| u_1 \|\) et \( \| u_2 \|\). Cela contredirait le fait que \( \| u_1 \|\) et \( \| u_2 \|\) étaient les deux plus petits.
    \end{subproof}
\end{proof}

\begin{theorem}[\cite{NHXUsTa}]
    Nous notons \( \tau_v\) la translation de vecteur \( v\), \( r_{A,\theta}\) la rotation de centre \( A\) et d'angle \( \theta\) ainsi que \( \tau_i=\tau_{e_i}\).

    À conjugaison par une application affine près, il n'existe que les groupes de pavage suivants :
    \begin{enumerate}
        \item
            \( \gr(\tau_1,\tau_i)\)
        \item
            \( \gr(\tau_1,\tau_i,r_{0,\pi})\)
        \item
            \( \gr(\tau_1,\tau_j,r_{0,2\pi/3})\)
        \item
            \( \gr(\tau_1,\tau_i,r_{0;\pi/2})\)
        \item
            \( \gr(\tau_1,\tau_j,r_{0;\pi/3})\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Soit \( (G,K)\) un pavage de \( \eR^2\).  Nous notons \( T\) l'ensemble des translations dans \( G\), plus précisément,
    \begin{equation}
        T=\{ v\in \eR^2\tq  \tau_v\in G\}.
    \end{equation}
    \begin{subproof}
        \item[Une borne pour \( T\)]
            Nous prouvons qu'il existe \( \delta>0\) tel que \( \| v \|>\delta\) pour tout \( v\neq 0\in T\). En effet, soit \( m\in \Int(K)\) ainsi que \( r\) tel que \( B(m,r)\subset \Int(K)\). Alors si \( v\in T\) est tel que \( \| v \|<r\) nous avons \( \tau_v(m)\in \Int(K)\), ce qui donnerait
            \begin{equation}
                m\in\tau_v\big( \Int(K) \big)\cap\Int(K)
            \end{equation}
            par hypothèse, cette intersection est non vide seulement si \( v=0\).

            Donc il existe \( \delta\) tel que \( \| v \|\geq \delta\) pour tout \( v\in T\).
        \item[Utilisation du lemme]

            La partie \( T\) est donc dans la position du lemme \ref{LEMooDGSJooCiBhZz} et nous avons
            \begin{equation}
                T=\eZ u_1+\eZ u_2
            \end{equation}
            pour les vecteurs \( u_1\) et \( u_2\) les plus petits en norme de \( T\).

            En réalité, il se peut que \( T\) soit plus petit que ça, parce que \( G\) peut par exemple ne contenir aucune translations. Nous avons trois possibilités :
            \begin{itemize}
                \item \( T=\{ 0 \}\),
                \item \( T=\eZ u\) pour un certain \( u\neq 0\) dans \( \eR^2\),
                \item \( T=\eZ u_1+\eZ u_2\) pour certains \( u_1,u_2\in \eR^2\setminus\{ 0 \}\).
            \end{itemize}
        \item[Translation]
            Si \( r,s\in G\), alors l'élément \( rsr^{-1}s^{-1}\) est une translation. En effet, vu que les éléments de \( G\) sont des déplacements, ce sont des applications affines et donc il existe des applications linéaires \( A_r,A_s\) et des translations \( \tau_r,\tau_s\) telles que \( r=A_r\circ \tau_r\) et \( s=A_s\circ \tau_s\). Le lemme \ref{LEMooUBGZooBIlmAN} nous donne les inverses. Nous avons
            \begin{equation}
                rsr^{-1}s^{-1}=(A_r\circ \tau_r)(A_s\circ \tau_s)(A_r^{-1}\circ \tau_{-A_rv_r})(A_s^{-1}\circ \tau_{-A_sv_s}).
            \end{equation}
            La partie linéaire de cela est
            \begin{equation}
                A_r\circ A_s\circ A_r^{-1}\circ A_s^{-1}.
            \end{equation}
            C'est donc une composée de rotations centrées en \( (0,0)\). Mais ces rotations forment un groupe abélien (proposition \ref{PROPooWMESooNJMdxf}). Donc nous pouvons écrire
            \begin{equation}
                A_r\circ A_s\circ A_r^{-1}\circ A_s^{-1}=A_r\circ A_r^{-1}\circ A_s\circ A_s^{-1}=\id.
            \end{equation}

            Tout ceci pour dire que dès que \( r,s\in G\), l'élément \( rsr^{-1} s^{-1}\) est une translation.

        \item[Les parties linéaires\cite{MonCerveau}]

            Nous savons de l'exemple \ref{EXooAGINooYmvPML} que les éléments de \( G\) s'écrivent sous la forme \( f=\tau_v\circ \alpha\) où \( v\in \eR^2\) et \( \alpha\colon \eR^2\to \eR^2\) est linéaire.

            De plus, \( f\) étant une isométrie de \( (\eR^2,d)\), l'application \( \alpha\) est une isométrie de \( (\eR^2,\| . \|)\). Vu que \( \alpha\) est une isométrie, \( \det(\alpha)=\pm1\). Mais les déplacement conservent l'orientation; donc \( \alpha\) doit conserver l'orientation, et la proposition \ref{PROPooNBAXooKNUrnk} nous dit que \( \det(\alpha)>0\). Donc
            \begin{equation}
                \det(\alpha)=1.
            \end{equation}

            Le théorème \ref{THOooWBIYooCtWoSq} dit que \( \alpha\) est la composition d'un nombre pair de réflexions. Mais comme il y en a au plus trois (théorème \ref{THOooRORQooTDWFdv}), l'application \( \alpha\) est composée de zéro ou deux réflexions.

            Donc les parties linéaires des éléments de \( G\) sont des rotations.
           
        \item[Les autres]

            Les parties linéaires des éléments de \( G\) sont des rotations. Mais les éléments de \( G\) eux-mêmes ne sont pas tellement mystérieux. Vu que ce sont des isométries de \( (\eR^2,d)\), elles sont composées de \( 0\), \( 1\), \( 2\) ou \( 3\) réflexions.

            Mais ce sont des déplacement, donc ils préservent l'orientation et le théorème \ref{THOooWBIYooCtWoSq} dit qu'ils sont des composées de zéro ou deux réflexions (nombre pair). Ce sont donc des rotations.

        \item[Hein ?]

            Les éléments linéaires de \( G\) sont des rotations. Et les autres aussi ? Les linéaires sont des rotations autour de \( (0,0)\); les autres sont des rotations autour de points autres que \( (0,0)\).

            C'est pourquoi dans la suite, nous préciserons «rotation linéaire» pour une rotation autour de \( (0,0)\) et nous dirons «rotation» pour une rotation en général. Dans le contexte affine, il faut toujours faire attention à ça : une rotation peut très bien n'être pas linéaire\footnote{Lorsque, ailleurs dans le Frido, nous disons «rotation», souvent nous pensons «rotation linéaire». Gardez cependant à l'esprit qu'une rotation peut très bien être centrée ailleurs qu'en l'origine, et soyez toujours capable de préciser le cas échéant.}.

        \item[Les rotations linéaires stabilisent \( T\)]

            Nous prouvons maintenant que les rotations linéaires de \( G\) stabilisent \( T\), c'est à dire que si \( v\in T\) et si \( \alpha\) est une rotation linéaire de \( G\), alors \( \alpha(v)\in T\). La transformation \( \alpha\tau_v\alpha^{-1}\) est dans \( G\). Mais pour tout \( x\in \eR^2\) nous avons
            \begin{equation}
                (\alpha\tau_v\alpha^{-1})(x)=\alpha\big( \alpha^{-1}(x)+v \big)=x+\alpha(v)=\tau_{\alpha(v)}(x).
            \end{equation}
            Donc \( \alpha\tau\alpha^{-1}=\tau_{\alpha(v)}\) et \( \alpha(v)\in T\).
            
        \item[Exclusion de \( T=\{ 0 \}\)]

            Le fait que \( T=\{ 0 \}\) ne signifie pas que tous les éléments de \( G\) sont des rotations; il peut encore y avoir des composées de rotations et de translations \( A\circ \tau\). Cela étant dit, si \( T=\{ 0 \}\), il n'en reste pas moins que \( rsr^{-1}s^{-1}\) est une translation, c'est à dire est égal à \( \id\). Mais \( rsr^{-1}s^{-1}=e\) implique \( rs=sr\).

            Donc \( G\) est abélien. Les éléments de \( G\) sont donc des rotations qui commutent deux à deux. Vu qu'une rotation a son centre comme unique point fixe, le lemme \ref{LEMooWKTGooQlfuxm} nous dit que toutes le éléments de \( G\) sont des rotations de même centre.

            Soit \( c\) le centre commun de tous les éléments de \( G\). Vu que \( K\) est compact dans \( \eR^2\), il existe \( r>0\) tel que \( K\subset B(c,r)\). Vu que \( G\) stabilise toutes les boules centrées en \( c\), nous avons
            \begin{equation}
                G\cdot K\subset B(c,r).
            \end{equation}
            Donc nous n'avons pas un recouvrement de \( \eR^2\). Le cas \( T=\{0  \}\) est exclu.

        \item[Exclusion de \( T=\eZ u\)]

            Nous supposons à présent que \( T=\eZ u\) pour un certain \( u\in \eR^2\). 

            \begin{subproof}
                \item[\( r_0=\pm\id\)]
                    Nous savons que tous les éléments de \( G\) sont des rotations; soit un élment \( r\) de \( G\). La proposition \ref{PROPooJTEXooEeOihO}\ref{ITEMooEOWBooPjDavw} nous indique qu'il existe un point \( a\in \eR^2\) ainsi qu'une rotation linéaire \( r_0\) telle que \( r=\tau_a^{-1}r_0\tau_a\). Soit \( r=\tau_{a}^{-1}r_0\tau_a\in G \). Nous allons prouver que \( r_0\) est \( \pm\id\). D'abord,
                    \begin{equation}
                        r\tau_u r^{-1}=\tau_a^{-1}r_0\tau_a\tau_u\tau_a^{-1}r_0^{-1}\tau_a=\tau_a^{-1}r_0\tau_ur_0^{-1}\tau_a.
                    \end{equation}
                    Ensuite, nous appliquons cela à \( x\in \eR^2\) :
                    \begin{subequations}
                        \begin{align}
                            (\tau_a^{-1}r_0\tau_ur_0^{-1}\tau_a)(x)&=(\tau_a^{-1}r_0\tau_u)\big( r_0^{-1}(x+a) \big)\\
                            &=(\tau_a^{-1}r_0)\big( r_0^{-1}(x+a)+u \big)\\
                            &=\tau_a^{-1}\big( x+a+r_0(u) \big)\\
                            &=x+r_0(u).
                        \end{align}
                    \end{subequations}
                    Donc \( r\circ\tau_u\circ r^{-1}=\tau_{r_0(u)}\), ce qui prouve que \( r_0(u)\in T\). Vu que \( \| r_0(u) \|=\| u \|\) nous avons forcément \( r_0(u)=\pm u\).

                    Si \( r_0(u)=u\), alors \( r_0=\id\) (parce que \( r_0\) est une rotation fixant plus que un seul point). Dans ce cas, \( r=\id\).

                    Si au contraire \( r_0(u)=-u\), alors \( r_0=-\id\).

                \item[Forme générale]

                    Donc si \( r\) est un élément non trivial de \( G\) nous avons \( r=\tau_a^{-1}\circ(-\id)\circ\tau_a\), et alors
                    \begin{equation}
                        \big( \tau_a^{-1}\circ(-\id)\circ\tau_a \big)(x)=\tau_a^{-1}  (-\id)(x+a)=\tau_a^{-1}(-x-a)=-x-2a.
                    \end{equation}
                    Donc pour tout \( r\in G\), il existe \( a\in \eR^2\) tel que
                    \begin{equation}        \label{EQooQGNVooKyCCYW}
                        r(x)=-x-2a.
                    \end{equation}
                    Pour information, le centre de cette rotation est \( -a\) (c'est le seul point fixe).

                \item[Les centres sont alignés]

                    Soit \( r\) une rotation de centre \( -a\) et \( s\) de centre \( -b\). Alors
                    \begin{equation}
                        (rs)(x)=r(-x-2b)=x+2b-2a=x+2(b-a).
                    \end{equation}
                    Donc \( rs=\tau_{2(b-a)}\).

                    Cela prouve que \(2(b-a)\in \eZ u\).

                \item[Une bande]

                    Soit la droite \( D=\eR u\). Nous consiérons la bande
                    \begin{equation}
                        B_r=\{ x\in \eR^2\tq d(x,D)<R \}.
                    \end{equation}

                \item[Une inclusion]
                    Nous prouvons à présent que pour tout \( r\), nous avons
                    \begin{equation}
                        G\cdot B_r \subset B_r.
                    \end{equation}
                    Nous savons qu'un élément général de \( G\) est une rotation centrée en un point de \( D\), et que l'action d'une telle rotation est donnée par \eqref{EQooQGNVooKyCCYW}. Nous avons
                    \begin{subequations}
                        \begin{align}
                            d\big( r(x),D \big)&=d(x+2a,D)      \label{EQooNCQGooGGAhCR}\\
                            &=d\big( \tau_{2a}^{-1}(x+2a),\tau_{2a}^{-1}(D) \big)   \label{EQooNFJNooSAYtBD}\\
                            &=d(x,D)        \label{SUBEQooMGTMooAlRwWT}.
                        \end{align}
                    \end{subequations}
                    Justifications :
                    \begin{itemize}
                        \item Pour \eqref{EQooNCQGooGGAhCR}, nous avons \( D=-D\) et \(d(x,y)=d(-x,-y) \).
                        \item Pour \eqref{EQooNFJNooSAYtBD}, invariance par translation de la distance dans \( \eR^2\).
                        \item Pour \eqref{SUBEQooMGTMooAlRwWT}, les éléments de \( D\) sont les multiples de \( a\); donc cette droite est invariante par cette translation.
                    \end{itemize}
                    Bref, si \(  d(x,D)=d\big( r(x),D \big)   \). Donc pour tout \( r>0\)\footnote{Notez la notation malheureuse pour \( r\) qui est maintenant une distance alors que trois mots plus tôt, c'était un élément de \( G\).} et pour tout \( g\in G\), si \( x\in B_r\), alors \( g(x)\in B_r\).

                \item[Exclusion]

                    Vu que \( K\) est compact et que la fonction \( x\mapsto d(x,D)\) est continue, il existe \( r>0\) tel que \( K\subset B_r\). Avec ça, \( G\cdot K\subset G\cdot B_r\subset B_r\). Donc \( G\cdot K\) ne recouvre par tout \( \eR^2\) et \( G\) n'est pas un groupe de pavage.

            \end{subproof}
    \end{subproof}

    Et nous voila avec seulement \( T=\eZ u_1+\eZ u_2\) en lice.
    
\end{proof}


