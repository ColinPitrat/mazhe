% This is part of Mes notes de mathématique
% Copyright (c) 2008-2018
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Parties libres, génératrices, bases et dimension}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous avons déjà défini (dans~\ref{DEFooKHWZooIfxdNc}) un espace vectoriel comme étant un module sur un corps commutatif. En explicitant un peu, cela donne ceci\cite{ooQLVLooEUrNLS}.

Un espace vectoriel sur le corps \( \eK\) est un ensemble \( V\) muni de deux opérations :
\begin{itemize}
    \item une loi de composition interne \( +\colon V\times V\to V\),
    \item une loi de composition externe \( \cdot\colon \eK\times V\to V\)
\end{itemize}
telles que
\begin{enumerate}
    \item
        \( (V,+)\) soit un groupe abélien,
    \item
        pour tout \( u,v\in V\) et pour tout \( k,k'\in \eK\),
        \begin{subequations}
           \begin{align}
                k(u+v)=(ku)+(kv)\\
                (kk')u=k(k'u)\\
                (k+k')u=(ku)+(k'u)\\
                1u=u
            \end{align}
        \end{subequations}
        où \( 1\) est le neutre de \( \eK\) et où nous avons directement adopté la notation \( ku\) pour \( k\cdot u\).
\end{enumerate}
Si \( u\in V\), nous notons \( -u\) l'inverse de \( u\) dans le groupe \( (V,+)\).

\begin{definition}[Partie libre]
    Si \( E\) est un espace vectoriel, une partie \( A\) de \( E\) est \defe{libre}{libre!partie} si pour tout choix d'un nombre fini d'éléments \( \{ u_i \}_{i=1,\ldots, n}\), l'égalité
    \begin{equation}
        a_1 u_1+\cdots +a_nu_n=0
    \end{equation}
    implique \( a_i=0\) pour tout \( i\) (ici les \( a_i\) sont dans le corps de base).

    Une partie infinie est libre si toute ses parties finies le sont.
\end{definition}

\begin{remark}
    Notons que le vecteur nul n'est dans aucune partie libre, ne fut-ce que parce que \( a0=0\) n'implique pas \( a=0\).
\end{remark}

Si \( A\) est une partie de l'espace vectoriel \( E\) nous notons \( \Span(A)\)\nomenclature[A]{$\Span(A)$}{l'ensemble des combinaisons linéaires finies d'éléments de \( A\)} l'ensemble des combinaisons linéaires finies d'éléments de \( A\). Les coefficients de ces combinaisons linéaires sont dans le corps de base \( \eK\).

\begin{definition}[Partie génératrice]
    Une partie $B$ d'un espace vectoriel \( E\) est \defe{génératrice}{partie!génératrice} si \( \Span(B)=E\).
\end{definition}

\begin{remark}
    Ces définitions demandent des commentaires en dimension infinie\footnote{Nous n'avons pas encore définit le concept de dimension, mais nous nous adressons au lecteur trop pressé.}.

    \begin{enumerate}
        \item
    Tout élément peut être écrit comme combinaison linéaire finie d'une partie génératrice. Cela ne signifie pas que nous pouvons extraire une partie finie qui convient pour tous les éléments à la fois. Lorsque l'espace est de dimension infinie, ceci est particulièrement important.
\item
    La définition séparée de liberté dans le cas des parties infinies a son importance lorsqu'on parle d'espaces vectoriels de dimension infinies (en dimension finie, aucune partie infinie n'est libre) parce que cela fera une différence entre une base algébrique et une base hilbertienne par exemple.
    \end{enumerate}
\end{remark}

\begin{definition}[Base]
    Une \defe{base}{base} de l'espace vectoriel \( E\) est une partie à la fois génératrice et libre.
\end{definition}

\begin{proposition}[\cite{MonCerveau}]      \label{PROPooEIQIooXfWDDV}
    Tout élément non nul d'un espace vectoriel possédant une base\footnote{Nous n'avons pas démontré que tout espace vectoriel possède une base. Donc à notre niveau, il est possible que ce théorème soit sans objet pour beaucoup d'espaces.} se décompose de façon unique en combinaison linéaire finie d'éléments d'une base.
\end{proposition}

\begin{proof}
    Soit un espace vectoriel \( E\) et une base \( \{ e_i \}_{i\in I}\) où \( I\) est un ensemble a priori quelconque. Soit \( v\in E\). Vu que \( E=\Span\{ e_i \}_{i\in I}\), il existe une partie finie \( J\) de \( I\) et des coefficients \( \{ v_j \}_{j\in J}\) dans \( \eK\) tels que
    \begin{equation}
        v=\sum_{j\in J}v_je_j.
    \end{equation}
    Cela donne l'existence.

    En ce qui concerne l'unicité, soient \( J \) et \( K\) des parties finies de \( I\) et des coefficients \( \{ v_j \}_{j\in J}\) et \( \{ w_{k} \}_{k\in K}\) tels que
    \begin{equation}
        v=\sum_{j\in J}v_je_j=\sum_{k\in K}w_{k}e_{k}.
    \end{equation}
    Nous posons \( L=J\cup K\) et, pour \( l\in L\),
    \begin{equation}
        \alpha_l=\begin{cases}
            v_l    &   \text{si } l\in J\setminus K\\
            w_l    &    \text{si } l\in K\setminus J\\
            v_l-w_l    &    \text{si } l\in K\cap J.
        \end{cases}
    \end{equation}
    Nous avons alors
    \begin{equation}
        \sum_{l\in L}\alpha_le_l=0,
    \end{equation}
    ce qui implique que \( \alpha_l=0\) pour tout \( l\in L\) parce que la partie \( \{ e_i \}_{i\in I}\) est libre et que \( L\) est finie.

    L'unicité de la décomposition de \( v\) signifie que
    \begin{equation}
        \{ j\in J \tq v_j\neq 0 \}=\{ k\in K\tq w_k\neq 0 \}
    \end{equation}
    et que pour \( l\) dans cet ensemble, \( v_l=w_l\).

    Soit \( j\in J\); il y a deux possibilités : \( j\in J\setminus K\) et \( j\in J\cap K\). Dans le premier cas nous avons déjà vu que \( \alpha_j=v_j=0\). Dans le second cas, \( \alpha_j=v_j-w_j=0\), c'est à dire \( v_j=w_j\).

    Donc \( j\in J\) vérifiant \( v_j\neq 0\) implique \( j\in J\cap K\) et l'égalité des coefficients. Idem avec \( k\in K\) tel que \( w_k\neq 0\) implique \( k\in J\cap K\).
\end{proof}

\begin{definition}
    Un espace vectoriel est \defe{de type fini}{type!fini!espace vectoriel} s'il contient une partie génératrice finie.
\end{definition}
Nous verrons dans les résultats qui suivent que cette définition est en réalité inutile parce qu'une espace vectoriel sera de type fini si et seulement s'il est de dimension finie.

\begin{lemma}       \label{LemytHnlD}
    Si \( E\) a une famille génératrice de cardinal \( n\), alors toute famille de \( n+1\) éléments est liée.
\end{lemma}

\begin{proof}
    Nous procédons par récurrence sur \( n\). Pour \( n=1\), nous avons \( E=\Span(e)\) et donc si \( v_1,v_2\in E\) nous avons \( v_1=\lambda_1 e\), \( v_2=\lambda_2e\) pour certains éléments non nuls \( \lambda_1,\lambda_2\) du corps de base. Nous avons donc \( \lambda_2v_1-\lambda_1v_1=0\). Cela prouve que \( \{ v_1,v_2 \}\) est liée.

    Supposons maintenant que le résultat soit vrai pour \( k<n\), c'est à dire que pour tout espace vectoriel contenant une partie génératrice de cardinal \( k<n\), les parties de \( k+1\) éléments sont liées. Soit maintenant un espace vectoriel muni d'une partie génératrice \( G=\{ e_1,\ldots, e_n \}\) de \( n\) éléments, et montrons que toute partie \( V=\{ v_1,\ldots, v_{n+1} \}\) contenant \( n+1\) éléments est liée. Dans nos notations nous supposons que les \( e_i\) sont des vecteurs distincts et les \( v_i\) également. Nous les supposons également tous non nuls. Étant donné que \( \{ e_i \}\) est génératrice nous pouvons définir les nombres \( \lambda_{ij}\) par
    \begin{equation}
        v_i=\sum_{k=1}^n\lambda_{ij}e_j
    \end{equation}
    Vu que
    \begin{equation}
        v_{n+1}=\sum_{k=1}^n\lambda_{n+1,k}e_k\neq 0,
    \end{equation}
    quitte à changer la numérotation des \( e_i\) nous pouvons supposer que \( \lambda_{n+1,n}\neq 0\). Nous considérons les vecteurs
    \begin{equation}
        w_i=\lambda_{n+1,n}v_i-\lambda_{i,n}v_{n+1}.
    \end{equation}
    En calculant un peu,
    \begin{subequations}
        \begin{align}
            w_i&=\lambda_{n+1,n}\sum_k\lambda_{i,k}e_k-\lambda_{i,n}\sum_k\lambda_{n+1,k}e_k\\
            &=\sum_{k=1}^{n-1}\big( \lambda_{n+1,n}\lambda_{i,k}-\lambda_{i,n}\lambda_{n+1,} \big)e_k
        \end{align}
    \end{subequations}
    parce que les termes en \( e_n\) se sont simplifiés. Donc la famille \( \{ w_1,\ldots, w_n \}\) est une famille de \( n\) vecteurs dans l'espace vectoriel \( \Span\{ e_1,\ldots, e_{n-1} \}\); elle est donc liée par l'hypothèse de récurrence. Il existe donc des nombres \( \alpha_1,\ldots, \alpha_n\in \eK\) non tous nuls tels que
    \begin{equation}        \label{EqOQGGoU}
        0=\sum_{i=1}^n\alpha_iw_i=\sum_{i=1}^n\alpha_i\lambda_{n+1,n}v_i-\left( \sum_{i=1}^n\alpha_i\lambda_{i,n} \right)v_{n+1}.
    \end{equation}
    Vu que \( \lambda_{n+1,n}\neq 0\) et que parmi les \( \alpha_i\) au moins un est non nul, nous avons au moins un des produits \( \alpha_i\lambda_{n+1,n}\) qui est non nul. Par conséquent \eqref{EqOQGGoU} est une combinaison linéaire nulle non triviale des vecteurs de \( \{ v_1,\ldots, v_{n+1} \}\). Cette partie est donc liée.
\end{proof}

\begin{lemma}   \label{LemkUfzHl}
    Soit \( L\) une partie libre et \( G\) une partie génératrice. Si l'ensemble des parties libres \( L'\) telles que \( L\subset L'\subset G\) possède un élément maximum\footnote{Encore une fois, à part quelques cas triviaux, il n'est pas clair à ce point que ce maximum existe.}, alors cet élément est une base.
\end{lemma}
Qu'entend-on par «maximale» ? La partie \( B\) doit être libre, contenir \( L\), être contenue dans \( G\) et de plus avoir la propriété que \( \forall x\in G\setminus B\), la partie \( B\cup\{ x \}\) est liée.

\begin{proof}
    D'abord si \( G\) est une base, alors toutes les parties de \( G\) sont libres et le maximum est \( B=G\). Dans ce cas le résultat est évident. Nous supposons donc que \( G\) est liée.

    La partie \( B=\{ b_1,\ldots, b_l \}\) est libre parce qu'on l'a prise parmi les libres. Montrons que \( B\) est génératrice. Soit \( x\in G\setminus B\); par hypothèse de maximalité, \( B\cup\{ x \}\) est liée, c'est à dire qu'il existe des nombres \( \lambda_i\), \( \lambda_x\) non tous nuls tels que
    \begin{equation}    \label{EqxfkevM}
        \sum_{i=1}^l\lambda_ib_i+\lambda_xx=0.
    \end{equation}
    Si \( \lambda_x=0\) alors un de \( \lambda_i\) doit être non nul et l'équation \eqref{EqxfkevM} devient une combinaison linéaire nulle non triviale des \( b_i\), ce qui est impossible parce que \( B\) est libre. Donc \( \lambda_x\neq 0\) et
    \begin{equation}
        x=\frac{1}{ \lambda_x }\sum_{i=1}^l\lambda_ib_i.
    \end{equation}
    Donc tous les éléments de \( G\setminus B\) sont des combinaisons linéaires des éléments de \( B\), et par conséquent, \( G\) étant génératrice, tous les éléments de \( E\) sont combinaisons linéaires d'éléments de \( B\).
\end{proof}

\begin{theorem}[Théorème de la base incomplète] \label{ThonmnWKs}
    Soit \( E\) un espace vectoriel de type fini sur le corps \( \eK\).
    \begin{enumerate}
        \item   \label{ItemBazxTZ}
            Si \( L\) est une partie libre et si \( G\) est une partie génératrice contenant \( L\), alors il existe une base \( B\) telle que \( L\subset B\subset G\).
        \item       \label{ITEMooFVJXooGzzpOu}
            Toute partie libre peut être étendue en une base.
        \item
            Toutes les bases sont finies et ont même cardinal.
    \end{enumerate}
\end{theorem}
\index{théorème!base incomplète}

\begin{proof}
    Point par point.
    \begin{enumerate}
        \item
    Vu que \( E\) est de type fini, il admet une partie génératrice \( G\) de cardinal fini \( n\). Donc une partie libre est de cardinal au plus \( n\) par le lemme~\ref{LemytHnlD}. Soit \( L\), une partie libre contenue dans \( G\) (ça existe : par exemple \( L=\emptyset\)). La partie \( B\) maximalement libre contenue dans \( G\) et contenant \( L\) est une base par le lemme~\ref{LemkUfzHl}.
\item
Notons que puisque \( E\) lui-même est générateur, le point~\ref{ItemBazxTZ} implique que toute partie libre peut être étendue en une base.
\item
    Soient \( B\) et \( B'\), deux bases. En particulier \( B\) est génératrice et \( B'\) est libre, donc le lemme~\ref{LemytHnlD} indique que \( \Card(B')\leq \Card(B)\). Par symétrie on a l'inégalité inverse. Donc \( \Card(B)=\Card(B')\).
    \end{enumerate}
\end{proof}

\begin{remark}      \label{REMooYGJEooEcZQKa}
    Le théorème de la base incomplete~\ref{ThonmnWKs}\ref{ITEMooFVJXooGzzpOu} est ce qui permet de construire une base d'une espace vectoriel en « commençant par» une base d'un sous-espace. En effet si \( H\) est un sous-espace de \( E\) alors une base de \( H\) est une partie libre de \( E\) et donc peut être étendue en une base de \( E\).
\end{remark}

\begin{definition}
    La \defe{dimension}{dimension} d'un espace vectoriel de type fini est la cardinal d'une de ses bases.
\end{definition}
\index{dimension!définition}

\begin{example}
    Il existe une infinité de bases de $\eR^m$. On peut démontrer que le cardinal de toute base de $\eR^m$ est $m$, c'est à dire que toute base de $\eR^m$ possède exactement $m$ éléments.

    La base de $\eR^m$ qu'on dit \defe{canonique}{canonique!base}\index{base!canonique de $\eR^m$} (c.à.d. celle qu'on utilise tout le temps) est $\mathcal{B}=\{e_1,\ldots, e_m\}$, où le vecteur $e_j$ est
    \begin{equation}\nonumber
      e_j=
    \begin{array}{cc}
      \begin{pmatrix}
        0\\\vdots\\0\\1\\ 0\\\vdots\\0
      \end{pmatrix} &
      \begin{matrix}
        \quad\\\quad\\\leftarrow\textrm{j-ème} \quad\\\quad\\\quad\\
      \end{matrix}
    \end{array}.
    \end{equation}
    La composante numéro $j$ de $e_i$ est $1$ si $i=j$ et $0$ si $i\neq j$. Cela s'écrit $(e_i)_j=\delta_{ij}$ où $\delta$ est le \defe{symbole de Kronecker}{Kronecker} défini par
    \begin{equation}
        \delta_{ij}=\begin{cases}
            1	&	\text{si }i=j\\
            0	&	 \text{si }i\neq j
        \end{cases}
    \end{equation}
    Les éléments de la base canonique de $\eR^m$ peuvent donc être écrits $e_i=\sum_{k=1}^m\delta_{ik}e_k$.
\end{example}

Le théorème suivant est essentiellement une reformulation du théorème~\ref{ThonmnWKs}.
\begin{theorem} \label{ThoBaseIncompjblieG}     \label{ThoMGQZooIgrXjy}
    Soit \( E\) un espace vectoriel de dimension finie et \( \{ e_i \}_{i\in I}\) une partie génératrice de \( E\).

    \begin{enumerate}
        \item
            Il existe \( J\subset I\) tel que \( \{ e_i \}_{i\in J}\) est une base. Autrement dit : de toute partie génératrice nous pouvons extraire une base.
        \item
            Soit \( \{ f_1,\ldots, f_l \}\) une partie libre. Alors nous pouvons la compléter en utilisant des éléments \( e_i\). C'est à dire qu'il existe \( J\subset I\) tel que \( \{ f_k \}\cup\{ e_i \}_{i\in J}\) soit une base.
        \item       \label{ItemHIVAooPnTlsBi}
            Si \( E\) est un espace vectoriel de dimension \( n\), alors toute partie libre contenant \( n\) éléments est une base.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous ne prouvons que le troisième point. Une partie libre contenant \( n\) éléments peut être étendue en une base; si ladite extension est non triviale (c'est à dire qu'on ajoute vraiment au moins un élément) une telle base contiendra une partie de \( n+1\) éléments qui serait liée par le lemme~\ref{LemytHnlD}.
\end{proof}

Soit \( F\) un sous-espace vectoriel de l'espace vectoriel \( E\). La \defe{codimension}{codimension} de \( F\) dans \( E\) est
\begin{equation}
    \codim_E(F)=\dim(E/F).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Et en dimension infinie}
%---------------------------------------------------------------------------------------------------------------------------

Dans ZFC, en dimension infinie, il existe aussi une base pour tout espace vectoriel ainsi qu'un théorème de la base incomplète. Nous ne parlerons pas de ce qu'il se passe lorsque nous ne considérons que ZF\footnote{Si vous ne savez pas ce que signifient les sigles «ZF» et «ZFC» vous ne devriez pas être en train de lire ceci, et encore moins penser à le resservir à un jury d'agrégation.}.

\begin{lemma}[\cite{ooXEFKooHikcdE}]        \label{LEMooSSRXooIyfgNz}
    Soient un \( \eK\)-espaces vectoriel \( E\) et un sous-espace vectoriel \( V\) de \( E\). Soient encore deux sous-espaces vectoriels \( W_1\) et \( W_2\) tels que
    \begin{enumerate}
        \item
            \( V\cap W_1=\{ 0 \}\);
        \item
            \( V+W_2=E\).
    \end{enumerate}
    Alors il existe un supplémentaire \( W\) de \( V\) tel que \( W_1\subset W\subset W_2\).
\end{lemma}

Juste une remarque : dans le Frido le symbole «\( \subset\)» ne signifie pas une inclusion stricte.

\begin{proof}
    Nous divisons en petits morceaux.
    \begin{subproof}
        \item[Un gros ensemble]
            Soit \( \mA\) l'ensemble des sous-espaces vectoriels \( S\) de \( E\) tels que \( W_1\subset S\subset W_2\) et \( S\cap V=\{ 0 \}\). Vu que \( W_1\subset \mA\), cet ensemble n'est pas vide. De plus \( \mA\) est partiellement ordonné pour l'inclusion.
        \item[\( \mA\) est inductif]
            Nous prouvons maintenant que \( \mA\) est inductif\footnote{Définition~\ref{DefGHDfyyz}.}. Pour cela, soit une partie \( \mA'\) totalement ordonnée et \( U=\bigcup_{A\in \mA'}A\). Quelques propriétés de \( U\).
            \begin{enumerate}
                \item
                    La partir \( U\) est un sous-espace vectoriel de \( E\). En effet si \( x,y\in U\), alors il existe \( A_1,A_2\in\mA'\) tels que \( x\in A_1\) et \( y\in A_2\). Vu que \( \mA'\) est totalement ordonné, soit \( A_1\) soit \( A_2\) est inclus dans l'autre. Disons que c'est \( A_1\subset A_2\). Alors \( x,y\in A_2\) et \( x+y\in A_2\subset U\).
                \item
                    Elle vérifie \( W_1\subset U\subset W_2\).
            \end{enumerate}
            Donc \( U\in \mA\) et majore \( \mA'\) pour l'inclusion. En bref, \( \mA\) est inductif.
        \item[Utilisation de Zorn]

            Le lemme de Zorn~\ref{LemUEGjJBc} nous donne alors un maximum \( W\) de \( \mA\). Ce maximum vérifie
            \begin{enumerate}
                \item
                    \( W\cap V=\{ 0 \}\),
                \item
                    \( W_1\subset W\subset W_2\),
                \item
                    pour tout \( W'\in\mA\), nous avons \( W'\subset W\) parce que \( W\) est maximum.
            \end{enumerate}
        \item[Supplémentaire]
            Montrons que ce \( W\) est un supplémentaire de \( V\). Soit \( x\in E\). Le but est maintenant de trouver une décomposition de \( x\) en somme d'un élément de \( W\) et un de \( V\). Vu que \( V+W_2=E\) nous avons \( v\in V\) et \( w_2\in W_2\) tels que \( x=v+w_2\). Si \( w_2\in W\) alors c'est fait. Sinon \ldots

            Soit \( X=\Span\{ W,w_2 \}\). Vu que \( X\) contient strictement \( W\) et que \( W\) est maximum dans \( \mA\), la partie \( X\) n'est pas un élément de \( \mA\). Vu que \( X\) est un sous-espace vectoriel de \( E\) tel que \( W_1\subset X\subset W_2\), la seule possibilité pour que \( X\) ne soit pas dans \( \mA\) est que \( X\cap V\neq \{ 0 \}\). Soit donc \( y\neq 0\) dans \( X\cap V\). Par définition de \( X\),
            \begin{equation}
                y=w'+\lambda w_2
            \end{equation}
            pour \( w'\in W\), \( w_2\in W_2\) et \( \lambda\in \eK\). Nous avons \( \lambda\neq 0\), sinon ce serait \( y\in W\cap V\{ 0 \}\). Donc nous pouvons écrire \( w_2=(y-w)/\lambda\) et finalement
            \begin{equation}
                x=v+\frac{1}{ \lambda }(y-w')=\underbrace{v+\frac{1}{ \lambda }y}_{\in V}-\underbrace{\frac{1}{ \lambda }w'}_{\in W}.
            \end{equation}
            Donc oui pour \( E=V+W\).
    \end{subproof}
\end{proof}

\begin{corollary}
    Tout sous-espace vectoriel d'un espace vectoriel possède un supplémentaire.
\end{corollary}

\begin{proof}
    C'est un cas particulier du lemme~\ref{LEMooSSRXooIyfgNz}.
\end{proof}

\begin{proposition}
    Tout espace vectoriel (non \( \{ 0 \}\)) possède une base.
\end{proposition}

\begin{proof}
    Soit \( \mA\) l'ensemble des familles libres de \( E\). Il n'est pas vide parce que \( \{ v \}\) en est une dès que \( v\) est non nul dans \( E\). Rapidement :
    \begin{itemize}
        \item l'ensemble \( \mA\) est ordonné pour l'inclusion,
        \item si \( \mA'\) est une partie totalement ordonnée, l'union est un majorant,
        \item donc \( \mA\) est inductif,
        \item soit un maximum \( F\) de \( \mA\).
    \end{itemize}
    La partie \( F\) est libre parce qu'elle est dans \( \mA\). Elle est génératrice parce que si \( v\) n'est pas dans \( \Span(F)\) alors la partie \( F\cup\{ v \}\) est encore libre, et majore strictement $F$ pour l'inclusion, ce qui n'est pas possible.

    Donc \( F\) est une base de \( E\).
\end{proof}

\begin{theorem}[Base incomplère, dimension quelconque]      \label{THOooOQLQooHqEeDK}
    Soit une partie \( \{ e_i \}_{i\in I}\) génératrice de l'espace vectoriel \( E\) (ici, \( I\) est un ensemble quelconque). Soit \( I_0\in I\) tel que \( \{ e_i \}_{i\in I_0}\) soit libre.

    Alors il existe \( I_1\) tel que \( I_0\subset I_1\subset I\) tel que \( \{ e_i \}_{i\in I_1}\) soit une base de \( E\).
\end{theorem}

Note : une telle partie \( I_0\) existe en prenant un singleton. Mais l'existence n'est pas le sujet ici.

\begin{proof}
    Soit \( \mA\) l'ensemble des parties \( J\) de \( I\) telles que \( I_0\subset J\subset I\) et telles que \( \{ e_i \}_{i\in J}\) soit libre.

    Encore une fois, \( \mA\) est inductif pour l'ordre partiel donné par l'inclusion. Soit \( J\) un maximum. Vu que \( J\in\mA\), la partie \( \{ e_i \}_{i\in J}\) est libre. Mais elle est également génératrice parce que si \( e_k\) n'est pas dedans, \( J\) ne serait pas maximum, étant majorée par \( J\cup\{ k \}\).

    Donc \( \{ e_i \}_{i\in J}\) engendre tous les \( e_i\) avec \( i\in I\) et donc tous les éléments de \( E\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Matrices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous parlons maintenant de matrices. Les matrices sont de simples tableaux de nombres sur lesquels nous allons définir des opérations. Le lien entre ces opérations et les opérations correspondantes sur les applications linéaires sera fait plus tard. D'ailleurs nous n'avons pas encore défini la notion d'application linéaire\footnote{Définition~\ref{DEFooULVAooXJuRmr}.}.

\begin{definition}
    Soit une corps \( \eK\) ainsi que des entiers \( m\), \( n\) strictement positifs. L'ensemble \( \eM(n\times n,\eK)\) est l'ensemble des applications
    \begin{equation}
        \{ 1,\ldots, n \}\times \{ 1,\ldots, m \}\to \eK,
    \end{equation}
    et est appelé ensemble des \defe{matrices}{matrice} \(n\times m\) sur \( \eK\).
\end{definition}
Si \( A\) est une matrice, nous notons \( A_{ij}\) au lieu de \( A(i,j)\) l'image de \( (i,j)\) par l'application \( A\). Si \( n=m\) alors nous disons que la matrice est \defe{carré}{carré!matrice}, et nous notons \( \eM(n,\eK)\) au lieu de \( \eM(n\times n,\eK)\).

Si \( A,B\in \eM(n\times m,\eK)\) et \( \lambda\in \eK\)  nous définissons
\begin{description}
    \item[La somme] \( (A+B)_{ij}=A_{ij}+B_{ij}\),
    \item[Le produit par un scalaire] \( (\lambda A)_{ij}=\lambda A_{ij}\),
    \item[La transposée] \( A^t_{ij}=A_{ji}\),
    \item[La trace] \( \tr(A)=\sum_iA_{ii}\).
\end{description}
Cela donne à \( \eM(n\times m,\eK)\) une structure d'espace vectoriel. La notion de produit de deux matrices est à peine plus subtile; c'est une application
\begin{equation}
    \begin{aligned}
         \eM(n\times p,\eK)\times \eM(p\times m,\eK)&\to \eM(n\times m,\eK) \\
         (A,B)&\mapsto (AB)_{ij}=\sum_{k=1}^pA_{ik}B_{kj}.
    \end{aligned}
\end{equation}
La motivation de cette définition apparaîtra plus loin, mais le Frido n'étant pas un livre d'introduction, j'imagine que le lecteur a déjà une idée.

\begin{lemma}[\cite{MonCerveau}]
    Nous avons :
    \begin{enumerate}
        \item
            \( (AB)^t=B^tA^t\),
        \item
            \( \tr(ABC)=\tr(CAB)\).
    \end{enumerate}
    La seconde égalité est importante et est nommée \defe{invariance cyclique}{invariance cyclique!trace} de la trace.
\end{lemma}

\begin{proof}
    La première est un simple calcul :
    \begin{equation}
        (AB)^t_{ij}=(AB)_{ji}=\sum_kA_{jk}B_{ki}=\sum_kA^t_{kj}B^t_{ik}=(B^tA^t)_{ij}.
    \end{equation}
    Pour la seconde :
    \begin{equation}
        \tr(ABC)=\sum_{ikl}A_{ik}B_{kl}C_{li}=\sum_{ikl}C_{li}A_{ik}B_{kl}=\sum_l(CAB)_l=\tr(CAB).
    \end{equation}
\end{proof}

Nous notons \( \mtu\) la matrice carrée\footnote{En toute rigueur nous devrions donner sa taille, mais elle sera toujours claire dans le contexte.} donnée par
\begin{equation}
    \mtu_{ij}=\begin{cases}
        1    &   \text{si } i=j\\
        0    &    \text{sinon.}
    \end{cases}
\end{equation}
Il est vite vu que si \( A\) est une matrice de même taille que \( \mtu\) alors \( A\mtu=\mtu A=A\).

\begin{propositionDef}[\cite{MonCerveau}]
    Une matrice \( A\in \eM(n,\eK)\) est \defe{inversible}{inversible!matrice} si il existe une matrice \( B\) telle que
    \begin{equation}
        AB=BA=\mtu.
    \end{equation}
    Dans ce cas, une telle matrice est unique et est nommée \defe{inverse}{matrice!inverse} de \( A\).
\end{propositionDef}

\begin{proof}
    Supposons que \( B_1\) et \( B_2\) soient des inverses de \( A\). Alors
    \begin{subequations}
        \begin{align}
            \mtu=AB_1=\mtu\\
            \mtu=AB_2=B_2A.
        \end{align}
    \end{subequations}
    En multipliant la première par \( B_2\) à gauche :
    \begin{equation}
        B_2=B_2AB_1,
    \end{equation}
    mais vu que \( B_2A=\mtu\) nous avons \( B_2=B_1\). D'où l'unicité.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Si \( A\in\eM(n,\eK)\) nous définissons le \defe{déterminant}{déterminant!matrice} de \( A\) par la formule
    \begin{equation}
        \det(A)=\sum_{\sigma\in S_n}\prod_{i=1}^nA_{i\sigma(i)}
    \end{equation}
    où la somme est effectuée sur tous les éléments du groupe symétrique\footnote{Définition~\ref{DEFooJNPIooMuzIXd}.} \( S_n\).
\end{definition}
En se souvenant que \( | S_n |=n!\), nous sommes frappés de stupeur devant le fait que le nombre de termes dans la somme croît de façon factorielle (c'est plus qu'exponentiel, pour info) en la taille de la matrice. Cette formule est donc sans espoir pour une matrice plus grande que \( 3\times 3\) ou à la rigueur \( 4\times 4\) à la main. À l'ordinateur, il est possible de monter plus haut, mais pas tellement.

\begin{theorem}     \label{THOooSNXWooSRjleb}
    Une matrice sur \( \eR\) ou \( \eC\) est inversible si et seulement si son déterminant est non nul.
\end{theorem}

\begin{proposition}     \label{PROPooLOQMooQzrZLH}
    Si deux lignes ou deux colonnes d'une matrice sont égales alors son déterminant est nul.
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transvections}
%---------------------------------------------------------------------------------------------------------------------------

Nous nommons \( E_{ij}\) la matrice remplie de zéros sauf à la case \( ij\) qui vaut \( 1\). Autrement dit
\begin{equation}
    (E_{ij})_{kl}=\delta_{ik}\delta_{jl}.
\end{equation}
\begin{definition}
    Une \defe{matrice de transvection}{transvection (matrice)}\index{matrice!de transvection} est une matrice de la forme
    \begin{equation}
        T_{ij}(\lambda)=\id+\lambda E_{ij}
    \end{equation}
    avec \( i\neq j\).

    Une \defe{matrice de dilatation}{matrice!de dilatation}\index{dilatation (matrice)} est une matrice de la forme
    \begin{equation}
        D_i(\lambda)=\id+(\lambda-1)E_{ii}.
    \end{equation}
    Ici le \( (\lambda-1)\) sert à avoir \( \lambda\) et non \( 1+\lambda\). C'est donc une matrice qui dilate d'un facteur \( \lambda\) la direction \( i\) tout en laissant le reste inchangé.

    Si \( \sigma\) est une permutation (un élément du groupe symétrique \( S_n\)) alors la \defe{matrice de permutation}{matrice!de permutation}\index{permutation!matrice} associée est la matrice d'entrées
    \begin{equation}
        (P_{\sigma})_{ij}=\delta_{i\sigma(j)}.
    \end{equation}
\end{definition}

\begin{lemma}   \label{LemyrAXQs}
    La matrice \( T_{ij}(\lambda)A=(\mtu+\lambda E_{ij})A\) est la matrice \( A\) à qui on a effectué la substitution
    \begin{equation}
        L_i\to L_i+\lambda L_j.
    \end{equation}
    La matrice \( AT_{ij}(\lambda)\) est la substitution
    \begin{equation}
        C_j\to C_j+\lambda C_i.
    \end{equation}

    La matrice \( AP_{\sigma}\) est la matrice \( A\) dans laquelle nous avons permuté les colonnes avec \( \sigma\).

    La matrice \( P_{\sigma}A\) est la matrice \( A\) dans laquelle nous avons permuté les lignes avec \( \sigma^{-1}\).
\end{lemma}

\begin{proof}
    Calculons la composante \( kl\) de la matrice \( E_{ij}A\) :
    \begin{subequations}
        \begin{align}
            (E_{ij}A)_{kl}&=\sum_m(E_{ij})_{km}A_{ml}\\
            &=\sum_m\delta_{ik}\delta_{jm}A_{ml}\\
            &=\delta_{ik}A_{jl}.
        \end{align}
    \end{subequations}
    C'est donc la matrice pleine de zéros, sauf la ligne \( i\) qui est donnée par la ligne \( j\) de \( A\). Donc effectivement la matrice
    \begin{equation}
        A+\lambda E_{ij}A
    \end{equation}
    est la matrice \( A\) à laquelle on a substitué la ligne \( i\) par la ligne \( i\) plus \( \lambda\) fois la ligne \( j\).

    En ce qui concerne l'autre assertion sur les transvections, le calcul est le même et nous obtenons
    \begin{equation}
        (AE_{ij})=A_{ki}\delta_{jl}.
    \end{equation}

    Pour les matrices de permutation, nous avons
    \begin{equation}
        (AP_{\sigma})_{kl}=A_{k\sigma(l)}
    \end{equation}
    et
    \begin{equation}
        (P_{\sigma}A)_{kl}=\sum_m\delta_{k\sigma(m)}A_{ml}=\sum_m\delta_{\sigma^{-1}(k)m}A_{ml}=A_{\sigma^{-1}(k)l}.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Mineur, rang}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{ooLFZTooJWJUed}]
    Les mineurs d'une matrice sont les déterminants de ses sous-matrices carrées.
\end{definition}
Dans la suite nous désignerons souvent par le mot «mineur» la sous-matrice carrée elle-même au lieu de son déterminant.

\begin{definition}      \label{DEFooVVBYooJbliTi}
    Le \defe{rang}{rang de matrice} d'une matrice est la taille de son plus grand mineur non nul.
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices équivalentes et similaires}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefBLELooTvlHoB}
    Deux relations d'équivalence entre les matrices.
    \begin{enumerate}
        \item   \label{ItemPFXCooOUbSCt}
    Deux matrices \( A\) et \( B\) sont \defe{équivalentes}{matrice!équivalence} dans \( \eM(n,\eK)\) s'il existe \( P,Q\in\GL(n,\eK)\) telles que \( A=PBQ^{-1}\).
\item
    Deux matrices sont \defe{semblables}{matrices!similitude} s'il existe une matrice \( P\in \GL(n,\eK)\) telle que \( A=PBP^{-1}\).
    \end{enumerate}
\end{definition}

\begin{lemma}   \label{LemZMxxnfM}
    Une matrice de rang\footnote{Définition du rang d'une matrice :~\ref{DEFooVVBYooJbliTi}.} \( r\) dans \( \eM(n,\eK)\) est équivalente à la matrice par blocs
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r    &   0    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
\end{lemma}
\index{rang!classe d'équivalence}

\begin{proof}
    Nous devons prouver que pour toute matrice \( A\in\eM(n,\eK)\) de rang \( r\), il existe \( P,Q\in\GL(n,\eK)\) telles que \(QAP=J_r\). Soit \( \{ e_i \}\) la base canonique de \( \eK^n\), puis \( \{ f_i \}\) une base telle que \( Af_i=0\) dès que \( i>r\).

    Nous considérons la matrice inversible \( P\) telle que \( Pe_i=f_i\). Elle vérifie
    \begin{equation}
        APe_i=Af_i=\begin{cases}
            0    &   \text{si } i>r\\
            \neq 0    &    \text{sinon}.
        \end{cases}
    \end{equation}
    La matrice \( AP\) se présente donc sous la forme
    \begin{equation}
        AP=\begin{pmatrix}
            M    &   0    \\
            *    &   0
        \end{pmatrix}
    \end{equation}
    où \( M\) est une matrice \( r\times r\). Nous considérons maintenant une base \( \{ g_i \}_{i=1,\ldots, n}\) dont les \( r\) premiers éléments sont les \( r\) premières colonnes de \( AP\) et une matrice inversible \( Q\) telle que \( Qg_i=e_i\). Alors
    \begin{equation}
        QAPe_i=\begin{cases}
            e_i    &   \text{si } i<r\\
            0    &    \text{sinon}.
        \end{cases}.
    \end{equation}
    Cela signifie que \( QAP\) est la matrice \( J_r\).
\end{proof}

\begin{corollary}[Équivalence et rang]      \label{CorGOUYooErfOIe}
    Deux matrices sont équivalentes\footnote{Définition~\ref{DefBLELooTvlHoB}\ref{ItemPFXCooOUbSCt}.} si et seulement si elles sont de même rang.
\end{corollary}

\begin{proof}
    D'abord il y a des implicites dans l'énoncé. Vu que nous voulons soit pas hypothèse soit par conclusion que les matrices $A$ et \( B\) soient équivalentes, nous supposons qu'elles ont même dimension. Soient donc \( A\) et \( B\) deux matrices carrées représentant des applications linéaires de l'espace vectoriel \( E=\eK^n\) vers lui-même.

    Par le lemme~\ref{LemZMxxnfM}, deux matrices de même rang \( r\) sont équivalentes à \( J_r\). Elles sont donc équivalentes entre elles.

    Inversement, supposons que \( A\) et \( B\) soient deux matrices équivalentes : \( A=PBQ^{-1}\) avec \( P\) et \( B\) inversibles. Alors
    \begin{subequations}
        \begin{align}
            \Image(PBQ^{-1})&=\{ PBQ^{-1}v\tq v\in E \}\\
            &=PB\underbrace{\{ Q^{-1}v\tq v\in E \}}_{=E}\\
            &=P\big( B(E) \big).
        \end{align}
    \end{subequations}
    L'ensemble \( B(E)\) est un sous-espace vectoriel de \( E\). Vu que le rang de \( P\) est maximum, la dimension de \( P\big( B(E) \big)\) est la même que celle de \( B(E)\). Par conséquent
    \begin{equation}
        \dim\Big( \Image(PBQ^{-1}) \Big)=\dim\big( B(E) \big)=\rang(B).
    \end{equation}
    Le membre de gauche de cela n'est autre que \( \rang(A)=\dim\big( \Image(PBQ^{-1}) \big)\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Algorithme des facteurs invariants}
%---------------------------------------------------------------------------------------------------------------------------

\begin{probleme}
    À propos de matrices.
    \begin{enumerate}
        \item
            Définir les anneaux de matrices, et en particulier \(GL_n(A) \).
        \item
            Définir \( \eM(n,m,A)\) comme étant l'ensemble des matrices ayant \( n\) lignes et \( m\) colonnes.
    \end{enumerate}
\end{probleme}

\begin{proposition}[Algorithme des facteurs invariants\cite{KXjFWKA}]   \label{PropPDfCqee}
    Soit \( (A,\delta)\) un anneau euclidien muni de son stathme  et \( U\in \eM(n,m,A)\). Alors il existe \( d_1,\ldots, d_s\in A^*\) et des matrices \( P\in\GL(m,A)\), \( Q\in \GL(n,A)\) tels que nous ayons
    \begin{equation}
        U=P \begin{pmatrix}
            \begin{matrix}
                d_1    &       &       \\
                    &   \ddots    &       \\
                    &       &   d_s
            \end{matrix}&   0    \\
            0    &   0
        \end{pmatrix}Q
    \end{equation}
    avec \( d_i\divides d_{i+1}\) pour tout \( i\).
\end{proposition}
\index{anneau!euclidien!facteurs invariants}
\index{algorithme!facteurs invariants}

\begin{proof}
    Nous allons donner la preuve plus ou moins sous forme d'algorithme.

    D'abord si \( U=0\) c'est bon, on a la réponse. Sinon, nous prenons l'élément \( (i_0,j_0)\) dont le stathme est le plus petit et nous l'amenons en \( (1,1)\) par les permutations
    \begin{equation}
        \begin{aligned}[]
            C_1&\leftrightarrow C_{j_0}\\
            L_1&\leftrightarrow L_{i_0}
        \end{aligned}
    \end{equation}
    Ensuite nous traitons la première colonne jusqu'à amener des zéros partout en dessous de \( u_{11}\) de la façon suivante : pour chaque ligne successivement nous calculons la division euclidienne
    \begin{equation}
        u_{i1}=qu_{11}+r_i,
    \end{equation}
    et nous faisons
    \begin{equation}
        L_i\to L_i-qL_1,
    \end{equation}
    c'est à dire que nous enlevons le maximum possible et il reste seulement \( r_i\) en \( u_{i1}\). Vu que le but est de ne laisser que des zéros dans la première colonne, si le reste n'est pas zéro nous ne sommes pas content\footnote{S'il est zéro, nous passons à la ligne suivante}. Dans ce cas nous permutons \( L_1\leftrightarrow L_i\), ce qui aura pour effet de strictement diminuer le stathme de \( u_{11}\) parce qu'on va mettre en \( u_{11}\) le nombre \( r_i\) dont le stathme est strictement plus petit que celui de \( u_{11}\).

    En faisant ce jeu de division euclidienne puis échange, on diminue toujours le stathme de \( u_{11}\), donc ça finit par s'arrêter, c'est à dire qu'à un certain moment la division euclidienne de \( u_{i1}\) par \( u_{11}\) va donner un reste zéro et nous serons content.

    Une fois la première colonne ramenée à la forme
    \begin{equation}
        C_1=\begin{pmatrix}
            u_{11}    \\
            0    \\
            \vdots    \\
            0
        \end{pmatrix},
    \end{equation}
    nous faisons tout le même jeu avec la première ligne en faisant maintenant des sommes divisions et permutations de colonnes. Notons que ce faisant nous ne changeons plus la première colonne.

    En fin de compte nous trouvons une matrice\footnote{Nous nommons toujours par la même lettre \( U\) la matrice originale et la modifiée, comme il est d'usage en informatique.}
    \begin{equation}
        U=\begin{pmatrix}
            u_{11}   &   0    &   \ldots    &   0    \\
             0   &       &       &       \\
             \vdots   &       &   A    &       \\
             0   &       &       &
         \end{pmatrix}
    \end{equation}
    Si l'élément \( u_{11}\) ne divise pas un des éléments de \( A\), disons \( a_{ij}\), alors nous faisons
    \begin{equation}
        C_1\to C_1-C_j.
    \end{equation}
    Cela nous détruit un peu la première colonne, mais ne change pas \( u_{11}\). Nous avons maintnant
    \begin{equation}
        U=\begin{pmatrix}
            u_{11}   &   0    &   \ldots    &   0    \\
             0   &       &       &       \\
             *   &       &       &       \\
             u_{ij}   &       &   A    &       \\
             *   &       &       &       \\
             0   &       &       &
         \end{pmatrix}
    \end{equation}
    Et nous refaisons tout le jeu depuis le début. Cependant lorsque nous allons nous attaquer à la ligne \( i\), \( u_{11}\) ne divisera pas \( u_{ij}\), ce qui donnera lieu à une division euclidienne et un échange \( L_1\leftrightarrow L_i\). L'échange consistant à mettre \( r_i\) à la place de \( u_{11}\) et inversement  diminuera encore strictement le stathme. Encore une fois nous allons travailler jusqu'à avoir la matrice sous la forme
    \begin{equation}    \label{EqADcNVgI}
        U=\begin{pmatrix}
            u_{11}   &   0    &   \ldots    &   0    \\
             0   &       &       &       \\
             \vdots   &       &   A    &       \\
             0   &       &       &
         \end{pmatrix},
    \end{equation}
    sauf que cette fois le stathme de \( u_{11}\) est strictement plus petit que la fois précédente. Si \( u_{11}\) ne divise toujours pas tous les éléments de \( A\), nous recommençons encore et encore. En fin de compte nous finissons par avoir une matrice de la forme \eqref{EqADcNVgI} avec \( u_{11}\) qui divise tous les éléments de \( A\).

    Une fois que cela est fait, il faut continuer en recommençant tout sur la matrice \( A\). Nous avons maintenant
    \begin{equation}
        U=\begin{pmatrix}
            \begin{matrix}
                u_{11}  &       \\
                &   u_{22}
            \end{matrix}&   0    \\
            0    &   B
        \end{pmatrix}.
    \end{equation}
    Sous cette forme nous avons \( u_{11}\divides u_{22}\) et \( u_{11}\) divise tous les éléments de \( B\). En effet \( u_{11}\) divisant tous les éléments de \( A\), il divise toutes les combinaisons de ces éléments. Or tout l'algorithme ne consiste qu'à prendre des combinaisons d'éléments.

    Nous finissons donc bien sûr une matrice comme annoncée. De plus n'ayant effectué que des combinaisons de lignes, nous avons seulement multiplié par des matrices inversibles (lemme~\ref{LemyrAXQs}).
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Applications linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définition}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooULVAooXJuRmr}
    Soient des espaces vectoriels \( E \) et \( F\) sur le corps \( \eK\). Une application \( T\colon E\to F\) est dite \defe{linéaire}{linéaire!application} si
    \begin{itemize}
        \item $T(x+y)=T(x)+T(y)$ pour tout $x$ et $y$ dans \( E\),
        \item $T(\lambda x)=\lambda T(x)$ pour tout $\lambda$ dans $\eK$ et \( x\) dans \( E\).
    \end{itemize}
\end{definition}
Si vous avez bien suivi, les égalités dans la définition~\ref{DEFooULVAooXJuRmr} sont des égalités dans \( F\).


\begin{example}
Pour tout $b$ dans $\eR$ la fonction $T_b(x)= bx$ est une application linéaire de $\eR$ dans $\eR$. En effet,
\begin{itemize}
\item  $T_b(x+y)= b(x+y)= bx + by = T_b(x)+T_b(y)$,
\item $T_b(ax)=b(ax)= abx = a T_b(x)$.
\end{itemize}
De la même façon on peut montrer que la fonction $T_{\lambda}$ définie par $T_{\lambda}(x)=bx$ est un application linéaire de $\eR^m$ dans $\eR^m$ pour tout $\lambda$ dans $\eR$ et $m$ dans $\eN$.
\end{example}

\begin{example}\label{ex_affine}
	Soit $m=n$. On fixe $\lambda$ dans $\eR$ et $v$ dans $\eR^m$. L'application $U_{\lambda}$ de $\eR^m$ dans $\eR^m$ définie par $U_{\lambda}(x)=\lambda x+v$ n'est pas une application linéaire, parce que
\[
U_{\lambda}(ax)=\lambda(ax)+v\neq \lambda(bx+v)=a U_{\lambda}(x).
\]
\end{example}

\begin{example}\label{exampleT_A}
	Soit $A$ une matrice fixée de $\mathcal{M}_{n\times m}$\nomenclature{$\mathcal{M}_{n\times m}$}{l'ensemble des matrices $n\times m$}. La fonction $T_A\colon \eR^m\to \eR^n$ définie par $T_A(x)=Ax$ est une application linéaire. En effet,
\begin{itemize}
\item  $T_A(x+y)= A(x+y)= Ax + Ay = T_A(x)+T_A(y)$,
\item $T_A(ax)=A(ax)= a(Ax) = a T_A(x)$.
\end{itemize}
\end{example}

On peut observer que, si on identifie $\mathcal{M}_{1\times 1}$ et $\eR$, on obtient le premier exemple comme cas particulier.

\begin{definition}[Quelques ensembles d'applications linéaires]      \label{DEFooOAOGooKuJSup}
    Soient \( E\) et \( F\) des espaces vectoriels.
    \begin{itemize}
        \item
        L'ensemble des applications linéaires de \( E\) vers \( F\) est noté $\aL(E,F)$\nomenclature{$\aL(E,F)$}{Ensemble des applications linéaires de $E$ dans $F$}.
        \item Une application linéaire \( E\to E\) est un \defe{endomorphisme}{endomorphisme} de \( E\). L'ensemble des endomorphismes de \( E\) est noté \( \End(E)\)\nomenclature[B]{$\End(E)$}{les endomorphismes de \( E\)}.
        \item Un endomorphisme bijectif est un \defe{automorphisme}{automorphisme!d'espace vectoriel}. L'ensemble des automorphismes de \( E\) est noté \( \Aut(E)\)\nomenclature[B]{$\Aut(E)$}{automorphisme de l'espace vectoriel \( E\)}.
        \item
            Une application linéaire bijective \( E\to F\) est un \defe{isomorphisme}{isomorphisme!espaces vectoriels} d'espace vectoriel. L'ensemble des isomorphismes \( E\to F\) est noté \( \GL(E,F)\).
    \end{itemize}
\end{definition}

\begin{remark}
    Les ensembles définis en~\ref{DEFooOAOGooKuJSup} concernent la structure d'espace vectoriel seulement. Lorsque nous verrons la notion d'espace vectoriel normé, nous demanderons de plus la continuité, laquelle n'est pas automatique en dimension infinie. Voir aussi les définitions~\ref{DEFooTLQUooJvknvi}.
\end{remark}

\begin{definition}  \label{DefDQRooVGbzSm}
    Si \( V\) et \( W\) sont des espaces vectoriels nous munissons \( \aL(V,W)\) d'une structure d'espace vectoriel en définissant la somme et le produit par un scalaire de la façon suivante. Si $T$ et $U$ sont des élément de $\aL(V,W)$ et si $\lambda$ est un réel, nous définissons les éléments $T+U$ et $\lambda T$ par
    \begin{enumerate}
        \item
            $(T+U)(x)=T(x)+U(x)$;
        \item
            $(\lambda T)(x)=\lambda T(x)$
    \end{enumerate}
    pour tout \( x\in V\).
\end{definition}

\begin{normaltext}
    Pour bien faire, il faudrait vérifier que la définition~\ref{DefDQRooVGbzSm} produit effectivement une structure d'espace vectoriel sur \( \aL(V,W)\).
\end{normaltext}

\begin{proposition}
    Si \( E\) et \( F\) sont des espaces vectoriels de dimension \( n\) et si \( \{ e_i \}_{i=1,\ldots, n}\) et \( \{ f_i \}_{i=1,\ldots, n}\) sont des bases respectivement de \( E\) et \( F\), alors il existe une unique application linéaire \( A\colon E\to F\) telle que \( E(e_i)=f_i\) pour tout \( i\).
\end{proposition}

\begin{proof}
    En deux parties.\begin{subproof}
        \item[Existence]
            Soit \( v\in E\). Vu que \( \{ e_i \}\) est une base, il se décompose de façon unique en \( v=\sum_iv_ie_i\). Alors définir
            \begin{equation}
                A(v)=\sum_iv_if_i
            \end{equation}
            est une bonne définition et satisfait aux exigences.
        \item[Unicité]
            Soient \( A\) et \( B\) satisfaisant aux exigences. Alors pour tout \( i\) nous avons \( A(e_i)=B(e_i)\). Si \( v\in E\) s'écrit de la forme \( v=\sum_iv_ie_i\) alors la linéarité impose \( A(v)=\sum_iv_iA(e_i)=\sum_iv_iB(e_i)=B(v)\). Donc \( A=B\).
    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Rang}
%---------------------------------------------------------------------------------------------------------------------------

La proposition~\ref{DefALUAooSPcmyK} et le théorème~\ref{ThoGkkffA} sont valables également en dimension infinie; ce sera une des rares incursions en dimension infinie de ce chapitre.
\begin{propositionDef}\label{DefALUAooSPcmyK}
    L'image d'une application linéaire est un espace vectoriel. La dimension de cet espace est le \defe{rang}{rang} de ladite application linéaire.
\end{propositionDef}

\begin{proof}
    Soit une application linéaire \( f\colon E\to F\). Nous considérons \( v,w\) dans l'image de \( f\) ainsi que \( \lambda\) dans le corps de base commun à \( E\) et \( F\).

    Soient \( v_0\in E\) et \( w_0\in E\) tels que \( v=f(v_0)\) et \( w=f(w_0)\). Alors \( v+w=f(v_0+w_9)\) et \( \lambda v=f(\lambda v_0)\). Donc l'image est bien un espace vectoriel.
\end{proof}

\begin{theorem}[Théorème du rang]       \label{ThoGkkffA}
    Soient \( E\) et \( F\) deux espaces vectoriels (de dimensions finies ou non) et soit \( f\colon E\to F\) une application linéaire. Alors le rang\footnote{Définition~\ref{DefALUAooSPcmyK}.} de \( f\) est égal à la codimension du noyau, c'est à dire
       \begin{equation}
           \rang(f)+\dim\ker f=\dim E.
       \end{equation}

       Dans le cas de dimension infinie afin d'éviter les problèmes d'arithmétique avec l'infini nous énonçons le théorème en disant que si \( (u_s)_{s\in S}\) est une base de \( \ker(f)\) et si \( \big( f(v_t) \big)_{t\in T}\) est une base de \( \Image(f)\) alors  \( (u_s)_{s\in s}\cup (v_t)_{t\in T}\) est une base de \( E\).
\end{theorem}
\index{théorème!du rang}

\begin{proof}
    Nous devons montrer que
    \begin{equation}
          (u_s)_{s\in S}\cup (v_t)_{t\in T}
    \end{equation}
    est libre et générateur.

    Soit \( x\in E\). Nous définissons les nombres \( x_t\) par la décomposition de \( f(x)\) dans la base \( \big( f(v_t) \big)\) :
    \begin{equation}
        f(x)=\sum_{t\in T}x_tf(v_t).
    \end{equation}
    Ensuite le vecteur \( x=\sum_tx_tv_t\) est dans le noyau de \( f\), par conséquent nous le décomposons dans la base \( (u_s)\) :
    \begin{equation}
        x-\sum_tx_tv_t=\sum_{s\in S} x_su_s.
    \end{equation}
    Par conséquent
    \begin{equation}
        x=\sum_sx_su_s+\sum_tx_tv_t.
    \end{equation}

    En ce qui concerne la liberté nous écrivons
    \begin{equation}
        \sum_tx_tv_t+\sum_sx_su_s=0.
    \end{equation}
    En appliquant \( f\) nous trouvons que
    \begin{equation}
        \sum_tx_tf(v_t)=0
    \end{equation}
    et donc que les \( x_t\) doivent être nuls. Nous restons avec \( \sum_sx_su_s=0\) qui à son tour implique que \( x_s=0\).
\end{proof}
Un exemple d'utilisation de ce théorème en dimension infinie sera donné dans le cadre du théorème de Fréchet-Riesz, théorème~\ref{ThoQgTovL}.

\begin{corollary}       \label{CORooCCXHooALmxKk}
    Si \( E\) est un espace vectoriel de dimension finie, alors un endomorphisme de \( E\) est bijectif si et seulement si il est injectif si et seulement si il est surjectif.
\end{corollary}

\begin{proof}
    Si un endomorphisme \( f\colon E\to E\) est surjectif, alors \( \rang(f)=\dim(E)\), ce qui donne, par le théorème du rang~\ref{ThoGkkffA}, \( \dim\big( \ker(f) \big)=0\), c'est à dire que \( f\) est injectif.

    De la même façon, si \( f\) est injective, alors \( \dim\big( \ker(f) \big)=0\), ce qui donne \( \rang(f)=\dim(E)\) ou encore que \( f\) est surjective.
\end{proof}

Une conséquence du théorème du rang est que les endomorphismes ont un inverse à gauche et à droite égaux (lorsqu'ils existent).
\begin{corollary}
    Soit un endomorphisme \( f\) d'un espace vectoriel de dimension finie. Si \( f\) admet un inverse à gauche, alors
    \begin{enumerate}
        \item
            \( f\) est bijective,
        \item
            \( f\) admet également un inverse à droite,
        \item
            ils sont égaux.
    \end{enumerate}
    Tout cela tient également en remplaçant «gauche» par «droite».
\end{corollary}

\begin{proof}
    Soit \( g\), un inverse à gauche de \( f\) : \( gf=\id\). Cela implique que \( f\) est injective et que \( g\) est surjective, et donc qu'elles sont toutes deux bijectives par le corollaire~\ref{CORooCCXHooALmxKk}. Vu que \( f\) est bijective, elle admet également un inverse à droite, soit \( h\). Nous avons : \( gf=\id\) et \( fh=\id\).

    Alors \( gfh=h\) parce que \( gf=\id\), mais également \( gfh=g\) parce que \( fh=\id\). Donc \( g=h\).
\end{proof}
C'est ce corollaire qui nous permet d'écrire \( f^{-1}\) sans plus de précisions dès que \( f\) est une bijection.

\begin{example}[Pas en dimension infinie]
    Tout cela ne fonctionne pas en dimension infinie. Par exemple avec une base \( \{ e_k \}_{k\in \eN}\) nous pouvons considérer l'opérateur
    \begin{equation}
        f(e_k)=e_{k+1}.
    \end{equation}
    Il est injectif, mais pas surjectif. Si on pose
    \begin{equation}
        g(e_k)=\begin{cases}
            e_{k-1}    &   \text{si } k\geq 1\\
            0    &    \text{si } k=0
        \end{cases}
    \end{equation}
    alors nous avons \( gf=\id\), mais pas \( fg=\id\) parce que ce \( (fg)(e_0)=0\).
\end{example}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Injection, surjection}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
  Une application $S: \eR^m\to\eR^n$ est dite \defe{affine}{affine (application)} si elle est la somme d'une application linéaire et d'une application constante. Autrement dit, $S$ est affine s'il existe $T: \eR^m\to\eR^n$, linéaire, telle que $S(x)-T(x)$ soit un vecteur constant dans $\eR^n$.
\end{definition}

\begin{example}
	Les exemples les plus courants d'applications affines sont les droites et les plans ne passant pas par l'origine.
	\begin{description}
		\item[Les droites] Une droite dans $\eR^2$ (ou $\eR^3$) qui ne passe pas par l'origine est le graphe d'une fonction de la forme $s(x)=ax+b$ (ou $s(t)=u x +v$, avec $u$ et $v$  dans $\eR^2$). On reconnaît ici la fonction de l'exemple~\ref{ex_affine}.

		\item[Les plans]
			De la même façon nous savons que tout plan qui ne passe pas par l'origine dans $\eR^3$ est le graphe d'une application affine, $P(x,y)= (a,b)^T\cdot(x,y)^T+(c,d)^T$.
	\end{description}
\end{example}

\begin{lemma}[\cite{ooEPEFooQiPESf}]        \label{LEMooDAACooElDsYb}
    Soit une application linéaire \( f\colon E\to F\).
    \begin{enumerate}
        \item       \label{ITEMooEZEWooZGoqsZ}
            L'application \( f\) est injective si et seulement s'il existe \( g\colon F\to E\) telle que \( g\circ f=\id|_E\).
        \item
            L'application \( f\) est surjective si et seulement s'il existe \( g\colon F\to E\) telle que \( f\circ g=\id|_F\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Nous démontrons séparément les deux affirmations.
    \begin{enumerate}
        \item
            Si \( f\) est injective, alors \( f\colon E\to \Image(f)\) est un isomorphisme. Si $V$ est un supplémentaire de \( \Image(f)\) dans \( F\) (c'est à dire \( F=\Image(f)\oplus V\)) alors nous pouvons poser \( g(x+v)=f^{-1}(x)\) où \( x+v\) est la décomposition (unique) d'un élément de \( F\) en \( x\in\Image(f)\) et \( v\in V\). Avec cela nous avons bien \( g\circ f=\id\).

            Inversement, s'il existe \( g\colon F\to E\) telle que \( g\circ f=\id\) alors \( f\colon E\to E\) doit être injective. Parce que si \( f(x)=0\) avec \( x\neq 0\) alors \( (g\circ f)(x)=0\neq x\).
        \item
            Si \( f\) est surjective nous pouvons choisir des éléments \( x_1,\ldots, x_p\) dans \( E\) tels que \( \{ f(x_i) \}\) soit une base de \( F\). Ensuite nous définissons
            \begin{equation}
                \begin{aligned}
                    g\colon F&\to E \\
                    \sum_ka_kf(x_k)&\mapsto \sum_kx_k.
                \end{aligned}
            \end{equation}
            Cela donne \(  f\circ g=\id|_F\) parce que si \( v\in F\) alors \( v=\sum_kv_kf(x_k)\) avec \( v_k\in \eK\), et nous avons
            \begin{equation}
                (f\circ g)(v)=\sum_kv_k(f\circ g)f(x_k)=f\left( \sum_kv_kx_k \right)=\sum_kf(x_k)=v.
            \end{equation}

            Inversement, s'il existe \( g\colon F\to E\) tel que \( f\circ g=\id\) alors \( f\) soit être surjective parce que
            \begin{equation}
                F=\Image(f\circ g)=f\big( \Image(g) \big)\subset \Image(f).
            \end{equation}
    \end{enumerate}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice d'une application linéaire}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PROPooCESFooGOZBNI}
 Toute application linéaire \( t\colon \eR^m\to \eR^n\) s'écrit de manière unique par rapport aux bases canoniques de $\eR^m$ et $\eR^n$ sous la forme
\[
t(x)=Ax,
\]
avec $A$ dans $\mathcal{M}_{n\times m}$.
\end{proposition}

\begin{proof}
  Soit $x$ un vecteur dans $\eR^m$. On peut écrire $x$ sous la forme $ x=\sum_{i=1}^{m}x_i e_i$. Comme $t$ est une application linéaire on a
\[
t(x)=\sum_{i=1}^{m}x_it(e_i).
\]
Les images de la base de $\eR^m$, $t(e_j), \, j=1,\ldots,m$, sont des éléments de $\eR^n$, donc on peut les écrire sous la forme de vecteurs
\[
t(e_i)=
\begin{pmatrix}
  a_{1i}\\
\vdots\\
a_{ni}
\end{pmatrix}.
\]
On obtient alors
\[
t(x)=\sum_{i=1}^{m}x_it(e_i)=\sum_{i=1}^{m}x_i\begin{pmatrix}
  a_{1i}\\
\vdots\\
a_{ni}
\end{pmatrix}=
\begin{pmatrix}
  a_{11} \ldots a_{1m}\\
\vdots \ddots \vdots\\
 a_{n1} \ldots a_{nm}\\
\end{pmatrix}
\begin{pmatrix}
  x_1\\
\vdots\\
x_m
\end{pmatrix}=Ax.
\]
\end{proof}

\begin{normaltext}
    Soient deux espaces vectoriels de dimension finie \( V,W\) sur le corps \( \eK\) ainsi qu'une application linéaire \( t\colon V\to W\). Nous considérons les bases\footnote{C'est le théorème~\ref{ThonmnWKs} qui nous permet de considérer des bases. Et ce théorème ne fonctionne que parce que nous avons supposé une dimension finie.} \( \{ e_i \}\) pour \( V\) et \( \{ f_{\alpha} \}\) pour \( W\). Ces bases ne sont pas supposées canoniques en aucun sens du terme. Les dimensions de \( V\) et \( W\) ne sont pas non plus supposées identiques.

    Nous définissons les nombres
    \begin{equation}
        A_{\alpha i}=t(e_i)_{\alpha}\in \eK.
    \end{equation}
    Vous noterez que les indices sont «à l'envers» par rapport au sens de l'application \( t\). Si \( x=\sum_ix_ie_i\) alors nous avons\footnote{Les sommes sur \( \alpha\) et sur \( i\) n'ont pas les mêmes bornes a priori.}
    \begin{equation}
        t(x)=\sum_ix_it(e_i)=\sum_{i\alpha}t(e_i)_{\alpha}f_{\alpha}=\sum_{i\alpha}A_{\alpha i}x_if_{\alpha}.
    \end{equation}
    Cela peut encore s'écrire
    \begin{equation}
        t(x)_{\alpha}=\sum_iA_{\alpha i}x_i=(Ax)_{\alpha}
    \end{equation}
    où, à droite, nous avons utilisé un abus de notations en notant \( x\) l'élément de \( \eK^n\) donnant les coordonnées de \( x\in V\) dans la base \( \{ e_i \}\). Avec encore plus d'abus de notations, nous pouvons également écrire
    \begin{equation}
        t(x)=Ax.
    \end{equation}
    Cette écriture n'est valable que si les bases de \( V\) et de \( W\) sont ultra-fixées. En effet, elle signifie que le vecteur \( t(x)\in W\) est le vecteur dont les composantes dans la base \( \{ f_{\alpha} \}\) sont données par le produit de la matrice \( A\) par l'élément de \( \eK^n\) dont les composantes sont celles de \( x\) dans la base \( \{ e_i \}\).
\end{normaltext}

\begin{normaltext}
    Dans le cas d'applications \( \eR^n\to \eR^m\) nous nous permettrons de considérer une application linéaire \( A\) et d'écrire
    \begin{equation}
        Ax=\sum_{ij}A_{ij}x_je_i
    \end{equation}
    lorsque les bases canoniques sont considérées des deux côtés.
\end{normaltext}

\begin{normaltext}
    Il convient de ne pas confondre matrice et application linéaires (bien que nous le ferons sans vergogne). Une matrice est un bête tableau de nombres, et nous savons déjà en calculer le déterminant, le dual, la trace, l'inverse et bien d'autres choses.

    Une application linéaire est une application linéaire. Nous savons comment associer, lorsque deux bases sont données, une matrice à une application linéaire. Mais nous n'avons encore pas vu quel est le lien entre les propriétés de la matrice et celles de l'application linéaire.
\end{normaltext}

%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces de polynômes}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecEspacePolynomes}

Attention : les polynômes en soi sont définis par la définition~\ref{DefRGOooGIVzkx}.

Pour chaque $k>0$ donné nous définissons
\begin{equation}
\mathcal{P}_\eR^k=\{p:\eR\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \cdots+a_k x^k, \, a_i\in\eR,\,\forall i=0,\ldots,k\}.
\end{equation}
Il est facile de se convaincre que la somme de deux polynômes de degré inférieur ou égal à $k$ est encore un polynôme de degré inférieur ou égal à $k$. En outre il est clair que la multiplication par un scalaire ne peut pas augmenter le degré d'un polynôme. L'ensemble $\mathcal{P}_\eR^k$ est donc un espace vectoriel muni des opérations héritées de $\mathcal{P}_{\eR}$.

La base canonique de l'espace $\mathcal{P}_\eR^k$ est donné par les monômes $\mathcal{B}=\{x\mapsto x^j \,|\, j=0, \ldots, k\}$. Le fait que cela soit une base est vraiment facile à démontrer et est un exercice très utile si vous ne l'avez pas encore vu dans un cours précédent.

Nous allons maintenant étudier trois applications linéaires de $\mathcal{P}_\eR^k$ vers des autres espaces vectoriels
\begin{description}
  \item[L'isomorphisme canonique  $\phi:\mathcal{P}_\eR^k \to\eR^{k+1}$] Nous définissons $\phi$ par les relations suivantes
\[
\phi(x^j)=e_{j+1}, \qquad \forall j\in\{0,\dots, k\}.
\]
Cela veut dire que pour tout $p$ dans $\mathcal{P}_\eR^k$, avec $p(x)=a_0+a_1 x +a_2 x^2 + \cdots+a_k x^k$, l'image de $p$ par $\phi$ est
\[
\phi(p)=\phi\left(\sum_{j=0}^k a_j x^j\right)=\sum_{j=0}^k a_j e_{j+1}.
\]
\begin{example} Soit $k=5$ on a
  \begin{equation}
    \phi(-8-7x-4x^2+4x^3+2x^5)=
  \begin{pmatrix}
    -8\\
    -7\\
    -4\\
    4\\
    0\\
    2
  \end{pmatrix}.
  \end{equation}
\end{example}

Cette application est clairement bijective et respecte les opérations d'espace vectoriel, donc elle est un isomorphisme d'espaces vectoriels. L'existence d'un isomorphisme entre $\mathcal{P}_\eR^k$  et $\eR^{k+1}$ est un cas particulier du théorème qui dit que  pour chaque $m$ dans $\eN_0$ fixée, tous les espaces vectoriels sur $\eR$ de dimension $m$ sont isomorphes à $\eR^m$. Vous connaissez peut être déjà ce théorème depuis votre cours d'algèbre linéaire.
    \item[La dérivation $d: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k-1}$] L'application de dérivation $d$ fait exactement ce qu'on s'attend d'elle
\[
d(x^0)=d(1)=0, \qquad d(x^j)=j x^{j-1}, \quad \forall j\in\{1,\dots, k\}.
\]
Cette application n'est pas injective, parce que l'image de $p$ ne dépend pas de la valeur de $a_0$, donc si deux polynômes sont les mêmes à une constante près ils auront la même image par $d$.

\begin{example} Soit $k=3$ on a
  \begin{equation}
    d(-8-12x+4x^3)= -12 (1) + 4 (3x^2) = -12+12 x^2.
    \end{equation}

    Noter que $d(-30-12x+4x^3)=d(-8-12x+4x^3)$. Cela confirme, comme mentionné plus haut, que la dérivée n'est pas injective.
\end{example}
      \item[L'intégration $I: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k+1}$] Nous pouvons définir une application que est <<à une constante prés>> l'application inverse de la dérivation. Cette application est définie sur les éléments de base par
          \begin{equation}
                I(x^j)= \frac{x^{j+1}}{j+1}.
          \end{equation}
          Bien entendu la raison d'être et la motivation de cette définition apparaîtra lorsque nous développerons une théorie générale de l'intégration.

\begin{example}
   Soit $k=4$ on a
  \begin{equation}
    I(6+2x+x^2+x^4)= 6x+x^2+\frac{x^3}{3}+\frac{x^5}{5}.
    \end{equation}
\end{example}

Remarquez que, étant donné que dans la définition de $I$ nous avons décidé d'intégrer entre zéro et $x$, tous les polynômes dans $\mathcal{P}_\eR^{k+1}$ qui sont l'image par $I$ d'un polynôme de $\mathcal{P}_\eR^{k}$ ont $a_0=0$. Cela veut dire que nous pouvons générer toute l'image de $I$ en utilisant un sous-ensemble de la base canonique de $\mathcal{P}_\eR^{k+1}$,  en particulier $\mathcal{B}_1=\{x\mapsto x^j \,|\, j=1, \ldots, k\}\subset \mathcal{B}$ nous suffira. Cela n'est guère surprenant, parce que l'image par une application linéaire d'un espace vectoriel de dimension finie ne peut pas être un espace de dimension supérieure.
\end{description}

Les applications de dérivation et intégration correspondent évidemment à des applications linéaires de $\mathcal{P}_\eR$ dans lui-même.

L'espace de tous les polynômes étant de dimension infinie, il peut servir de contre-exemple assez simple. Dans la sous-section~\ref{SubSecPOlynomesCE}, nous verrons que toutes les normes ne sont pas équivalentes sur l'espace des polynômes.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecTQkRXIu}

\begin{definition}      \label{DEFooEEQGooNiPjHz}
    Une \defe{forme bilinéaire}{forme!bilinéaire} sur un espace vectoriel \( E\) est une application \( b\colon E\times E\to \eK\) telle que
    \begin{enumerate}
        \item
            \( b(u,v)=b(v,u)\),
        \item
            \( b(u+v,w)=b(u,w)+b(v,w)\),
        \item
            \( b(\lambda u,v)=\lambda b(u,v)\)
    \end{enumerate}
    pour tout \( u,v,w\in E\) et \( \lambda\in \eK\) où \( \eK\) est un corps commutatif.
\end{definition}

\begin{normaltext}
    Une application bilinéaire \( E\times E\to \eK\) n'est pas une aplication linéaire; la distinction est importante. La linéarité est
    \begin{equation}
        b(\lambda u,\lambda v)= b\big( \lambda(u,v) \big)=\lambda b(u,v)
    \end{equation}
    et la bilinéarité est
    \begin{equation}
        b(\lambda u,v)=b(u,\lambda v)=\lambda b(u,v).
    \end{equation}
    En réalité la seule forme qui soit à la fois linéaire et bilinéaire est la forme identiquement nulle : la condition
    \begin{equation}
        b(\lambda u,\lambda v)=\lambda^2b(u,v)=\lambda b(u,v)
    \end{equation}
    pour tout \( \lambda\in \eK\) implique \( b(u,v)=0\).
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{RUAoonJAym}]   \label{DefBSIoouvuKR}
    Soit un espace vectoriel \( E\) et \( \eF\) un corps de caractéristique différente de \( 2\). Une \defe{forme quadratique}{forme!quadratique} sur \( E\) est une application \( q\colon V\to \eF\) pour laquelle il existe une forme bilinéaire symétrique \( b\colon V\times V\to \eF\) satisfaisant \( q(x)=b(x,x)\) pour tout \( x\in V\).

    L'ensemble des formes quadratiques réelles sur \( E\) est noté \( Q(E)\)\nomenclature[B]{\( Q(E)\)}{formes quadratiques réelles sur \( E\)}.
\end{definition}

\begin{lemma}       \label{LEMooLKNTooSfLSHt}
    Si \( q\) est une forme quadratique, il existe une unique forme bilinéaire \( b\) telle que \( q(x)=b(x,x)\).
\end{lemma}

\begin{proof}
    L'existence n'est pas en cause : c'est la définition d'une forme quadratique. Pour l'unicité, étant donné une forme quadratique, la forme bilinéaire \( b\) doit forcément vérifier l'\defe{identité de polarisation}{identité de polarisation} :
\begin{equation}    \label{EqMrbsop}
    b(x,y)=\frac{ 1 }{2}\big( q(x)+q(y)-q(x-y) \big).
\end{equation}
Elle est donc déterminée par \( q\).
\end{proof}
Notons la division par \( 2\) qui est le pourquoi de la demande de la caractéristique différente de \( 2\) pour \( \eF\) dans la définition de forme quadratique.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice associée à une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

Soit une forme bilinéaire \( q\colon V\times V\to \eK\) et une base \( \{ f_{\alpha} \}\) de \( V\), pas spécialement orthonormée. Nous définissons les nombres
\begin{equation}    \label{EQooCUGFooRlKUtu}
    B_{\alpha\beta}=q(f_{\alpha},f_{\beta}),
\end{equation}
qui forment une matrice symétrique dans \( \eM(n,\eK)\). Alors nous avons aussi, si \( y=\sum_{\alpha}y_{\alpha}f_{\alpha}\) et \( y'=\sum_{\beta}y'_{\beta}f_{\beta}\) :
\begin{equation}
    q(y,y')=\sum_{\alpha\beta}q(f_{\alpha},f_{\beta})=\sum_{\alpha\beta}B_{\alpha\beta}y_{\alpha}y'_{\beta}.
\end{equation}
La matrice \( B\) est la matrice de \( q\) dans la base \( \{ f_{\alpha} \}\) de \( V\).

Notons que la matrice associée à une forme bilinéaire (ou quadratique associée) est uniquement valable pour une base donnée. Si nous changeons de base, la matrice change. Cependant lorsque nous travaillons sur \( \eR^n\), la base canonique est tellement canonique que nous allons nous permettre de parler de «la» matrice associée à une forme bilinéaire.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Matrices et changements de bases}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous nous proposons à présent de fixer toutes les notations concernant le calcul matriciel et les changements de bases. Nous considérons un espace vectoriel \( V\) sur le corps commutatif\footnote{Comme toujours.} \( \eK\). Si \( t\colon V\to V \) est une application linéaire de matrice \( A\) dans la base \( \{ e_i \}_{i=1,\ldots, n}\) alors nous avons :
\begin{equation}
    A_{ij}=t(e_j)_i.
\end{equation}
Donc \( t(e_j)=\sum_i A_{ij}e_i\). Nous avons alors, pour \( x=\sum_ix_ie_i\) :
\begin{equation}
    t(x)=\sum_ix_it(e_i)=\sum_{ij}A_{ji}x_ie_j,
\end{equation}
ou encore, en changeant le nom des indices pour être plus joli :
\begin{equation}
    t(x)_i=\sum_jA_{ij}x_j.
\end{equation}
Ici \( t\) est une application \( V\to V\) et \( A\in \eM(n,\eK)\) est simplement un tableau de nombres sans prétentions d'être une application linéaire. Cependant, \( A\) peut être vu comme application linéaire \( \eK\to \eK\), et l'ensemble de nombres \( \{ x_i \}_{i=1,\ldots, n}\) peut être vu comme vecteur de \( \eK^n\). Dans ce contexte nous pouvons écrire
\begin{equation}
    t(x)_i=(Ax)_i.
\end{equation}
Cela signifie que si on identifie un vecteur au vecteur de ses composantes, l'application linéaire se réduit au produit «matrice fois vecteur» sur le corps de base.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Le changement de base}
%---------------------------------------------------------------------------------------------------------------------------

Soit un espace vectoriel \( V\) muni de deux bases \( \{ e_i\}_{i=1,\ldots, n}\) et \( \{ f_{\alpha}\}_{\alpha=1,\ldots, n}\). Les deux bases sont liées entre elles par
\begin{equation}        \label{EQooFRQRooSMsQQB}
    f_{\alpha}=\sum_iQ_{i\alpha}e_i.
\end{equation}
Ici \( Q\) n'est pas une application linéaire \( V\to V\) : \( Q\) est seulement un tableau de nombres, donnant les coordonnées des vecteurs \( f_{\alpha}\) dans la base de \( e_i\). Éventuellement \( Q\) peut être vu comme une application linéaire \( \eK^n\to \eK^n\).

Dans la suite nous nommerons \( Q^{-1}\) la matrice inverse de \( Q\). Inverse au sens des bêtes tableaux de nombres, sans interprétations en tant qu'application linéaire. De même pour \( Q^t\) qui est la transposée de \( Q\).

\begin{lemma}       \label{LEMooLXAHooPRyHaF}
    Soient des matrices \( A,B\in \eM(n,\eK)\). Si pour tout \( x,y\in \eK^n\) nous avons
    \begin{equation}
        \sum_{ij}A_{ij}x_iy_j=\sum_{ij}B_{ij}x_iy_j
    \end{equation}
    alors \( A=B\).
\end{lemma}

\begin{proof}
    Il suffit de choisir \( x_i=\delta_{ik}\) et \( y_j=\delta_{jl}\), et d'effectuer les sommes; par exemple
    \begin{equation}
        \sum_{ij}A_{ij}\delta_{ik}y_j=\sum_jA_{kj}y_j.
    \end{equation}
    Après avoir effectué toutes les sommes nous nous retrouvons avec \( A_{kl}=B_{kl}\), ce qui signifie \( A=B\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : vecteurs de base}
%---------------------------------------------------------------------------------------------------------------------------

Nous multiplions l'égalité \eqref{EQooFRQRooSMsQQB} par \( Q^{-1}_{\alpha j}\) et nous sommons sur \( \alpha\) :
\begin{equation}
    \sum_{\alpha}Q^{-1}_{\alpha j}f_{\alpha}=\sum_{i\alpha}(A_{i\alpha}Q^{-1}_{\alpha j})e_i=e_j.
\end{equation}
Donc :
\begin{equation}    \label{EQooZQPAooAbKAdg}
    e_i=\sum_{\alpha}Q^{-1}_{\alpha i}f_{\alpha}.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : coordonnées}
%---------------------------------------------------------------------------------------------------------------------------

Soit un vecteur \( x\in V\). Il peut être écrit dans les deux bases :
\begin{equation}
    x=\sum_ix_ie_i=\sum_{\alpha}y_{\alpha}f_{\alpha}.
\end{equation}
En remplaçant \( e_i\) par sa valeur \eqref{EQooZQPAooAbKAdg} nous avons l'égalité
\begin{equation}
    \sum_{i\alpha}x_iQ^{-1}_{\alpha i}f_{\alpha}=\sum_{\alpha}y_{\alpha}f_{\alpha}.
\end{equation}
Vu que les \( f_{\alpha}\) sont linéairement indépendants, l'égalité des sommes donne l'égalité de chacun de termes :
\begin{equation}        \label{EQooFXYLooCRmRdA}
    y_{\alpha}=\sum_ix_iQ^{-1}_{\alpha i}.
\end{equation}
En identifiant \( x\in V\) au vecteur dans \( \eK^n\) de ses coordonnées dans la base \( \{ e_i \}\) nous pouvons écrire
\begin{equation}
    y_{\alpha}=(Q^{-1}x)_{\alpha},
\end{equation}
ou pire :
\begin{equation}
    y=Q^{-1}x.
\end{equation}
Cette dernière égalité repose sur un petit paquet d'abus de notations qu'il convient de bien comprendre. Ici, \( x\) et \( y\) sont les éléments de \( \eK^n\) donnés par les composantes de \( x\) dans les bases \( \{ e_i \}\) et \( \{ f_{\alpha} \}\), et \( Q\) est vu comme une matrice, un opérateur linéaire sur \( \eK^n\).

Une chose agréable avec cette façon d'écrire est que nous trouvons tout de suite la transformation inverse \( x=Qy\) qui peut être écrire de différentes manières :
\begin{subequations}
    \begin{align}
        x&=Qy\\
        x_i&=(Qy)_i   \label{SUBEQooPVGBooDafCcBk}  \\
        x_i&=\sum_{\alpha}Q_{i\alpha}y_{\alpha}
    \end{align}
\end{subequations}
Attention à l'ordre des indices dans la dernière égalité : la matrice \( Q\) vient avec les indices dans l'ordre \( i\alpha\), tandis que la matrice \( Q^{-1}\) vient avec les indices dans l'ordre opposé : \( \alpha i\). C'est pour cela qu'il est intéressant de noter avec des lettres latines les indices se rapportant à la première base et avec des lettres grecques ceux se rapportant à la seconde base.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : matrice d'une application linéaire}
%---------------------------------------------------------------------------------------------------------------------------

Soit une application linéaire \( t\colon V\to V\) de matrices \( A\) et \( B\) dans les bases \( \{ e_i \}\) et \( \{ f_{\alpha} \}\) :
\begin{equation}
    t(x)=\sum_{ij}A_{ji}x_ie_j=\sum_{\alpha\beta}B_{\alpha\beta}y_{\beta}f_{\alpha}.
\end{equation}
En remplaçant \( e_j\) par son expression \eqref{EQooZQPAooAbKAdg} en termes des \( f_{\alpha}\) et \( x_i\) par son expression \eqref{SUBEQooPVGBooDafCcBk}, nous avons
\begin{subequations}
    \begin{align}
        (By)_{\alpha}&=\sum_{ij\alpha}A_{ji}(Qy)_iQ^{-1}_{\alpha j}f_{\alpha}\\
        &=\sum_{i \alpha}(Q^{-1}A)_{\alpha i}(Qy)_if_{\alpha}\\
        &=\sum_{\alpha}(Q^{-1} AQy)_{\alpha}f_{\alpha}.
    \end{align}
\end{subequations}
Vu que les \( f_{\alpha}\) forment une base nous en déduisons \( Q^{-1}AQy=By\). Et vu que \( y\) est un élément quelconque de \( \eK^n\), nous en déduisons l'égalité de matrices
\begin{equation}        \label{ooWKTYooOJfclT}
    B=Q^{-1}AQ.
\end{equation}
Il s'agit bien d'une égalité de matrices, ou à la limite d'applications linéaires sur \( \eK^n\), et non d'une égalité d'application linéaire sur \( V\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de base : matrice d'une forme bilinéaire}
%---------------------------------------------------------------------------------------------------------------------------

Soit une forme bilinéaire\footnote{Définition~\ref{DEFooEEQGooNiPjHz}} \( q\colon V\times V\to \eK\) dont la matrice\footnote{Définition~\ref{EQooCUGFooRlKUtu}.} dans la base \( \{ e_i \}\) est \( A\) et celle dans la base \( \{ f_{\alpha} \}\) est \( B\).

Rien n'indique pour l'instant que \( A\) et \( B\) sont les mêmes qu'avant. Au contraire, nous allons voir qu'elles ne sont en général pas les mêmes.

Soit \( x,x'\in V\) de coordonnées \( (x_i)\) et \( (x'_i)\) dans la base \( \{ e_i \}\) et \( (y_{\alpha})\), \( (y'_{\alpha})\) dans la base \( \{ f_{\alpha} \}\). Par définition de la matrice associée à une forme bilinéaire,
\begin{equation}
    q(x,x')=\sum_{ij}A_{ij}x_ix'_j=\sum_{\alpha\beta}B_{\alpha\beta}y_{\alpha}y'_{\beta}.
\end{equation}
En remplaçant les \( x_i\) et \( x'_i\) par leurs valeurs en fonction de \( y_{\alpha}\) et \( y'_{\beta}\),
\begin{subequations}
    \begin{align}
        q(x,x')&=\sum_{ij\alpha\beta}A_{ij}Q_{i\alpha}y_{\alpha}Q_{j\beta}y'_{\beta}\\
        &=\sum_{\alpha\beta}(Q^tAQ)_{\alpha\beta}y_{\alpha}y'_{\beta}
    \end{align}
\end{subequations}
où \( Q^t\) désigne la transposée de la matrice \( Q\) :  \( Q^t_{ij}=Q_{ji}\). Vu que les nombres \( y_{\alpha}\) et \( y'_{\beta}\) sont arbitraires nous déduisons\footnote{Lemme~\ref{LEMooLXAHooPRyHaF}.}
\begin{equation}        \label{EQooZUVTooKjqnJj}
    B=Q^tAQ.
\end{equation}

\begin{remark}
    Notons que cette «loi de transformation» n'est pas la même que celle pour une application linéaire. Ici nous avons \( Q^t\) alors que pour les applications linéaires nous avions \( Q^{-1}\).

    Pour cette raison, tant que nous travaillons avec des bases orthonormées, c'est à dire tant que \( Q\) est orthogonale, nous pouvons confondre une application linéaire avec une application bilinéaire en passant par la matrice. Mais cette identification n'est pas du tout canonique : elle repose sur le fait que les bases soient orthonormées.

    Il en découle que la réduction des endomorphismes et la réduction des formes bilinéaires ne sont pas tout à fait les mêmes théories. Par exemple la pseudo-diagonalisation simultanée (corollaire~\ref{CorNHKnLVA}) est un résultat de réduction de forme bilinéaire et non d'endomorphismes.
    % TODO : lorsque ce sera fait, il faudra approfondir cette remarque.
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Dualité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{proposition} \label{PropEJBZooTNFPRj}
    Si $A$ est la matrice d'une application linéaire, alors le rang de cette application linéaire est égal à la taille de la plus grande matrice carrée de déterminant non nul contenue dans $A$.
\end{proposition}

\begin{definition}  \label{DefJPGSHpn}
    Si \( E\) est un espace vectoriel sur \( \eK\), le \defe{dual algébrique}{dual algébrique} de \( E\), noté \( E^*\), l'ensemble des formes linéaires sur \( E\).
\end{definition}

Nous verrons plus tard qu'en dimension infinie, les applications linéaires ne sont pas toujours continue. Nous définirons donc aussi un concept de dual topologique. Voir la proposition~\ref{PROPooQZYVooYJVlBd}, la remarque~\ref{RemOAXNooSMTDuN} et la définition~\ref{DEFooKSDFooGIBtrG}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Orthogonal}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DEFooEQSMooHVzbfz}
    Soit \( E\), un espace vectoriel, et \( F\) une sous-espace de \( E\). L'\defe{orthogonal}{orthogonal!sous-espace} de \( F\) est la partie \( F^{\perp}\subset E^*\) donnée par
    \begin{equation}    \label{Eqiiyple}
        F^{\perp}=\{ \alpha\in E^*\tq \forall x\in F,\alpha(x)=0 \}.
    \end{equation}
\end{definition}

Cette définition d'orthogonal via le dual n'est pas du pur snobisme. En effet, la définition «usuelle» qui ne parle pas de dual,
\begin{equation}
    F^{\perp}=\{ y\in E\tq \forall x\in F,y\cdot x=0 \},
\end{equation}
demande la donnée d'un produit scalaire. Évidemment dans le cas de \( \eR^n\) munie du produit scalaire usuel et de l'identification usuelle entre \( \eR^n\) et \( (\eR^n)^*\) via une base, les deux notions d'orthogonal coïncident.

La définition~\ref{DEFooEQSMooHVzbfz}, au contraire, est intrinsèque : elle ne dépend que de la structure d'espace vectoriel.

Si \( B\subset E^*\), on note \( B^o\)\nomenclature[G]{\( B^o\)}{orthogonal dans le dual} son orthogonal :
\begin{equation}
    B^o=\{ x\in E\tq \omega(x)=0\,\forall \omega\in B \}.
\end{equation}
Notons qu'on le note \( B^o\) et non \( B^{\perp}\) parce qu'on veut un peu s'abstraire du fait que \( (E^*)^*=E\). Du coup on impose que \( B\) soit dans un dual et on prend une notation précise pour dire qu'on remonte au pré-dual et non qu'on va au dual du dual.


\begin{proposition} \label{PropXrTDIi}
    Soit \( E\) un espace vectoriel, et \( F\) un sous-espace vectoriel. Alors nous avons
    \begin{equation}
        \dim F+\dim F^{\perp}=\dim E.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \{ e_1,\ldots, e_p \}\) une base de \( F\) que nous complétons en une base \( \{ e_1,\ldots, e_n \}\) de \( E\) par le théorème~\ref{ThonmnWKs}. Soit \( \{ e_1^*,\ldots, e^*_n \}\) la base duale. Alors nous prouvons que \( \{ e^*_{p+1},\ldots, e_n^* \}\) est une base de \( F^{\perp}\).

    Déjà c'est une partie libre en tant que partie d'une base.

    Ensuite ce sont des éléments de \( F^{\perp}\) parce que si \( i\leq p\) et si \( k\geq 1\), nous avons \( e^*_{p+k}(e_i^*)=0\); donc oui, \( e^*_{p+k}\in F^{\perp}\).

    Enfin \( F^{\perp}\subset\Span\{ e_{p+1}^*,e_n^* \}\) parce que si \( \omega=\sum_{k=1}^n\omega_ke_k^*\), alors \( \omega(e_i)=\omega_i\), mais nous savons que si \( \omega\in F^{\perp}\), alors \( \omega(e_i)=0\) pour \( i\leq p\). Donc \( \omega=\sum_{k=p+1}^n\omega_ke^*_k\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée : pas d'approche naïve}
%---------------------------------------------------------------------------------------------------------------------------

Il est légitime, si \( t\colon V\to V\) est une application linéaire, de dire que sa transposée soit l'application linéaire \( t^t\colon V\to V\) dont la matrice est la matrice transposée de celle de \( t\). Lorsque nous travaillons sur \( \eR^n\) muni de la base canonique, cela ne pose pas de problèmes et nous pouvons écrire des égalités du type \( \langle x, Ay\rangle =\langle A^tx, y\rangle \).

Hélas nous allons voir que cette façon de définir une transposée est mauvaise.

Soit une application linéaire \( t\colon V\to V\) de matrice \( A\) dans la base \( \{ e_i \}_{i=1,\ldots, n}\) et de matrice \( B\) dans la base \( \{ f_{\alpha} \}_{\alpha=1,\ldots, n}\).

Nous nommons \( t_1\) l'application linéaire associée à \( A^t\) dans la base \( \{ e_i \}\) et \( t_2\) l'application linéaire associée à la matrice \( B^t\) dans la base \( \{ f_{\alpha} \}\). Définir la transposée d'une application linéaire comme étant l'application linéaire associée à la transposée de sa matrice ne sera une bonne définition que si \( t_1=t_2\).

La première chose facile à voir est
\begin{equation}        \label{EQooAMHPooUQEkJo}
    t_1(e_i)_j=\sum_k(A^t)_{jk}(e_i)_k=A^t_{ji}=A_{ij}.
\end{equation}
Pour calculer \( t_2(e_i)_j\), c'est un peu plus laborieux :
\begin{subequations}
    \begin{align}
        t_2(e_i)&=\sum_{\alpha}Q_{\alpha i}^{-1} t_2(f_\alpha)=\sum_{\beta\gamma\alpha}Q_{\alpha i}^{-1}B^t_{\gamma\beta}\underbrace{(t_{\alpha})_{\beta}}_{\delta_{\alpha\beta}}f_{\gamma}=\sum_{\beta\gamma}Q_{\beta i}^{-1}B^t_{\gamma\beta}f_{\gamma}\\
        &=(B^tQ^{-1})_{\gamma i}Q_{j\gamma}e_j\\
        &=\sum_j(QB^tQ^{-1})_{ji}e_j.
    \end{align}
\end{subequations}
Donc \( t_2(e_i)_j=(QB^tQ^{-1})_{ji}\). En tenant compte du fait que \( B=Q^{-1}AQ\) nous avons
\begin{equation}
    t_2(e_i)_j=(QQ^tA^t(Q^{-1})^tQ^{-1})_{ji}.
\end{equation}
Cela est égal à l'expression \eqref{EQooAMHPooUQEkJo} lorsque \( Q^t=Q^{-1}\). Nous voyons que confondre transposée d'une application linéaire avec transposée de la matrice associée n'est valable que si nous sommes certain de ne considérer que des changements de base par des matrices orthogonales.

Cela est la situation typique dans laquelle nous nous trouvons lorsque nous considérons des applications linéaires sur \( \eR^n\) muni de la base canonique et que nous n'avons aucune intention de changer de base, et encore moins de chercher une base non orthonormale. Cette situation est clairement la situation la plus courante.

\begin{example}[\cite{ooLIOMooBuCPUS}]
    Soit la base canonique \( \{ e_1,e_2 \}\) de \( \eR^2\). Nous considérons l'application linéaire \( t\colon \eR^2\to \eR^2\) définie par
    \begin{subequations}
        \begin{align}
            t(e_1)&=e_1\\
            t(e_2)&=0.
        \end{align}
    \end{subequations}
    La matrice de \( t\) dans cette base est
    \begin{equation}
        A=\begin{pmatrix}
            1    &   0    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
    Elle est symétrique : elle vérifie \( A^t=A\). Si nous comptions sur la transposée de matrice pour définir la transposée de \( t\), nous aurions \( t^t=t\).

    Soit maintenant la base \( f_1=e_1\), \( f_2=e_1+e_2\). Nous avons \( t(f_1)=f_1\) et
    \begin{equation}
        t(f_2)=t(e_1)+t(e_2)=e_1=f_1.
    \end{equation}
    Donc la matrice de \( t\) dans cette base est
    \begin{equation}
        B=\begin{pmatrix}
            1    &   1    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
    Et là, nous avons \( B^t\neq B\). Donc en comptant sur cette base pour définir la transposée de \( t\) nous aurions \( t^t\neq t\).
\end{example}

\begin{normaltext}      \label{NooMZVRooExWVKJ}
    Autrement dit, la façon «usuelle» de voir la transposée d'une application linéaire ne fonctionne dans les livres pour enfant uniquement parce qu'on n'y considère toujours \( \eR^n\) muni de la base canonique ou de bases orthonormées.

    Notons que nous avons tout de même les notions d'opérateur adjoint et autoadjoint pour parler d'application orthogonale sans passer par la transposée, voir~\ref{DEFooYKCSooURQDoS}.
\end{normaltext}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}      \label{DefooZLPAooKTITdd}
    Si \( f\colon E\to F\) est une application linéaire entre deux espaces vectoriels, la \defe{transposée}{transposée} est l'application \( f^t\colon F^*\to E^*\) donnée par
    \begin{equation}
        f^t(\omega)(x)=\omega\big( f(x) \big).
    \end{equation}
    pour tout \( \omega\in F^*\) et \( x\in E\).
\end{definition}

\begin{lemma}
    Soit \( E\) muni de la base \( \{ e_i \}\) et \( F\) muni de la base \( \{ g_i \}\) et une application \( f\colon E\to F\). Si \( A\) est la matrice de \( f\) dans ces bases, alors \( A^t\) est la matrice de \( f^t\) dans les bases \( \{ e^*_i \}\) et \( \{ g^*_i \}\) de \( E^*\) et \( F^*\).
\end{lemma}

\begin{proof}
    Nous allons montrer que les formes \( f^t(g^*_i)\) et \( \sum_k(A^t)_{ik}g^*_k\) sont égales en les appliquant à un vecteur.

    Par définition de la matrice d'une application linéaire dans une base,
    \begin{equation}
        f^t(g_i^*)=\sum_j(f^t)_{ij}e^*_j,
    \end{equation}
    et
    \begin{equation}
        f(e_k)=\sum_lA_{kl}g_l.
    \end{equation}
    Du coup, si \( x=\sum_kx_ke_k\), nous avons
    \begin{equation}    \label{EqCzwftH}
        f^t(g_i^*)x=\sum_{kl}x_kg_i^*A_{kl}g_l=\sum_{kl}x_kA_{kl}\delta_{il}=\sum_k x_kA_{ki}=\sum_k(A^t)_{ik}x_k.
    \end{equation}
    D'autre part,
    \begin{equation}    \label{EqWlQlrR}
        \sum_k(A^t)_{ik}g_k^*x=\sum_{kl}(A^t)_{ik}g^*_kx_le_l=\sum_k(A^t)_{ik}x_k.
    \end{equation}
    Le fait que \eqref{EqCzwftH} et \eqref{EqWlQlrR} donnent le même résultat prouve le lemme.
\end{proof}
En corollaire, les rangs de \( f\) et de \( f^t\) sont égaux parce que le rang est donné par la plus grande matrice carrée de déterminant non nul. Nous prouvons cependant ce résultat de façon plus intrinsèque.

\begin{lemma}[\href{http://gilles.dubois10.free.fr/algebre_lineaire/dualite.html}{Gilles Dubois}]   \label{LemSEpTcW}
    Si \( f\colon E\to F\) est une application linéaire, alors
    \begin{equation}
        \rang(f)=\rang(f^t).
    \end{equation}
\end{lemma}

\begin{proof}
    Nous posons \( \dim\ker(f)=p\) et donc \( \rang(f)=n-p\). Soit \( \{ e_1,\ldots, e_p \}\) une base de \( \ker(f)\) que l'on complète en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Nous considérons maintenant les vecteurs
    \begin{equation}
        g_i=f(e_{p+i})
    \end{equation}
    pour \( i=1,\ldots, n-p\). C'est à dire que les \( g_i\) sont les images des vecteurs qui ne sont pas dans le noyau de \( f\). Prouvons qu'ils forment une famille libre. Si
    \begin{equation}
        \sum_{k=1}^{n-p}a_kf(e_{p+k})=0,
    \end{equation}
    alors \( f\big( \sum_ka_ke_{p+k} \big)=0\), ce qui signifierait que \( \sum_ka_ke_{p+k}\) se trouve dans le noyau de \( f\), ce qui est impossible par construction de la base \( \{ e_i \}_{i=1,\ldots, n}\). Étant donné que les vecteurs \( g_1,\ldots, g_{n-p}\) sont libres, nous les complétons en une base
    \begin{equation}
        \{ \underbrace{g_1,\ldots, g_{n-p}}_{\text{images}},\underbrace{g_{n-p+1},\ldots, g_r}_{\text{complétion}} \}
    \end{equation}
    de \( F\).

    Nous prouvons maintenant que \( \rang(f^t)\geq n-p\) en montrant que les formes \( \{ g_i^* \}_{i=1,\ldots, n-p}\) forment une partie libre (et donc l'espace image de \( f^t\) est au moins de dimension \( n-p\)). Pour cela nous prouvons que \( f^t(g_i^*)=e^*_{i+p}\). En effet
    \begin{equation}
        f^t(g^*_i)e_k=g_i^*(fe_k),
    \end{equation}
    Si \( k=1,\ldots, p\), alors \( fe_k=0\) et donc \( g_i^*(fe_k)=0\); si \( k=p+l\) alors
    \begin{equation}
        f^t(g_i^*)e_k=g_i^*(fe_{k+l})=g^*_i(g_l)=\delta_{i,l}=\delta_{i,k-p}=\delta_{k,i+p}.
    \end{equation}
    Donc \( f^t(g_i^*)=e^*_{i+p}\). Cela prouve que les formes \( f^t(g_i^*)\) sont libres et donc que
    \begin{equation}
        \rang(f^t)\geq n-p=\rang(f).
    \end{equation}
    En appliquant le même raisonnement à \( f^t\) au lieu de \( f\), nous trouvons
    \begin{equation}
        \rang\big( (f^t)^t \big)\geq \rang(f^t)
    \end{equation}
    et donc, vu que \( (f^t)^t=f\), nous obtenons \( \rang(f)=\rang(f^t)\).

\end{proof}

\begin{proposition}[\cite{DualMarcSAge}]        \label{PropWOPIooBHFDdP}
    Si \( f\) est une application linéaire entre les espaces vectoriels \( E\) et \( F\), alors nous avons
    \begin{equation}
        \Image(f^t)=\ker(f)^{\perp}.
    \end{equation}
\end{proposition}

\begin{proof}
    Soient donc l'application \( f\colon E\to F\) et sa transposée \( f^t\colon F^*\to E^*\). Nous commençons par prouver que \( \Image(f^{t})\subset(\ker f)^{\perp}\). Pour cela nous prenons \( \omega\in \Image(f^t)\), c'est à dire \( \omega=\alpha\circ f\) pour un certain élément \( \alpha\in F^*\). Si \( z\in\ker(f)\), alors \( \omega(z)=(\alpha\circ f)(z)=0\), c'est à dire que \( \omega\in (\ker f)^{\perp}\).

    Pour prouver qu'il y a égalité, nous n'allons pas démontrer l'inclusion inverse, mais plutôt prouver que les dimensions sont égales. Après, on sait que si \( A\subset B\) et si \( \dim A=\dim B\), alors \( A=B\). Nous avons
    \begin{subequations}
        \begin{align}
            \dim\big( \Image(f^t) \big)&=\rang(f^t)\\
            &=\rang(f)  &\text{lemme~\ref{LemSEpTcW}}\\
            &=\dim(E)-\dim\ker(f)   &\text{théorème~\ref{ThoGkkffA}}\\
            &=\dim\big( (\ker f)^{\perp} \big)  &\text{proposition~\ref{PropXrTDIi}}.
        \end{align}
    \end{subequations}
\end{proof}

\begin{lemma}[\cite{ooEPEFooQiPESf}]
    Soit \( \eK\) un corps, \( E\) et \( F\) deux \( \eK\)-espaces vectoriels de dimension finie et une application linéaire \( f\colon E\to F\). L'application \( f\) est injective si et seulement si sa transposée\footnote{Définition~\ref{DefooZLPAooKTITdd}.} \( f^t\) est surjective.
\end{lemma}

\begin{proof}
    Supposons que \( f\) soit injective. Alors par le lemme~\ref{LEMooDAACooElDsYb}, il existe \( g\colon F\to E\) tel que \( g\circ f=\id|_E\). Nous avons alors aussi \( (g\circ f)^t=\id|_{E^*}\), mais \( (g\circ f)^t=f^t\circ g^t\), donc \( f^t\) est surjective.

    Inversement, nous supposons que \( f^t\colon F^*\to E^*\) est surjective. Alors en nous souvenant que \( E\) et \( F\) sont de dimension finie et en faisant jouer les identifications \( (f^t)^t=f\) et \( (E^*)^*=E\) nous savons qu'il existe \( s\colon E^*\to F^*\) tel que \( f^t\circ s=\id|_{E^*}\). En passant à la transposée,
    \begin{equation}
        s^t\circ f=\id|_{E},
    \end{equation}
    qui implique que \( f\) est injective.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes de Lagrange}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E=\eR_n[X]\) l'ensemble des polynômes à coefficients réels de degré au plus \( n\). Soient les \( n+1\) réels distincts \( a_0,\ldots, a_n\). Nous considérons les formes linéaires associées \( f_i\in E^*\),
\begin{equation}
    f_i(P)=P(a_i).
\end{equation}
\begin{lemma}
    Ces formes forment une base de \( E^*\).
\end{lemma}

\begin{proof}
    Nous prouvons que l'orthogonal est réduit au nul :
    \begin{equation}
        \Span\{ f_0,\ldots, f_n \}^{\perp}=\{ 0 \}
    \end{equation}
    pour que la proposition~\ref{PropXrTDIi} conclue. Si \( P\in\Span\{ f_i \}^{\perp}\), alors \( f_i(P)=0\) pour tout \( i\), ce qui fait que \( P(a_i)=0\) pour tout \( i=0,\ldots, n\). Un polynôme de degré au plus \( n\) qui s'annule en \( n+1\) points est automatiquement le polynôme nul.
\end{proof}

Les \defe{polynômes de Lagrange}{Lagrange!polynôme}\index{polynôme!Lagrange} sont les polynômes de la base (pré)duale de la base \( \{ f_i \}\).

\begin{proposition}
    Les polynômes de Lagrange sont donnés par
    \begin{equation}
        P_i=\prod_{k\neq i}\frac{ X-a_k }{ a_i-a_k }.
    \end{equation}
\end{proposition}

\begin{proof}
    Il suffit de vérifier que \( f_j(P_i)=\delta_{ij}\). Nous avons
    \begin{equation}
        f_j(P_i)=P_i(a_j)=\prod_{k\neq i}\frac{ a_j-a_k }{ a_i-a_k }.
    \end{equation}
    Si \( j\neq i\) alors un des termes est nul. Si au contraire \( i=j\), tous les termes valent \( 1\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dual de \texorpdfstring{$ \eM(n,\eK)$}{M(n,K)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{KXjFWKA}]     \label{PropHOjJpCa}
    Soit \( \eK\), un corps. Les formes linéaires sur \( \eM(n,\eK)\) sont les applications de la forme
    \begin{equation}
        \begin{aligned}
            f_A\colon \eM_n(\eK)&\to \eK \\
            M&\mapsto \tr(AM).
        \end{aligned}
    \end{equation}
\end{proposition}
\index{trace!dual de \( \eM(n,\eK)\)}
\index{dual!de \( \eM(n,\eK)\)}


\begin{proof}
    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eK)&\to \eM(n,\eK)' \\
            A&\mapsto f_A
        \end{aligned}
    \end{equation}
    et nous voulons prouver que c'est une bijection. Étant donné que nous sommes en dimension finie, nous avons égalité des dimensions de \( \eM_n(\eK)\) et \( \eM_n(\eK)'\), et il suffit de prouver que \( f\) est injective. Soit donc \( A\) telle que \( f_A=0\). Nous l'appliquons à la matrice \( (E_{ij})_{kl}=\delta_{ik}\delta_{jl}\) :
    \begin{equation}
            0=f_A(E_{ij})
            =\sum_{k}(AE_{ij})_{kk}
            =\sum_{kl}A_{kl}\delta_{il}\delta_{jk}
            =A_{ij}.
    \end{equation}
    Donc \( A=0\).
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]
    Soit \( \eK\) un corps et \( \phi\in\eM(n,\eK)^*\) telle que pour tout \( X,Y\in \eM(n,\eK)\) on ait
    \begin{equation}
        \phi(XY)=\phi(YX).
    \end{equation}
    Alors il existe \( \lambda\in \eK\) tel que \( \phi=\lambda\Tr\).
\end{corollary}
\index{trace!unicité pour la propriété de trace}

\begin{proof}
    La proposition~\ref{PropHOjJpCa} nous donne une matrice \( A\in \eM(n,\eK)\) telle que \( \phi=f_A\). L'hypothèse nous dit que \( f_A(XY)=f_A(YX)\), c'est à dire
    \begin{equation}
        \Tr(AXY)=\Tr(AYX)
    \end{equation}
    pour toute matrices \( X,Y\in \eM(n,\eK)\). L'invariance cyclique de la trace nous dit que \( \Tr(AXY)=\Tr(XAY)\), ce qui signifie que
    \begin{equation}
        \Tr\big( (AX-XA)Y \big)=0
    \end{equation}
    ou encore que \( f_{AX-XA}=0\) pour tout \( X\). La fonction \( f\) étant injective nous en déduisons que la matrice \( A\) doit satisfaire
    \begin{equation}
        AX=XA
    \end{equation}
    pour tout \( X\in\eM\). En particulier en prenant la fameuse matrice \( E_{ij}\) et en calculant un peu,
    \begin{equation}
        A_{li}\delta_{jm}=\delta_{il}A_{jm}
    \end{equation}
    pour tout \( i,j,l,m\). Cela implique que \( A_{ll}=A_{mm}\) pour tout \( l\) et \( m\) et que \( A_{jm}=0\) dès que \( j\neq m\). Il existe donc \( \lambda\in \eK\) tel que \( A=\lambda\mtu\). En fin de compte,
    \begin{equation}
        \phi(X)=f_{\lambda\mtu}(X)=\lambda\Tr(X).
    \end{equation}
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]       \label{CorICUOooPsZQrg}
    Soit \( \eK\) un corps. Tout hyperplan de \( \eM(n,\eK)\) coupe \( \GL(n,\eK)\).
\end{corollary}
\index{groupe!linéaire!hyperplan}

\begin{proof}
    Soit \( \mH\) un hyperplan de \( \eM\). Il existe une forme linéaire \( \phi\) sur \( \eM(n,\eK)\) telle que \( \mH=\ker(\phi)\). Encore une fois la proposition~\ref{PropHOjJpCa} nous donne \( A\in \eM\) telle que \( \phi=f_A\); nous notons \( r\) le rang de \( A\). Par le lemme~\ref{LemZMxxnfM} nous avons \( A=PJ_rQ\) avec \( P,Q\in \GL(n,\eK)\) et
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r    &   0    \\
            0    &   0
        \end{pmatrix}.
    \end{equation}
    Pour tout \( X\in \eM\) nous avons
    \begin{equation}
        \phi(X)=\Tr(AX)=\Tr(PJ_rQX)=\Tr(J_rQXP).
    \end{equation}
    Ce que nous cherchons est \( X\in \GL(n,\eK)\) telle que \( \phi(X)=0\). Nous commençons par trouver \( Y\in\GL(n,\eK)\) telle que \( \Tr(J_rY)=0\). Celle-là est facile : c'est
    \begin{equation}
        Y=\begin{pmatrix}
            0    &   1    \\
            \mtu_{n-1}    &   0
        \end{pmatrix}.
    \end{equation}
    Les éléments diagonaux de \( J_rY\) sont tous nuls. Par conséquent en posant \( X=Q^{-1}YP^{-1}\) nous avons notre matrice inversible dans le noyau de \( \phi\).
\end{proof}
\index{hyperplan!de \( \eM(n,\eK)\)}
