% This is part of Mes notes de mathématique
% Copyright (c) 2011-2017
%   Laurent Claessens, Carlotta Donadello
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Localisation des valeurs propres}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Sur l'ensemble \( \eM(n,\eR)\) des matrices \( n\times n\) à coefficients réels nous introduisons l'ordre partiel\footnote{Définition \ref{DEFooVGYQooUhUZGr}.} donné par \( A\geq B\) lorsque \( A_{ij}\geq B_{ij}\) pour tout \( i\) et \( j\). Nous définissons de façon similaire les relations \( A\leq B\), \( A<B\) et \( A>B\).

Si \( x\in \eR^n\) nous notons \( | x |=\big( | x_1 |,\ldots, | x_n | \big)\) et \( x\leq y\) lorsque \( x_i\leq y_i\) pour tout \( i\).

\begin{proposition}
    Soit \( A\in \eM(n,\eR)\) et \( x,y\in \eR^n\).
    \begin{enumerate}
        \item
            Si \( A\geq 0\) et si \( x\leq y\) alors \( Ax\leq Ay\).
        \item
            Si \( A\geq 0\) alors \( Ax\leq | Ax |\leq A| x |\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Pour la première inégalité, pour tout \( i\) et \( k\) nous avons \( A_{ik}x_k\leq A_{ik}y_k\) et donc
    \begin{equation}
        (Ax)_i=\sum_kA_{ik}x_k\leq \sum_kA_{ik}y_k=(Ay)_k.
    \end{equation}
    
    Pour la seconde, d'abord l'inégalité \( Ax\leq | Ax |\) est évidente. Ensuite vu que \( A_{ik}\geq 0\) nous avons
    \begin{equation}
        | Ax |_i=| \sum_kA_{ik}x_k |\leq \sum_kA_{ik}| x_k |=\big( A| x | \big)_i.
    \end{equation}
\end{proof}

\begin{definition}
    Une matrice \( A\in \eM(n,\eR)\) est une \defe{M-matrice}{M-matrice} si
    \begin{enumerate}
        \item
            \( A_{ii}>0\) pour tout \( i\),
        \item
            \( A_{ij}\leq 0\) si \( i\neq j\)
        \item
            \( A\) est inversible et \( A^{-1}\geq 0\).
    \end{enumerate}
\end{definition}

\begin{definition}
    Soit \( A\in \eM(n,\eR)\) et \( i,j\in\{ 1,\ldots, n \}\). Nous disons que les indices \( i\) et \( j\) sont \defe{directement connectés}{connectés!indices d'une matrice} si \( A_{ij}\neq 0\) ou \( A_{ji}\neq 0\).
\end{definition}

\begin{definition}
    Soit \( A\in \eM(n,\eR)\) et \( i,j\in\{ 1,\ldots, n \}\). Nous disons que les indices \( i\) et \( j\) sont \defe{connectés}{connectés!indices d'une matrice} si il existe un ensemble d'indices \( i_0=i,i_1,\ldots, i_{r-1},i_r=j   \) tels que \( A_{i_k,i_{k+1}}\neq 0\) pour tout \( 0\leq k\leq r\).
\end{definition}

Par exemple pour que les indices \( 1\) et \( 4\) soient connectés, on peut avoir les éléments \( A_{13}\), \( A_{32}\) \( A_{24}\) non nuls.

\begin{definition}[\cite{ooZGNYooGgPFhl}]      \label{DEFooXIREooQtlzkO}
    Une matrice carré \( A\) est \defe{réductible}{matrice!réductible} si il existe une permutation \( \sigma\) telle que 
    \begin{equation}        \label{EQooGGZKooUyXSJk}
        \sigma^tA\sigma=\begin{pmatrix}
            K    &   L    \\ 
            0    &   M    
        \end{pmatrix}
    \end{equation}
    où \( K\) et \( M\) sont carrées.
\end{definition}
Notons que par définition des matrices d'applications linéaires,
\begin{equation}
    B_{ij}= \langle e_i, Be_j\rangle =\langle e_i, \sigma^tA\sigma e_j\rangle =\langle \sigma e_i, A\sigma e_j\rangle =A_{\sigma(i),\sigma(j)}.
\end{equation}

\begin{proposition}[\cite{ooZGNYooGgPFhl}]      \label{PROPooZTYDooZAxQxF}
    Soit une matrice carré \( A\). Les faits suivants sont équivalents :
    \begin{enumerate}
        \item       \label{ITEMooYULAooVqgOnt}
            \( A\) est réductible.
        \item       \label{ITEMooNLVXooYSQKwO}
            Il existe une partition non triviale \( I,J\) de \( \{ 1,\ldots, n \}\) telle que \( I\cup J=\{ 1,\ldots, n \}\), \( I\cap J=\emptyset\) et pour tout \( i\in I\), et pour tout \( j\in J\), \( A_{ij}=0\).
        \item       \label{ITEMooVNOHooRUNpwG}
            La matrice \( A\) admet des indices non connectés.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Dans plusieurs sens\ldots
    \begin{subproof}
        \item[\ref{ITEMooYULAooVqgOnt} implique \ref{ITEMooNLVXooYSQKwO}]
        
            Nous notons \( j^*\) la taille de la matrice \( K\) dans \eqref{EQooGGZKooUyXSJk}. Nous avons \(  B_{ij}=0  \) si
            \begin{subequations}
                \begin{numcases}{}
                    J^*+1\leq i\leq n\\
                    1\leq j\leq j^*.
                \end{numcases}
            \end{subequations}
            Donc en posant \( I=\sigma\{ j^*+1,\ldots, n \}\) et \( J=\sigma\{ 1,\ldots, j^* \}\) nous avons une partition non triviale de \( \{ 1,\ldots, n \}\) telle que si \( i\in I\) et \( j\in J\) alors \( i=\sigma(i_0)\), \( j=\sigma(j_0)\) et
            \begin{equation}
                A_{ij}=A_{\sigma(i_0),\sigma(j_0)}=B_{i_0,j_0}=0.
            \end{equation}
        \item[\ref{ITEMooNLVXooYSQKwO} implique \ref{ITEMooYULAooVqgOnt}]
            Soit une partition \( I,J\) comme indiquée dans l'hypothèse. Soit \( j^*\) le nombre d'éléments dans \( J\). Soit \( \sigma\) une permutation de \( \{ 1,\ldots, n \}\) telle que \( \sigma\{ j^*+1,\ldots, n \}=I\) et \( \sigma\{ 1,\ldots, j^* \}=J\). Nous posons ensuite \( B=\sigma^tA\sigma\). Par construction si \( i\in I\) et \( j\in J\) alors \( A_{ij}=0\).

            Mais si
            \begin{subequations}
                \begin{numcases}{}
                    J^*+1\leq i\leq n\\
                    1\leq j\leq j^*.
                \end{numcases}
            \end{subequations}
            alors \( B_{ij}=A_{\sigma(i)\sigma(j)}=0\). Donc \( B\) a la bonne forme.

        \item[\ref{ITEMooVNOHooRUNpwG} implique \ref{ITEMooNLVXooYSQKwO}]

            Soient \( i\) et \( j\) deux indices non connectés : il n'existe pas de chaînes partant de \( i\) et arrivant à \( j\). Nous notons \( I \) l'ensemble des indices connectés à \( i\), et \( J\) les autres. Pat hypothèses ces ensembles sont non vides.

            Si \( k\in i\) et \( l\in J\) alors \( A_{kl}=0\) parce que sinon on aurait une chaine de \( i\) à \( k\) puis de \( k\) à \( l\) et donc de \( i\) à \( l\), ce qui signifierait que \( l\) est connecté à \( i\).

        \item[\ref{ITEMooNLVXooYSQKwO} implique \ref{ITEMooVNOHooRUNpwG}]
            Soit une partition \( I,J\) comme dans l'hypothèse. Si \( j\in J\) est connecté à \( i\in I\) alors il existe une chaîne
            \begin{equation}
                i=i_0,i_1,\ldots, i_r=j.
            \end{equation}
            Si \( i_s\) est le premier dans \( J\) alors \( i_{s-1}\in I\) et \( A_{i_{s-1},i_s}=0\), ce qui empêche la chaîne de connecter \( j\) à \( i\).
    \end{subproof}
\end{proof}

Soit une matrice \( A\in\eM(n,\eR)\). Nous notons
\begin{equation}
    r_i=\sum_{j\neq i}| A_{ij} |.
\end{equation}
Notons la somme sur la ligne \( i\), pas sur la colonne : la somme est horizontale. 
\begin{definition}
    Les ensembles
    \begin{equation}
        D_i=\{ z\in \eC\tq | z-A_{ii} |\leq r_i \}
    \end{equation}
    sont les \defe{disques de Gershgorin}{Gershgorin!disque}. Nous allons également noter \( B_i=\Int(D_i)\) les boules ouvertes correspondantes.
\end{definition}

\begin{theorem}[Gershgorin]     \label{THOooUJNFooHpvCCF}
    Soit \( A\in \eM(n,\eR)\). Si \( \lambda\in \eC\) est valeur propre de \( A\) alors \(   \lambda\in D_i   \) pour un certain \( i\).
\end{theorem}

\begin{proof}
    Soit une valeur propre \( \lambda\) et un de ses vecteurs propres \( u\in \eR^n\) : \( Au=\lambda u\) avec \( u\neq 0\). Soit \( i\) un indice réalisant le maximum \( | u_i |=\max\{ | u_k | \}_k\). Nous écrivons la \( i\)\ieme ligne de \( Au=\lambda u\) :
    \begin{equation}
        \sum_kA_{ik}u_k=\lambda u_i,
    \end{equation}
    c'est à dire \( A_{ii}u_i+\sum_{k\neq i}A_{ik}u_k=\lambda u_i\), ou encore
    \begin{equation}
        A_{ii}+\sum_{k\neq i}A_{ik}\frac{ u_k }{ u_i }=\lambda,
    \end{equation}
    qui donne
    \begin{equation}
        | A_{ii} -\lambda|\leq \sum_{k\neq i}| A_{ik} |\frac{ | u_k | }{ | u_i | }\leq \sum_{k\neq i}| A_{ik} |
    \end{equation}
    pare que \( | u_i |\geq | u_k |\). Notons que sur la ligne précédente, \( | . |\) est le module dans \( \eC\), pas la valeur absolue dans \( \eR\).
\end{proof}

\begin{theorem}[Gershgorin 2\cite{ooZGNYooGgPFhl}]      \label{THOooTXAPooQqsBCj}
    Soit une matrice irréductible \( A\in \eM(n,\eR)\) et une valeur propre \( \lambda\) de \( A\). Si elle est sur la frontière de l'union des disques de Gershgorin, alors elle est sur le bord de tous les disques.
\end{theorem}

\begin{proof}
    Soit une valeur propre \( \lambda\) de \( A\) telle que \( \lambda\in \partial\big( \bigcup_iD_i \big)\). Alors \( \lambda\) n'est dans l'intérieur d'aucune boule et nous avons \( | \lambda-A_{ii} |\geq r_i\) pour tout \( i\).

    Soit un vecteur propre \( u\) de \( A\) tel que \( \| u \|_{\infty}=1\). Nous posons \( I= \{ 1\leq i\leq n \tq | u_i |=1 \}  \) et \( J=\{ 1\leq j\leq n\tq | i_j |<1 \}\). Par hypothèse \( I\) n'est pas vide, et de plus \( I\cap J=\emptyset\) et \( I\cup J=\{ 1,\ldots, n \}\) parce qu'aucune composante de \( u\) n'a un module\footnote{Les composantes de \( u\) sont a priori dans \( \eC\), et non spécialement dans \( \eR\), même si \( A\) est une matrice réelle.} plus grand que \( 1\).

    La \( i\)\ieme composante de la relation \( Au=\lambda u\) peut s'écrire
    \begin{equation}
        (A_{ii}-\lambda)u_i+\sum_{k\neq i}A_{ik}u_k=0.
    \end{equation}
    Forts de cela nous écrivons les inégalités suivantes :
    \begin{equation}
        r_i\leq | \lambda-A_{ii} |=\big| (\lambda-A_{ii})u_i \big|=| \sum_{k\neq i}A_{ik}u_k |\leq \sum_{k\neq i}| A_{ik} | |u_k |\leq \sum_{k\neq i}| A_{ik} |=r_i.
    \end{equation}
    Donc les inégalités sont des égalités :
    \begin{equation}        \label{EQooBIBJooFlscrx}
        r_i= | \lambda-A_{ii} |=\big| (\lambda-A_{ii})u_i \big|=| \sum_{k\neq i}A_{ik}u_k |=\sum_{k\neq i}| A_{ik} | |u_k |=\sum_{k\neq i}| A_{ik} |.
    \end{equation}
    En particulier l'égalité \( \sum_{k\neq i}| A_{ik} | |u_k |=\sum_{k\neq i}| A_{ik} |\) donne
    \begin{equation}        
        \sum_{k\neq i}| A_{ik} |\big( | u_k |-1 \big)=0.
    \end{equation}
    Donc pour tout \( k\in J\) nous avons \( A_{ik}=0\). Vu que \( A\) est irréductible, cela donnerait une partition impossible \( \{ 1,\ldots, n \}=I\cup J\). Nous en déduisons que \( J\) est vide et donc que \( | u_j |=1\) pour tout $j$. En repartant de \eqref{EQooBIBJooFlscrx} nous avons alors
    \begin{equation}
        r_i=\big| (\lambda-A_{ii})u_i \big|=| \lambda-A_{ii} | |u_i |=| \lambda-A_{ii} |.
    \end{equation}
    Cela prouve que \( \lambda\in\partial D_i\) pour tout \( i\).
\end{proof}

\begin{example}     \label{EXooUKQIooQqteHx}

    Soit la matrice
    \begin{equation}
        B=\begin{pmatrix}
            2    &   0    &   1    \\
            0    &   1    &   -1/2    \\
            -1    &   0    &   3
        \end{pmatrix}.
    \end{equation}
    
    D'abord nous rappelons que si vous voulez entrer cette matrice dans Sage (ou plus généralement dans Python2\footnote{Que vous n'avez aucune raison d'utiliser autre que Sage.}), vous devez faire attention au \( 1/2\) qui, tel quel, est évalué à \( 0\). Nous vous rappelons donc que tous vos codes Sage doivent commencer par ceci :

\lstinputlisting{tex/sage/sageSnip015.sage}
    
    Les éléments non nuls hors diagonale sont \( B_{13}\), \( B_{31}\) et \( B_{23}\). Elle n'est donc pas irréductible; nous avons par exemple la partition \( I=\{ 1,3 \}\), \( J=\{ 2 \}\) pour le critère de la proposition \ref{PROPooZTYDooZAxQxF}\ref{ITEMooNLVXooYSQKwO}. 

    Les disques de Gershgorin sont
    \begin{subequations}
        \begin{align}
            D_1=\{ z\in \eC\tq | z-2 |\leq 1 \}\\
            D_2=\{ z\in \eC\tq | z-1 |\leq 1/2 \}\\
            D_3=\{ z\in \eC\tq | z-3 |\leq 1 \}
        \end{align}
    \end{subequations}

    Les valeurs propres de la matrice sont sur des bords de disques de Gershgorin, sans être sur tous les bords, comme ça aurait été le cas par le théorème \ref{THOooTXAPooQqsBCj} si la matrice avait été irréductible. Elles sont sur la figure \ref{LabelFigDNRRooJWRHgOCw}; notez en particulier les valeurs propres \( \lambda_2\) et \( \lambda_3\) qui sont sur le bord de deux disques mais pas sur le bord des trois disques en même temps.

\newcommand{\CaptionFigDNRRooJWRHgOCw}{Les disques de Gershgorin et les valeurs propres pour l'exemple \ref{EXooUKQIooQqteHx}.}
\input{auto/pictures_tex/Fig_DNRRooJWRHgOCw.pstricks}

\end{example}

\begin{example}     \label{EXooDQYDooPxqHjZ}
    Soit la matrice
    \begin{equation}
        A=\begin{pmatrix}
            0    &   -1    &   0    \\
            0    &   1    &   2    \\
            3    &   0    &   2
        \end{pmatrix}.
    \end{equation}
    Nous avons
    \begin{subequations}
        \begin{align}
            D_1&=\{ z\in \eC\tq | z |\leq 1 \},\\
            D_2&=\{ z\in \eC\tq | z-1 |\leq 2 \},\\
            D_3&=\{ z\in \eC\tq | z-2 |\leq 3 \},
        \end{align}
    \end{subequations}
    Le polynôme caractéristique est
    \begin{equation}
        \chi(\lambda)=-\lambda^3+3\lambda^2-2\lambda-6.
    \end{equation}
    Une fois remarqué que \( \lambda_1=-1\) est une racine, les autres sont faciles à trouver (division euclidienne de \( \chi(\lambda)\) par \( \lambda+1\)) : \( \lambda_2=2+i\sqrt{2}\) et \( \lambda_3=2-i\sqrt{ 2 }\).

    La matrice \( A\) est irréductible. En effet les éléments non diagonaux non nuls sont \( A_{12}\), \( A_{23}\) et \( A_{31}\). Ils peuvent former une chaîne reliant tous les indices entre eux. 

    Les contraintes sur la localisation des valeurs propres est donc qu'elles doivent être dans ou sur les disques de Gershgorin, mais que celles qui sont sur le bord d'un disque doivent être sur le bord de tous les disques en même temps. C'est cela que nous observons sur la figure \ref{LabelFigDNHRooqGtffLkd}. Notez en particulier la position de la valeur propre \( \lambda_1\).


    \newcommand{\CaptionFigDNHRooqGtffLkd}{Les disques de Gershgorin et les valeurs propres pour l'exemple \ref{EXooDQYDooPxqHjZ}.}
   \input{auto/pictures_tex/Fig_DNHRooqGtffLkd.pstricks}

\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Sous espaces caractéristiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

% TODO : lire le blog de Pierre Bernard; en particulier celle-ci : http://allken-bernard.org/pierre/weblog/?p=2299

Lorsqu'un opérateur n'est pas diagonalisable, les valeurs propres jouent quand même un rôle important.

\begin{definition}  \label{DefFBNIooCGbIix}
    Soit \( E\) un \( \eK\)-espace vectoriel  \( f\in\End(E)\). Pour \( \lambda\in \eK\) nous définissons
    \begin{equation}
        F_{\lambda}(f)=\{ v\in E\tq (f-\lambda\mtu)^nv=0, n\in\eN \}
    \end{equation}
    et nous appelons ça un \defe{sous-espace caractéristique}{sous-espace!caractéristique} de \( f\).
\end{definition}
L'espace \( F_{\lambda}(f)\) est l'ensemble de nilpotence de l'opérateur \( f-\lambda\mtu\) et

\begin{lemma}   \label{LemBLPooHMAoyJ}
    L'ensemble \( F_{\lambda}(f)\) est non vide si et seulement si \( \lambda\) est une valeur propre de \( f\). L'espace \( F_{\lambda}(f)\) est invariant sous \( f\).
\end{lemma}

\begin{proof}
    Si \( F_{\lambda}(f)\) est non vide, nous considérons \( v\in F_{\lambda}(f)\) et \( n\) le plus petit entier non nul tel que \( (f-\lambda)^nv=0\). Alors \( (f-\lambda)^{n-1}v\) est un vecteur propre de \( f\) pour la valeur propre \( \lambda\). Inversement si \( v\) est une valeur propre de \( f\) pour la valeur propre \( \lambda\), alors \( v\in F_{\lambda}(f)\).

    En ce qui concerne l'invariance, remarquons que \( f\) commute avec \( f-\lambda\mtu\). Si \( x\in F_{\lambda}(f)\) il existe \( n\) tel que \( (f-\lambda\mtu)^nx=0\). Nous avons aussi
    \begin{equation}
        (f-\lambda\mtu)^nf(x)=f\big( (f-\lambda\mtu)^nx \big)=0,
    \end{equation}
    par conséquent \( f(x)\in F_{\lambda}(f)\).
\end{proof}

\begin{remark}  \label{RemBOGooCLMwyb}
    Toute matrice sur \( \eC\) n'est pas diagonalisable : nous en avons déjà donné une exemple simple en \ref{ExBRXUooIlUnSx}. Nous en voyons maintenant un moins simple. Considérons en effet l'endomorphisme \( f\) donné par la matrice
    \begin{equation}
        \begin{pmatrix}
            a&    \alpha    &   \beta    \\
            0    &   a    &   \gamma    \\
            0    &   0    &   b
        \end{pmatrix}
    \end{equation}
    où \( a\neq b\), \( \alpha\neq 0\), \( \beta\) et \( \gamma\) sont des nombres complexes quelconques.
    Son polynôme caractéristique est 
    \begin{equation}
        \chi_f(\lambda)=(a-\lambda)^2(b-\lambda),
    \end{equation}
    et les valeurs propres sont donc \( a\) et \( b\). Nous trouvons les vecteurs propres pour la valeur \( a\) en résolvant
    \begin{equation}
        \begin{pmatrix}
            a    &   \alpha    &   \beta    \\
            0    &   a    &   \gamma    \\
            0    &   0    &   b
        \end{pmatrix}\begin{pmatrix}
            x    \\ 
            y    \\ 
            z    
        \end{pmatrix}=\begin{pmatrix}
            ax    \\ 
            ay    \\ 
            az    
        \end{pmatrix}.
    \end{equation}
    L'espace propre \( E_a(f)\) est réduit à une seule dimension générée par \( (1,0,0)\). De la même façon l'espace propre correspondant à la valeur propre \( b\) est donné par 
    \begin{equation}
        \begin{pmatrix}
            \frac{1}{ b-a }\left( \beta+\frac{ \alpha\gamma }{ b-a } \right)    \\ 
            \frac{ \gamma }{ b-a }    \\ 
            1    
        \end{pmatrix}.
    \end{equation}
    Il n'y a donc pas trois vecteurs propres linéairement indépendants, et l'opérateur \( f\) n'est pas diagonalisable.

    Par contre nous pouvons voir que
    \begin{equation}
        (f-\alpha\mtu)^2\begin{pmatrix}
             0   \\ 
            1    \\ 
            0    
        \end{pmatrix}=
        \begin{pmatrix}
            a    &   \alpha    &   \beta    \\
            0    &   a    &   \gamma    \\
            0    &   0    &   b
        \end{pmatrix}
        \begin{pmatrix}
            \alpha    \\ 
            0    \\ 
            0    
        \end{pmatrix}-\begin{pmatrix}
            a\alpha    \\ 
            0    \\ 
            0    
        \end{pmatrix}=\begin{pmatrix}
            0    \\ 
            0    \\ 
            0    
        \end{pmatrix},
    \end{equation}
    de telle sorte que le vecteur \( (0,1,0)\) soit également dans l'espace caractéristique \( F_a(f)\).

    Dans cet exemple, la multiplicité algébrique de la racine \( a\) du polynôme caractéristique vaut \( 2\) tandis que sa multiplicité géométrique vaut seulement \( 1\).
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorèmes de décomposition}
%---------------------------------------------------------------------------------------------------------------------------

%TODO : Je crois qu'on peut remplacer l'hypothèse de corps algébriquement clos par le polynôme caractéristique scindé.
\begin{theorem}[Théorème spectral, décomposition primaire]\index{théorème!spectral}     \label{ThoSpectraluRMLok}
    Soit \( E\) espace vectoriel de dimension finie sur le corps algébriquement clos \( \eK\) et \( f\in\End(E)\). Alors
    \begin{equation}    \label{EqCTFHooBSGhYK}
        E=F_{\lambda_1}(f)\oplus\ldots\oplus F_{\lambda_k}(f)
    \end{equation}
    où la somme est sur les valeurs propres distinctes de \( f\).

    Les projecteurs sur les espaces caractéristique forment un système complet et orthogonal.
\end{theorem}
\index{décomposition!primaire}
\index{décomposition!spectrale}
\index{décomposition!sous-espaces caractéristiques}

\begin{proof}
    Soit \( P\) le polynôme caractéristique de \( f\) et une décomposition
    \begin{equation}
        P=(f-\lambda_1)^{\alpha_1}\ldots(f-\lambda_r)^{\alpha_r}
    \end{equation}
    en facteurs irréductibles. La le théorème de noyaux (\ref{ThoDecompNoyayzzMWod}) nous avons
    \begin{equation}        \label{EqDeFVSaYv}
        E=\ker(f-\lambda_1)^{\alpha_1}\oplus\ldots\oplus\ker(f-\lambda_r)^{\alpha_r}.
    \end{equation}
    Les projecteurs sont des polynômes en \( f\) et forment un système orthogonal. Il nous reste à prouver que \( \ker(f-\lambda_i)^{\alpha_i}=F_{\lambda_i}(f)\). L'inclusion
    \begin{equation}    \label{EqzmNxPi}
        \ker(f-\lambda_i)^{\alpha_i}\subset F_{\lambda_i}(f)
    \end{equation}
    est évidente. Nous devons montrer l'inclusion inverse. Prouvons que la somme des \( F_{\lambda_i}(f)\) est directe. Si \( v\in F_{\lambda_i}(f)\cap F_{\lambda_j}(f)\), alors il existe \( v_1=(f-\lambda_i)^nv\neq 0\) avec \( (f-\lambda_i)v_1=0\). Étant donné que \( (f-\lambda_i)\) commute avec \( (f-\lambda_j)\), ce \( v_1\) est encore dans \( F_{\lambda_j}(f)\) et par conséquent il existe \( w=(f-\lambda_j)^mv_1\) non nul tel que 
    \begin{subequations}
        \begin{numcases}{}
            (f-\lambda_i)w=0\\
            (f-\lambda_j)w=0.
        \end{numcases}
    \end{subequations}
    Ce \( w\) serait donc un vecteur propre simultané pour les valeurs propres \( \lambda_i\) et \( \lambda_j\), ce qui est impossible parce que les espaces propres sont linéairement indépendants. Les espaces \( F_{\lambda_i}\) sont donc en somme directe et
    \begin{equation}
        \sum_i\dim F_{\lambda_i}(f)\leq \dim E.
    \end{equation}
    En tenant compte de l'inclusion \eqref{EqzmNxPi} nous avons même
    \begin{equation}
        \dim E=\sum_i\dim\ker(f-\lambda_i)^{\alpha_i}\leq\sum_i F_{\lambda_i}(f)\leq \dim E.
    \end{equation}
    Par conséquent nous avons \( \dim\ker(f-\lambda_i)^{\alpha_i}=\dim F_{\lambda_i}(f)\) et l'égalité des deux espaces.
\end{proof}


\begin{probleme}
    Dans le cas où le corps n'est pas algébriquement clos, il paraît qu'il faut remplacer «diagonalisable» par «semi-simple».
\end{probleme}
%TODO : peut-être qu'il y a la réponse dans http://www.math.jussieu.fr/~romagny/agreg/dvt/endom_semi_simples.pdf

Si l'espace vectoriel est sur un corps algébriquement clos, alors les endomorphismes semi-simples\footnote{Définition \ref{DEFooBOHVooSOopJN}.} sont les endomorphismes diagonaux.


%TODO : Je crois qu'on peut remplacer l'hypothèse de corps algébriquement clos par le polynôme caractéristique scindé.
\begin{theorem}[Décomposition de Dunford] \label{ThoRURcpW}
    Soit \( E\) un espace vectoriel sur le corps algébriquement clos \( \eK\) et \( u\in\End(E)\) un endomorphisme de \( E\). 
    
    \begin{enumerate}
        \item
            
            L'endomorphisme \( u\) se décompose de façon unique sous la forme
            \begin{equation}
                u=s+n
            \end{equation}
            où \( s\) est diagonalisable, \( n\) est nilpotent et \( [s,n]=0\).
        \item
            Les endomorphismes \( s\) et \( n\) sont des polynômes en \( u\) et commutent avec \( u\).
        \item   \label{ItemThoRURcpWiii}
            Les parties \( s\) et \( n\) sont données par
            \begin{subequations}
                \begin{align}
                    s&=\sum_i\lambda_ip_i\\
                    n&=\sum_i(s-\lambda_i\mtu)p_i
                \end{align}
            \end{subequations}
            où les sommes sont sur les valeurs propres distinctes\footnote{C'est à dire sur les sous-espaces caractéristiques.} de \( f\) et où \( p_i\colon E\to F_{\lambda_i}(u)\) est la projection de \( E\) sur \( F_{\lambda_i}(u)\).
    \end{enumerate}
\end{theorem}
\index{décomposition!Dunford}
\index{Dunford!décomposition}
\index{réduction!d'endomorphisme}
\index{endomorphisme!sous-espace stable}
\index{polynôme!d'endomorphisme!décomposition de Dunford}
\index{endomorphisme!diagonalisable!Dunford}
\index{endomorphisme!nilpotent!Dunford}
%TODO : comprendre comment on calcule des exponentielles de matrices avec Dunford.

\begin{proof}
    Le théorème spectral \ref{ThoSpectraluRMLok} nous indique que
    \begin{equation}
        E=\bigoplus_iF_{\lambda_i}(f).
    \end{equation}
    Nous considérons l'endomorphisme \( s\) de \( E\) qui consiste à dilater d'un facteur \( \lambda\) l'espace caractéristique \( F_{\lambda}(f)\) :
    \begin{equation}
        s=\sum_i\lambda_ip_i
    \end{equation}
    où \( p_i\colon E\to F_{\lambda_i}(u)\) est la projection de \( E\) sur \( F_{\lambda_i}(u)\).

    Nous allons prouver que \( [s,f]=0\) et \( n=f-s\) est nilpotent. Cela impliquera que \( [s,n]=0\).

    Si \( x\in F_{\lambda}(f)\), alors nous avons \( sf(x)=\lambda f(x)\) parce que \( f(x)\in F_{\lambda}(f)\) tandis que \( fs(x)=f(\lambda x)=\lambda f(x)\). Par conséquent \( f\) commute avec \( s\).

    Pour montrer que \( f-s\) est nilpotent, nous en considérons la restriction
    \begin{equation}
        f-s\colon F_{\lambda}(f)\to F_{\lambda}(f).
    \end{equation}
    Cet opérateur est égal à \( f-\lambda\mtu\) et est par conséquent nilpotent.

    Prouvons à présent l'unicité. Soit \( u=s'+n'\) une autre décomposition qui satisfait aux conditions : \( s'\) est diagonalisable, \( n'\) est nilpotent et \( [n',s']=0\). Commençons par prouver que \( s'\) et \( n'\) commutent avec \( u\). En multipliant \( u=s'+n'\) par \( s'\) nous avons
    \begin{equation}
        s'u=s'^2+s'n'=s'^2+n's'=(s'+n')s'=us',
    \end{equation}
    par conséquent \( [u,s']=0\). Nous faisons la même chose avec \( n'\) pour trouver \( [u,n']=0\). Notons que pour obtenir ce résultat nous avons utilisé le fait que \( n'\) et \( s'\) commutent, mais pas leur propriétés de nilpotence et de diagonalisibilité.
    
    
    Si \( s'+n'=s+n\) est une autre décomposition, \( s'\) et \( n'\) commutent avec \( u\), et par conséquent avec tous les polynômes en \( u\). Ils commutent en particulier avec \( n\) et \( s\). Les endomorphismes \( s\) et \( s'\) sont alors deux endomorphismes diagonalisables qui commutent. Par la proposition \ref{PropGqhAMei}, ils sont simultanément diagonalisables. Dans la base de simultanée diagonalisation, la matrice de l'opérateur \( s'-s=n-n'\) est donc diagonale. Mais \( n-n'\) est également nilpotent, en effet si \( A\) et \( B\) sont deux opérateurs nilpotents,
    \begin{equation}
        (A+B)^n=\sum_{k=0}^n\binom{k}{n}A^kB^{n-k}.
    \end{equation}
    Si \( n\) est assez grand, au moins un parmi \( A^k\) ou \( B^{n-k}\) est nul.

    Maintenant que \( n-n'\) est diagonal et nilpotent, il est nul et \( n=n'\). Nous avons alors immédiatement aussi \( s=s'\).

\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Diverses conséquences}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}
    Soit une matrice \( A\in \eM(n,\eC)\). On a que la suite \( (A^kx)\) tends vers zéro pour tout \( x\) si et seulement si \( \rho(A)<1\) où \( \rho(A)\)\index{rayon!spectral} est le rayon spectral de $A$
\end{theorem}
\index{décomposition!Dunford!exponentielle de matrice}

\begin{proof}
    Dans le sens direct, il suffit de prendre comme \( x\), un vecteur propre de \( A\). Dans ce cas nous avons \( A^kx=\lambda^kx\). Mais \( \lambda^kx\) ne tend vers zéro que si \( \lambda<1\). Donc toute les valeurs propres de \( A\) doivent être plus petite que \( 1\) et \( \rho(A)<1\).

    Pour l'autre sens nous utilisons la décomposition de Dunford (théorème \ref{ThoRURcpW}) : il existe une matrice inversible \( P\) telle que
    \begin{equation}
        A=P^{-1}(D+N)P
    \end{equation}
    où \( D\) est diagonale, \( N\) est nilpotente et \( [D,N]=0\). Étant donné que \( D+N\) est triangulaire, son polynôme caractéristique que
    \begin{equation}
        \chi_{D+N}(\lambda)=\prod_i D_{ii}-\lambda.
    \end{equation}
    Par similitude, c'est le même polynôme caractéristique que celui de \( A\) et nous savons alors que la diagonale de \( D\) contient les valeurs propres de \( A\).

    Par ailleurs nous avons
    \begin{subequations}
        \begin{align}
            A^k&=P^{-1}(D+N)^kP\\
            &=P^{-1}\sum_{j=0}^k{j\choose k}D^{j-k}N^jP\\
            &=P^{-1}\sum_{j=0}^{n-1}{j\choose k}D^{j-k}N^jP
        \end{align}
    \end{subequations}
    où nous avons utilité le fait que \( D\) et \( N\) commutent ainsi que \( N^{n-1}=0\) parce que \( N\) est nilpotente. Nous utilisons la norme matricielle usuelle, pour laquelle \( \| D \|=\rho(D)=\rho(A)\). Nous avons alors
    \begin{equation}
        \| (D+N)^k \|\leq \sum_{j=0}^k{j\choose k}\rho(D)^{k-j}\| N \|^j.
    \end{equation}
    Du coup si \( \rho(D)<1\) alors \( \| (D+N)^k \|\to 0\) (et c'est même un si et seulement si).
\end{proof}

Une application de la décomposition de Jordan est l'existence d'un logarithme pour les matrices. La proposition suivant va d'une certaine manière donner un logarithme pour les matrices inversibles complexes. Dans le cas des matrices réelles \( m\) telles que \( \| m-\mtu \|<1\), nous donnerons au lemme \ref{LemQZIQxaB} une formule pour le logarithme sous forme d'une série; ce logarithme sera réel.
\begin{proposition} \label{PropKKdmnkD}
    Toute matrice inversible complexe est une exponentielle.
\end{proposition}
\index{exponentielle!de matrice}
\index{décomposition!Jordan!et exponentielle de matrice}

\begin{proof}
    Soit \( A\in \GL(n,\eC)\); nous allons donner une matrice \( B\in \eM(n,\eC)\) telle que \( A=\exp(B)\). D'abord remarquons qu'il suffit de prouver le résultat pour une matrice par classe de similitude. En effet si \( A=\exp(B)\) et si \( M\) est inversible alors 
    \begin{subequations}    \label{EqqACuGK}
        \begin{align}
            \exp(MBM^{-1})&=\sum_k\frac{1}{ k! }(MBM^{-1})^k\\
            &=\sum_k\frac{1}{ k! }MB^kM^{-1}\\
            &=M\exp(B)M^{-1}.
        \end{align}
    \end{subequations}
    Donc \( MAM^{-1}=\exp(MBM^{-1})\). Nous pouvons donc nous contenter de trouver un logarithme pour les blocs de Jordan. Nous supposons donc que \( A=(\mtu+N)\) avec \( N^m=0\). 
    En nous inspirant de \eqref{EqweEZnV}, nous posons\footnote{Le logarithme d'un nombre n'est pas encore définit à ce moment, mais cela ne nous empêche pas de poser une définition ici pour une application des réels vers les matrices.}
    \begin{equation}
        D(t)=tN-\frac{ t^2 }{ 2 }N^2+\cdots +(-1)^m\frac{ t^{m-1} }{ m-1 }N^{m-1}
    \end{equation}
    et nous allons prouver que \(  e^{D(1)}=\mtu+N\). Notons que \( N\) étant nilpotente, cette somme ainsi que toutes celles qui viennent sont finies. Il n'y a donc pas de problèmes de convergences dans cette preuve (si ce n'est les passages des équations \eqref{EqqACuGK}).

    Nous posons \( S(t)= e^{D(t)}\) (la somme est finie), et nous avons
    \begin{equation}
        S'(t)=D'(t) e^{D(t)}
    \end{equation}
    Afin d'obtenir une expression qui donne \( S'\) en termes de \( S\), nous multiplions par \( (\mtu+tN)\) en remarquant que \( (\mtu+tN)D'(t)=N\) nous avons
    \begin{equation}
        (\mtu+tN)S'(t)=NS(t).
    \end{equation}
    En dérivant à nouveau,
    \begin{equation}    \label{EqKjccqP}
        (\mtu+tN)S''(t)=0.
    \end{equation}
    La matrice \( (\mtu+tN)\) est inversible parce que son noyau est réduit à \( \{ 0 \}\). En effet si \( (\mtu+tN)x=0\), alors \( Nx=-\frac{1}{ t }x\), ce qui est impossible parce que \( N\) est nilpotente. Ce que dit l'équation \eqref{EqKjccqP} est alors que \( S''(t)=0\). Si nous développons \( S(t)\) en puissances de \( t\) nous nous arrêtons au terme d'ordre \( 1\) et nous avons
    \begin{equation}
        S(t)=S(0)+tS'(0)=\mtu+tD'(0)=1+tN.
    \end{equation}
    En \( t=1\) nous trouvons \( S(1)=\mtu+N\). La matrice \( D(1)\) donnée est donc bien un logarithme de $\mtu+N$.
\end{proof}

\begin{proposition}[\cite{fJhCTE}]
    Si \( A\in \eM(n,\eC)\) est telle que \( \rho(A)<1\), alors \( A^n\to 0\).
\end{proposition}

\begin{proof}
    Nous nous plaçons dans une base des espaces caractéristiques de \( A\), c'est à dire que nous supposons que la matrice \( A\) a la forme
    \begin{equation}        \label{EqWMvkgLo}
        A=\begin{pmatrix}
            \lambda_1\mtu+N_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_s\mtu+N_s
        \end{pmatrix}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( A\) et les \( N_i\) sont nilpotentes. En effet nous savons que l'espace caractéristique \( F_{\lambda_i}\) est l'espace de nilpolence de \( A-\lambda_i\mtu\). Si nous notons \( A_i\) la restriction de \( A\) à cet espace, la matrice \( N_i=A_i-\lambda_i\mtu\) est nilpotente. Du coup \( A_i=\lambda_I\mtu+N_i\) et nous avons bien la décomposition \eqref{EqWMvkgLo}.

    Nous avons donc \( A^n\to 0\) si et seulement si \( (N_i+\lambda_i\mtu)^n\to 0\) pour tout \( i\). Soit donc \( N\) nilpotente et \( \lambda<1\) (parce que nous savons que toutes les valeurs propres de \( A\) sont inférieures à un). Nous avons
    \begin{equation}
            (\lambda\mtu+N)^n=\sum_{k=0}^n\binom{ n }{ k }\lambda^{n-k}N^{k}
            =\sum_{k=0}^{r-1}\binom{ n }{ k }\lambda^{n-k}N^{k}.
    \end{equation}
    Nous voyons que le nombre de termes dans la somme ne dépend pas de \( n\). De plus pour chacun de termes, la puissance de \( N\) ne dépend pas non plus de \( n\). Le terme
    \begin{equation}
        \binom{ n }{ k }\lambda^{n-k}\leq P(n)\lambda^{n-k}
    \end{equation}
    où \( P\) est un polynôme tend vers zéro lorsque \( n\) devient grand parce que c'est une cas polynôme fois exponentielle.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Diagonalisabilité d'exponentielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{fJhCTE}]      \label{PropCOMNooIErskN}
    Si \( A\in \eM(n,\eR)\) a un polynôme caractéristique scindé, alors \( A\) est diagonalisable si et seulement si \( e^A\) est diagonalisable.
\end{proposition}
\index{décomposition!Dunford!application}
\index{exponentielle!de matrice}
\index{diagonalisable!exponentielle}

\begin{proof}
    Si \( A\) est diagonalisable, alors il existe une matrice inversible \( M\) telle que \( D=M^{-1}AM\) soit diagonale (c'est la définition \ref{DefCNJqsmo}). Dans ce cas nous avons aussi \( (M^{-1}AM)^k=M^{-1}A^kM\) et donc \( M^{-1}e^AM=e^{M^{-1}AM}=e^D\) qui est diagonale.

    La partie difficile est donc le contraire. 
    
    \begin{subproof}
        \item[Qui est diagonalisable et comment ?]
            Nous supposons que \( e^A\) est diagonalisable et nous écrivons la décomposition de Dunford (théorème \ref{ThoRURcpW}) :
            \begin{equation}
                A=S+N
            \end{equation}
            où \( S\) est diagonalisable, \( N\) est nilpotente, \( [S,N]=0\). Nous avons besoin de prouver que \( N=0\).
    
            Les matrices \( A\) est \( S\) commutent; en passant au développement nous en déduisons que \( A\) et \( e^S\) commutent, puis encore en passant au développement que \( e^A\) et \( e^S\) commutent. Vu que \( S\) est diagonalisable, \( e^S\) l'est et par hypothèse \( e^A\) est également diagonalisable. Donc \( e^A\) et \( e^{-S}\) sont simultanément diagonalisables par la proposition \ref{PropGqhAMei}.

            Étant donné que \( A\) et \( S\) commutent, nous avons \( e^N=e^{A-S}=e^Ae^{-S}\), et nous en déduisons que \( e^N\) est diagonalisable vu que les deux facteurs \( e^A\) et \( e^{-S}\) sont simultanément diagonalisables.

        \item[Unipotence]

            Si \( r\) est le degré de nilpotence de \( N\), nous avons
            \begin{equation}    \label{EqQHjvLZQ}
                e^N-\mtu=N+\frac{ N^2 }{2}+\cdots +\frac{ N^{r-1} }{ (r-1)! }.
            \end{equation}
            Donc
            \begin{equation}
                (e^N-\mtu)^k=\left( N+\frac{ N^2 }{2}+\cdots +\frac{ N^{r-1} }{ (r-1)! } \right)^k
            \end{equation}
            où le membre de droite est un polynôme en \( N\) dont le terme de plus bas degré est de degré \( k\). Donc \( (e^N-\mtu)\) est nilpotente et \( e^N\) est unipotente.

            Si \( M\) est la matrice qui diagonalise \( e^N\), alors la matrice diagonale \( M^{-1}e^NM\) est tout autant unipotente que \( e^N\) elle-même. En effet,
            \begin{subequations}
                \begin{align}
                    (M^{-1}e^NM-\mtu)^r&=\sum_{k=0}^r\binom{ r }{ k }(-1)^{r-k}M^{-1}(e^N)^kM\\
                    &=M^{-1}\left( \sum_{k=0}^r\binom{ r }{ k }(-1)^{r-k}(e^N)^k \right)M\\
                    &=M^{-1}(e^N-\mtu)^rM\\
                    &=0.
                \end{align}
            \end{subequations}

            La matrice \( M^{-1}e^NM\) est donc une matrice diagonale et unipotente; donc \( M^{-1}e^NM=\mtu\), ce qui donne immédiatement que \( e^N=\mtu\).

        \item[Polynômes annulateurs]

            En reprenant le développement \eqref{EqQHjvLZQ} sachant que \( e^N=\mtu\), nous savons que
            \begin{equation}
                N+\frac{ N^2 }{2}+\cdots +\frac{ N^{r-1} }{ (r-1)! }=0.
            \end{equation}
            Dit en termes pompeux (mais non moins porteurs de sens), le polynôme
            \begin{equation}
                Q(X)=X+\frac{ X^2 }{2}+\cdots +\frac{ X^{r-1} }{ (r-1)! }
            \end{equation}
            est un polynôme annulateur de \( N\).
            
            La proposition \ref{PropAnnncEcCxj} stipule que le polynôme minimal d'un endomorphisme divise tous les polynômes annulateurs. Dans notre cas, \( X^r\) est un polynôme annulateur et donc le polynôme minimal de \( N\) est de la forme \( X^k\). Donc il est \( X^r\) lui-même.
            
            Nous avons donc \( X^r\divides Q\). Mais \( Q\) est un polynôme contenant le monôme \( X\) donc \( X^r\) ne peut diviser \( Q\) que si \( r=1\). Nous en concluons que \( X\) est un polynôme annulateur de \( N\). C'est à dire que \( N=0\).

        \item[Conclusion]

            Vu que Dunford\footnote{Théorème \ref{ThoRURcpW}.} dit que \( A=S+N\) et que nous venons de prouver que \( N=0\), nous concluons que \( A=S\) avec \( S\) diagonalisable.

    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Valeurs singulières}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Soit \( M\) une matrice \( m\times n\) sur \( \eK\) (\( \eK\) est \( \eR\) ou \( \eC\)). Un nombre réel \( \sigma\) est une \defe{valeur singulière}{valeur!singulière} de \( M\) s'il existent des vecteurs unitaires \( u\in \eK^m\), \( v\in \eK^n\) tels que
    \begin{subequations}
        \begin{align}
            Mv&=\sigma u\\
            M^*u&=\sigma v.
        \end{align}
    \end{subequations}
\end{definition}

\begin{theorem}[Décomposition en valeurs singulières]
    Soit \( M\in \eM(m\times n,\eK)\) où \( \eK=\eR,\eC\). Alors \( M\) se décompose en
    \begin{equation}
        M=ADB
    \end{equation}
    où
    il existe deux matrices unitaires \( A\in \gU(m\times m)\), \( B\in \gU(n\times n)\) et une matrice (pseudo)diagonale \( D\in \eM(m\times n)\) tels que
    \begin{enumerate}
        \item 
            \( A\in\gU(m\times m)\), \( B\in\gU(n\times n)\) sont deux matrices unitaires;,
        \item
            \( D\) est (pseudo)diagonale,
        \item
            les éléments diagonaux de \( D\) sont les valeurs singulières de \( M\),
        \item
            le nombre d'éléments non nuls sur la diagonale de \( D\) est le rang de \( M\).
    \end{enumerate}
\end{theorem}

\begin{corollary}
    Soit \( M\in \eM(n,\eC)\). Il existe un isomorphisme \( f\colon \eC^n\to \eC^n\) tel que \( fM\) soit autoadjoint.
\end{corollary}

\begin{proof}
    Si \( M=ADB\) est la décomposition de \( M\) en valeurs singulières, alors nous pouvons prendre \( f=\overline{ B }^tA^{-1}\) qui est une matrice inversible. Pour la vérification que ce \( f\) répond bien à la question, ne pas oublier que \( D\) est réelle, même si \( M\) ne l'est pas.
\end{proof}
